_wandb:
    value:
        cli_version: 0.21.0
        e:
            d7jrfbpa9xtyasvb9w464ufj3xve7hy3:
                args:
                    - config/L4_dynamic_start.py
                codePath: train.py
                codePathLocal: train.py
                cpu_count: 4
                cpu_count_logical: 8
                cudaVersion: "12.2"
                disk:
                    /:
                        total: "395184570368"
                        used: "45097160704"
                email: adamskrodzki@gmail.com
                executable: /home/zeus/miniconda3/envs/cloudspace/bin/python
                git:
                    commit: d20859d51e445eb688cf086b9d2dfa8f0a33e208
                    remote: https://github.com/JinxCodesAI/nano_gpt.git
                gpu: NVIDIA L4
                gpu_count: 1
                gpu_nvidia:
                    - architecture: Ada
                      cudaCores: 7424
                      memoryTotal: "24152899584"
                      name: NVIDIA L4
                      uuid: GPU-d121b909-cb5c-d041-a4ec-24fe84d9adcb
                host: cs-01k0kwwdezdmt9vcrada4j4z8a
                memory:
                    total: "33652191232"
                os: Linux-6.8.0-1032-gcp-x86_64-with-glibc2.31
                program: /teamspace/studios/this_studio/nanoGPT/train.py
                python: CPython 3.10.10
                root: /teamspace/studios/this_studio/nanoGPT
                startedAt: "2025-07-20T12:35:47.183929Z"
                writerId: d7jrfbpa9xtyasvb9w464ufj3xve7hy3
        m: []
        python_version: 3.10.10
        t:
            "1":
                - 1
            "2":
                - 1
            "3":
                - 13
                - 16
            "4": 3.10.10
            "5": 0.21.0
            "12": 0.21.0
            "13": linux-x86_64
always_save_checkpoint:
    value: true
attn_lora_rank:
    value: 48
attn_lora_rank_divisor:
    value: 16
backend:
    value: nccl
batch_size:
    value: 32
batch_size_multiplier:
    value: 1
beta1:
    value: 0.9
beta2:
    value: 0.95
bias:
    value: false
block_size:
    value: 1024
compile:
    value: true
dataset:
    value: fineweb10B
decay_lr:
    value: true
device:
    value: cuda
dropout:
    value: 0
dtype:
    value: bfloat16
embedding_mode:
    value: lora
embedding_rank:
    value: 48
eval_interval:
    value: 100
eval_interval_multiplier:
    value: 1
eval_iters:
    value: 10
eval_iters_multiplier:
    value: 1
eval_only:
    value: false
file_logging:
    value: true
grad_accum_multiplier:
    value: 1
grad_clip:
    value: 1
gradient_accumulation_steps:
    value: 2
init_from:
    value: scratch
learning_rate:
    value: 0.001
log_dir:
    value: logs
log_interval:
    value: 10
lora_alpha:
    value: 1
lora_alpha_multiplier:
    value: 1
lr_decay_iters:
    value: 600000
lr_multiplier:
    value: 10
max_iters:
    value: 600000
min_lr:
    value: 6e-05
n_embd:
    value: 768
n_head:
    value: 12
n_hidden_divisor:
    value: 4
n_layer:
    value: 12
n_layer_divisor:
    value: 4
out_dir:
    value: out
rotary_base:
    value: 10000
rotary_max_position_embeddings:
    value: 2048
use_rotary_embeddings:
    value: true
vocab_lora_rank_divisor:
    value: 16
wandb_log:
    value: true
wandb_project:
    value: owt
wandb_run_name:
    value: gpt2
warmup_iters:
    value: 200
warmup_iters_multiplier:
    value: 1
weight_decay:
    value: 0.1
