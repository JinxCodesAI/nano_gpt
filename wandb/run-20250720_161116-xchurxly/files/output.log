Calculating target architecture based on schedule...

============================================================
       TARGET MODEL ARCHITECTURE (at end of schedule)
============================================================
  n_layer                | 6
  n_head                 | 12
  n_embd                 | 768
  n_hidden               | 1536
  block_size             | 1024
  vocab_size             | 50304
  dropout                | 0.0
  bias                   | False
  embedding_mode         | standard
  attn_lora_rank         | 0
  embedding_rank         | 0
  lora_alpha             | 0.0
============================================================


============================================================
           INITIAL MODEL ARCHITECTURE (at Iter 0)
============================================================
  n_layer                | 3
  n_head                 | 12
  n_embd                 | 768
  n_hidden               | 768
  block_size             | 1024
  vocab_size             | 50304
  dropout                | 0.0
  bias                   | False
  embedding_mode         | lora
  attn_lora_rank         | 48
  embedding_rank         | 48
  lora_alpha             | 1.0
============================================================


Detailed parameter count:
  total                  | Total:   52,149,504 | Trainable:    8,207,616
  token_embeddings       | Total:    2,451,456 | Trainable:    2,451,456
  position_embeddings    | Total:            0 | Trainable:            0
  attention_layers       | Total:    7,520,256 | Trainable:    2,211,840
  feed_forward_layers    | Total:    3,538,944 | Trainable:    3,538,944
  layer_norms            | Total:        4,608 | Trainable:        4,608
  final_layer_norm       | Total:          768 | Trainable:          768
  language_model_head    | Total:   38,633,472 | Trainable:            0
------------------------------------------------------------
eval every:100
step 0: train loss 10.9683, val loss 10.9694
--- Model Analysis ---
  MLP Rank Utilization (L0): 71.09% (546/768)
--- Model Analysis ---
  Attention Rank Utilization (L0): 91.93% (706/768)
--- Model Analysis ---
  MLP Rank Utilization (L1): 71.09% (546/768)
--- Model Analysis ---
  Attention Rank Utilization (L1): 91.93% (706/768)
--- Model Analysis ---
  MLP Rank Utilization (L2): 71.22% (547/768)
--- Model Analysis ---
  Attention Rank Utilization (L2): 91.93% (706/768)
--- Model Analysis ---
  Embedding Utilization (L2): 97.53% (749/768)
  Average Attention Entropy:  5.8975
----------------------
merge_lora_weights 10.969396591186523 6.7 1500
iter 0: loss 10.9639, lr 0.00005, time 9360.08ms, mfu -100.00%
iter 10: loss 9.6191, lr 0.00055, time 537.54ms, mfu 34.38%
iter 20: loss 9.4492, lr 0.00104, time 541.09ms, mfu 34.36%
iter 30: loss 9.3497, lr 0.00154, time 545.05ms, mfu 34.31%
iter 40: loss 9.1057, lr 0.00204, time 555.37ms, mfu 34.21%
iter 50: loss 8.9107, lr 0.00254, time 560.17ms, mfu 34.09%
iter 60: loss 8.9011, lr 0.00303, time 561.40ms, mfu 33.97%
iter 70: loss 8.8266, lr 0.00353, time 562.07ms, mfu 33.86%
iter 80: loss 8.7782, lr 0.00403, time 554.90ms, mfu 33.81%
iter 90: loss 8.5829, lr 0.00453, time 569.48ms, mfu 33.67%
step 100: train loss 8.5206, val loss 8.5739
--- Model Analysis ---
  MLP Rank Utilization (L0): 65.10% (500/768)
--- Model Analysis ---
  Attention Rank Utilization (L0): 91.93% (706/768)
--- Model Analysis ---
  MLP Rank Utilization (L1): 66.54% (511/768)
--- Model Analysis ---
  Attention Rank Utilization (L1): 91.93% (706/768)
--- Model Analysis ---
  MLP Rank Utilization (L2): 67.71% (520/768)
--- Model Analysis ---
  Attention Rank Utilization (L2): 91.93% (706/768)
--- Model Analysis ---
  Embedding Utilization (L2): 97.53% (749/768)
  Average Attention Entropy:  5.4785
----------------------
saving checkpoint to out
merge_lora_weights 8.57391357421875 6.7 1500
iter 100: loss 8.5509, lr 0.00502, time 5430.87ms, mfu 30.64%
iter 110: loss 8.4377, lr 0.00552, time 571.51ms, mfu 30.81%
iter 120: loss 8.3780, lr 0.00602, time 580.99ms, mfu 30.91%
iter 130: loss 8.3908, lr 0.00652, time 582.99ms, mfu 30.99%
iter 140: loss 8.2770, lr 0.00701, time 587.19ms, mfu 31.04%
iter 150: loss 8.1440, lr 0.00751, time 588.03ms, mfu 31.08%
iter 160: loss 8.1871, lr 0.00801, time 584.90ms, mfu 31.13%
iter 170: loss 8.0279, lr 0.00851, time 578.83ms, mfu 31.21%
iter 180: loss 7.9787, lr 0.00900, time 581.05ms, mfu 31.27%
iter 190: loss 8.0614, lr 0.00950, time 579.46ms, mfu 31.33%
step 200: train loss 7.9429, val loss 7.9865
--- Model Analysis ---
  MLP Rank Utilization (L0): 60.16% (462/768)
--- Model Analysis ---
  Attention Rank Utilization (L0): 91.93% (706/768)
--- Model Analysis ---
  MLP Rank Utilization (L1): 50.52% (388/768)
--- Model Analysis ---
  Attention Rank Utilization (L1): 91.93% (706/768)
--- Model Analysis ---
  MLP Rank Utilization (L2): 52.08% (400/768)
--- Model Analysis ---
  Attention Rank Utilization (L2): 91.93% (706/768)
--- Model Analysis ---
  Embedding Utilization (L2): 97.53% (749/768)
  Average Attention Entropy:  4.5269
----------------------
saving checkpoint to out
merge_lora_weights 7.986518859863281 6.7 1500
iter 200: loss 7.9605, lr 0.01000, time 5470.52ms, mfu 28.54%
iter 210: loss 7.9417, lr 0.01000, time 561.46ms, mfu 28.97%
iter 220: loss 7.9543, lr 0.01000, time 569.37ms, mfu 29.32%
iter 230: loss 7.9029, lr 0.01000, time 572.08ms, mfu 29.62%
iter 240: loss 7.8138, lr 0.01000, time 576.62ms, mfu 29.86%
iter 250: loss 7.9537, lr 0.01000, time 574.34ms, mfu 30.09%
iter 260: loss 7.8146, lr 0.01000, time 575.21ms, mfu 30.30%
iter 270: loss 7.6557, lr 0.01000, time 580.07ms, mfu 30.45%
iter 280: loss 7.8275, lr 0.01000, time 575.31ms, mfu 30.62%
iter 290: loss 7.6335, lr 0.01000, time 580.97ms, mfu 30.74%
step 300: train loss 7.6366, val loss 7.6801
--- Model Analysis ---
  MLP Rank Utilization (L0): 56.25% (432/768)
--- Model Analysis ---
  Attention Rank Utilization (L0): 91.93% (706/768)
--- Model Analysis ---
  MLP Rank Utilization (L1): 44.53% (342/768)
--- Model Analysis ---
  Attention Rank Utilization (L1): 91.93% (706/768)
--- Model Analysis ---
  MLP Rank Utilization (L2): 43.88% (337/768)
--- Model Analysis ---
  Attention Rank Utilization (L2): 91.93% (706/768)
--- Model Analysis ---
  Embedding Utilization (L2): 97.53% (749/768)
  Average Attention Entropy:  4.3990
----------------------
saving checkpoint to out
merge_lora_weights 7.68010950088501 6.7 1500
iter 300: loss 7.8123, lr 0.01000, time 5791.42ms, mfu 27.99%
iter 310: loss 7.6044, lr 0.01000, time 578.20ms, mfu 28.38%
iter 320: loss 7.5167, lr 0.01000, time 577.95ms, mfu 28.74%
iter 330: loss 7.5597, lr 0.01000, time 577.93ms, mfu 29.07%
iter 340: loss 7.4406, lr 0.01000, time 580.84ms, mfu 29.34%
iter 350: loss 7.6193, lr 0.01000, time 579.17ms, mfu 29.60%
iter 360: loss 7.5461, lr 0.01000, time 580.93ms, mfu 29.82%
iter 370: loss 7.4170, lr 0.01000, time 580.90ms, mfu 30.02%
iter 380: loss 7.4425, lr 0.01000, time 576.93ms, mfu 30.22%
iter 390: loss 7.5233, lr 0.01000, time 576.99ms, mfu 30.40%
step 400: train loss 7.4120, val loss 7.4061
--- Model Analysis ---
  MLP Rank Utilization (L0): 52.99% (407/768)
--- Model Analysis ---
  Attention Rank Utilization (L0): 91.93% (706/768)
--- Model Analysis ---
  MLP Rank Utilization (L1): 44.01% (338/768)
--- Model Analysis ---
  Attention Rank Utilization (L1): 91.93% (706/768)
--- Model Analysis ---
  MLP Rank Utilization (L2): 42.71% (328/768)
--- Model Analysis ---
  Attention Rank Utilization (L2): 91.93% (706/768)
--- Model Analysis ---
  Embedding Utilization (L2): 97.53% (749/768)
  Average Attention Entropy:  4.2258
----------------------
saving checkpoint to out
merge_lora_weights 7.406067848205566 6.7 1500
iter 400: loss 7.4557, lr 0.01000, time 5764.27ms, mfu 27.68%
iter 410: loss 7.5538, lr 0.01000, time 569.87ms, mfu 28.16%
iter 420: loss 7.3671, lr 0.01000, time 575.28ms, mfu 28.55%
iter 430: loss 7.4424, lr 0.01000, time 578.06ms, mfu 28.89%
iter 440: loss 7.3896, lr 0.01000, time 578.29ms, mfu 29.20%
iter 450: loss 7.3612, lr 0.01000, time 581.20ms, mfu 29.46%
iter 460: loss 7.3398, lr 0.01000, time 580.04ms, mfu 29.70%
iter 470: loss 7.3832, lr 0.01000, time 580.16ms, mfu 29.92%
iter 480: loss 7.2192, lr 0.01000, time 579.12ms, mfu 30.12%
iter 490: loss 7.3657, lr 0.01000, time 580.57ms, mfu 30.29%
step 500: train loss 7.3192, val loss 7.3120
--- Model Analysis ---
  MLP Rank Utilization (L0): 50.13% (385/768)
--- Model Analysis ---
  Attention Rank Utilization (L0): 91.93% (706/768)
--- Model Analysis ---
  MLP Rank Utilization (L1): 44.66% (343/768)
--- Model Analysis ---
  Attention Rank Utilization (L1): 91.93% (706/768)
--- Model Analysis ---
  MLP Rank Utilization (L2): 42.45% (326/768)
--- Model Analysis ---
  Attention Rank Utilization (L2): 91.93% (706/768)
--- Model Analysis ---
  Embedding Utilization (L2): 97.53% (749/768)
  Average Attention Entropy:  4.1242
----------------------
saving checkpoint to out
merge_lora_weights 7.312012672424316 6.7 1500
iter 500: loss 7.3193, lr 0.01000, time 5845.01ms, mfu 27.57%
iter 510: loss 7.3643, lr 0.01000, time 570.25ms, mfu 28.06%
iter 520: loss 7.3398, lr 0.01000, time 581.54ms, mfu 28.43%
iter 530: loss 7.2700, lr 0.01000, time 579.13ms, mfu 28.78%
iter 540: loss 7.2586, lr 0.01000, time 577.41ms, mfu 29.10%
iter 550: loss 7.3642, lr 0.01000, time 578.33ms, mfu 29.39%
iter 560: loss 7.2531, lr 0.01000, time 578.98ms, mfu 29.64%
iter 570: loss 7.2139, lr 0.01000, time 580.15ms, mfu 29.86%
iter 580: loss 7.2705, lr 0.01000, time 578.17ms, mfu 30.07%
iter 590: loss 7.0900, lr 0.01000, time 578.99ms, mfu 30.26%
step 600: train loss 7.2145, val loss 7.2451
--- Model Analysis ---
  MLP Rank Utilization (L0): 47.27% (363/768)
--- Model Analysis ---
  Attention Rank Utilization (L0): 91.93% (706/768)
--- Model Analysis ---
  MLP Rank Utilization (L1): 45.31% (348/768)
--- Model Analysis ---
  Attention Rank Utilization (L1): 91.93% (706/768)
--- Model Analysis ---
  MLP Rank Utilization (L2): 42.45% (326/768)
--- Model Analysis ---
  Attention Rank Utilization (L2): 91.93% (706/768)
--- Model Analysis ---
  Embedding Utilization (L2): 97.53% (749/768)
  Average Attention Entropy:  4.0722
----------------------
saving checkpoint to out
merge_lora_weights 7.245139122009277 6.7 1500
iter 600: loss 7.2049, lr 0.01000, time 5751.45ms, mfu 27.55%
iter 610: loss 7.1703, lr 0.01000, time 581.11ms, mfu 27.98%
iter 620: loss 7.2564, lr 0.01000, time 577.99ms, mfu 28.38%
iter 630: loss 7.1931, lr 0.01000, time 578.58ms, mfu 28.73%
iter 640: loss 7.2216, lr 0.01000, time 580.25ms, mfu 29.04%
iter 650: loss 7.3510, lr 0.01000, time 577.64ms, mfu 29.34%
iter 660: loss 7.2933, lr 0.01000, time 579.78ms, mfu 29.59%
iter 670: loss 7.1660, lr 0.01000, time 579.70ms, mfu 29.82%
iter 680: loss 7.1794, lr 0.01000, time 579.14ms, mfu 30.03%
iter 690: loss 7.1381, lr 0.01000, time 579.61ms, mfu 30.22%
step 700: train loss 7.2484, val loss 7.2403
--- Model Analysis ---
  MLP Rank Utilization (L0): 44.92% (345/768)
--- Model Analysis ---
  Attention Rank Utilization (L0): 91.93% (706/768)
--- Model Analysis ---
  MLP Rank Utilization (L1): 45.96% (353/768)
--- Model Analysis ---
  Attention Rank Utilization (L1): 91.93% (706/768)
--- Model Analysis ---
  MLP Rank Utilization (L2): 42.45% (326/768)
--- Model Analysis ---
  Attention Rank Utilization (L2): 91.93% (706/768)
--- Model Analysis ---
  Embedding Utilization (L2): 97.53% (749/768)
  Average Attention Entropy:  3.8600
----------------------
saving checkpoint to out
merge_lora_weights 7.2402544021606445 6.7 1500
iter 700: loss 7.1335, lr 0.01000, time 5912.54ms, mfu 27.51%
iter 710: loss 7.1806, lr 0.01000, time 572.05ms, mfu 27.99%
iter 720: loss 7.2762, lr 0.01000, time 577.64ms, mfu 28.39%
iter 730: loss 7.1917, lr 0.01000, time 579.55ms, mfu 28.74%
iter 740: loss 7.1323, lr 0.01000, time 580.15ms, mfu 29.05%
iter 750: loss 7.0895, lr 0.01000, time 575.43ms, mfu 29.36%
iter 760: loss 7.1335, lr 0.01000, time 580.21ms, mfu 29.61%
iter 770: loss 7.2394, lr 0.01000, time 581.15ms, mfu 29.82%
iter 780: loss 7.2013, lr 0.01000, time 578.87ms, mfu 30.03%
iter 790: loss 7.1325, lr 0.01000, time 581.69ms, mfu 30.21%
step 800: train loss 7.0663, val loss 7.1714
--- Model Analysis ---
  MLP Rank Utilization (L0): 43.10% (331/768)
--- Model Analysis ---
  Attention Rank Utilization (L0): 91.93% (706/768)
--- Model Analysis ---
  MLP Rank Utilization (L1): 46.61% (358/768)
--- Model Analysis ---
  Attention Rank Utilization (L1): 91.93% (706/768)
--- Model Analysis ---
  MLP Rank Utilization (L2): 42.45% (326/768)
--- Model Analysis ---
  Attention Rank Utilization (L2): 91.93% (706/768)
--- Model Analysis ---
  Embedding Utilization (L2): 97.53% (749/768)
  Average Attention Entropy:  3.8569
----------------------
saving checkpoint to out
merge_lora_weights 7.171423435211182 6.7 1500
iter 800: loss 7.0168, lr 0.01000, time 5758.30ms, mfu 27.51%
iter 810: loss 7.0779, lr 0.01000, time 571.25ms, mfu 27.99%
iter 820: loss 7.0559, lr 0.01000, time 577.98ms, mfu 28.39%
iter 830: loss 7.2587, lr 0.01000, time 580.34ms, mfu 28.74%
iter 840: loss 7.1497, lr 0.01000, time 580.35ms, mfu 29.05%
iter 850: loss 7.0341, lr 0.01000, time 578.50ms, mfu 29.34%
iter 860: loss 7.1889, lr 0.01000, time 579.72ms, mfu 29.59%
iter 870: loss 7.1506, lr 0.01000, time 578.22ms, mfu 29.83%
iter 880: loss 7.1032, lr 0.01000, time 579.17ms, mfu 30.04%
iter 890: loss 7.1227, lr 0.01000, time 579.72ms, mfu 30.22%
step 900: train loss 7.0541, val loss 7.0949
--- Model Analysis ---
  MLP Rank Utilization (L0): 41.93% (322/768)
--- Model Analysis ---
  Attention Rank Utilization (L0): 91.93% (706/768)
--- Model Analysis ---
  MLP Rank Utilization (L1): 47.53% (365/768)
--- Model Analysis ---
  Attention Rank Utilization (L1): 91.93% (706/768)
--- Model Analysis ---
  MLP Rank Utilization (L2): 42.84% (329/768)
--- Model Analysis ---
  Attention Rank Utilization (L2): 91.93% (706/768)
--- Model Analysis ---
  Embedding Utilization (L2): 97.53% (749/768)
  Average Attention Entropy:  3.7715
----------------------
saving checkpoint to out
merge_lora_weights 7.094874382019043 6.7 1500
iter 900: loss 7.0552, lr 0.01000, time 5773.69ms, mfu 27.52%
iter 910: loss 7.0535, lr 0.01000, time 571.00ms, mfu 28.00%
iter 920: loss 7.1445, lr 0.01000, time 580.73ms, mfu 28.39%
iter 930: loss 7.1137, lr 0.01000, time 581.83ms, mfu 28.72%
iter 940: loss 7.1114, lr 0.01000, time 580.33ms, mfu 29.04%
iter 950: loss 7.1202, lr 0.01000, time 580.57ms, mfu 29.31%
iter 960: loss 6.9947, lr 0.01000, time 579.20ms, mfu 29.57%
iter 970: loss 7.1442, lr 0.01000, time 578.79ms, mfu 29.81%
iter 980: loss 6.9916, lr 0.01000, time 580.16ms, mfu 30.01%
iter 990: loss 7.0302, lr 0.01000, time 580.27ms, mfu 30.20%
step 1000: train loss 7.0396, val loss 7.0234
--- Model Analysis ---
  MLP Rank Utilization (L0): 41.02% (315/768)
--- Model Analysis ---
  Attention Rank Utilization (L0): 91.93% (706/768)
--- Model Analysis ---
  MLP Rank Utilization (L1): 48.18% (370/768)
--- Model Analysis ---
  Attention Rank Utilization (L1): 91.93% (706/768)
--- Model Analysis ---
  MLP Rank Utilization (L2): 43.10% (331/768)
--- Model Analysis ---
  Attention Rank Utilization (L2): 91.93% (706/768)
--- Model Analysis ---
  Embedding Utilization (L2): 97.53% (749/768)
  Average Attention Entropy:  3.8055
----------------------
saving checkpoint to out
merge_lora_weights 7.0234174728393555 6.7 1500
iter 1000: loss 6.9529, lr 0.01000, time 5835.94ms, mfu 27.49%
iter 1010: loss 7.0404, lr 0.01000, time 572.98ms, mfu 27.97%
iter 1020: loss 6.9788, lr 0.01000, time 580.79ms, mfu 28.36%
iter 1030: loss 6.8590, lr 0.01000, time 579.76ms, mfu 28.71%
iter 1040: loss 7.0127, lr 0.01000, time 579.15ms, mfu 29.03%
iter 1050: loss 6.8970, lr 0.01000, time 580.71ms, mfu 29.31%
iter 1060: loss 6.9587, lr 0.01000, time 581.37ms, mfu 29.56%
iter 1070: loss 6.9752, lr 0.01000, time 577.23ms, mfu 29.80%
iter 1080: loss 7.1390, lr 0.01000, time 581.02ms, mfu 30.00%
iter 1090: loss 7.1609, lr 0.01000, time 579.95ms, mfu 30.19%
step 1100: train loss 7.0096, val loss 7.0100
--- Model Analysis ---
  MLP Rank Utilization (L0): 40.49% (311/768)
--- Model Analysis ---
  Attention Rank Utilization (L0): 91.93% (706/768)
--- Model Analysis ---
  MLP Rank Utilization (L1): 48.83% (375/768)
--- Model Analysis ---
  Attention Rank Utilization (L1): 91.93% (706/768)
--- Model Analysis ---
  MLP Rank Utilization (L2): 43.62% (335/768)
--- Model Analysis ---
  Attention Rank Utilization (L2): 91.93% (706/768)
--- Model Analysis ---
  Embedding Utilization (L2): 97.53% (749/768)
  Average Attention Entropy:  3.7534
----------------------
saving checkpoint to out
merge_lora_weights 7.010005950927734 6.7 1500
iter 1100: loss 6.9123, lr 0.01000, time 5550.00ms, mfu 27.50%
iter 1110: loss 6.9597, lr 0.01000, time 574.56ms, mfu 27.97%
iter 1120: loss 6.9789, lr 0.01000, time 577.60ms, mfu 28.37%
Traceback (most recent call last):
  File "/teamspace/studios/this_studio/nanoGPT/train.py", line 854, in <module>
    scaler.scale(loss).backward()
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/_tensor.py", line 648, in backward
    torch.autograd.backward(
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/autograd/__init__.py", line 353, in backward
    _engine_run_backward(
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/autograd/graph.py", line 824, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
KeyboardInterrupt
Traceback (most recent call last):
  File "/teamspace/studios/this_studio/nanoGPT/train.py", line 854, in <module>
    scaler.scale(loss).backward()
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/_tensor.py", line 648, in backward
    torch.autograd.backward(
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/autograd/__init__.py", line 353, in backward
    _engine_run_backward(
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/autograd/graph.py", line 824, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
KeyboardInterrupt
Exception ignored in atexit callback: <function _start_and_connect_service.<locals>.teardown_atexit at 0x72e451ccd7e0>
Traceback (most recent call last):
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/wandb/sdk/lib/service/service_connection.py", line 54, in teardown_atexit
    conn.teardown(hooks.exit_code)
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/wandb/sdk/lib/service/service_connection.py", line 182, in teardown
    self._router.join()
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/wandb/sdk/interface/router.py", line 75, in join
    self._thread.join()
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/threading.py", line 1096, in join
    self._wait_for_tstate_lock()
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/threading.py", line 1116, in _wait_for_tstate_lock
    if lock.acquire(block, timeout):
KeyboardInterrupt:
Exception ignored in atexit callback: <function shutdown_compile_workers at 0x72e45d0fd5a0>
Traceback (most recent call last):
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/_inductor/async_compile.py", line 113, in shutdown_compile_workers
    pool.shutdown()
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/_inductor/compile_worker/subproc_pool.py", line 239, in shutdown
    self.process.wait(300)
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/subprocess.py", line 1209, in wait
    return self._wait(timeout=timeout)
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/subprocess.py", line 1937, in _wait
    time.sleep(delay)
KeyboardInterrupt:
