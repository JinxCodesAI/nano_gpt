# **Guide: Training a Language Model with GRPO and a Self-Supervised Reward Model**

This document outlines the step-by-step process for implementing a Generative Reward Policy Optimization (GRPO) training loop. This approach is designed to solve a common challenge in reinforcement learning for language models: the absence of an explicit reward function for unsupervised pre-training data.  
The core of this strategy is a novel, self-supervised reward mechanism. Instead of relying on human feedback (RLHF) or a separate, pre-trained preference model, we create our own reward signal through an adversarial process between two models:

1. **The Base Model (Generator):** The language model we aim to improve.  
2. **The Reward Model (Discriminator/Evaluator):** A specialized version of the Base Model trained for a unique task.

The Reward Model is not a simple binary classifier for "real" vs. "fake" text. Instead, it is trained to solve a more nuanced problem: **given a sequence of text, predict the proportion of tokens that came from the original ("natural") dataset versus those that were synthetically generated by the Base Model.**  
To achieve this, the Reward Model is trained on a custom-generated dataset where each sample is a mix of natural and synthetic data. For a sequence of length N, a random crossover point K is chosen. The first K tokens are taken from the real dataset, and the remaining N-K tokens are generated by the current Base Model. The model is then trained to predict the target probability vector \[K/N, (N-K)/N\].  
The process becomes cyclical and adversarial. The Base Model is then fine-tuned using GRPO, where its "reward" is the P(natural) score assigned by the frozen Reward Model. This incentivizes the Base Model to generate text that so closely mimics the true data distribution that it can consistently "fool" the Reward Model into believing the generated text is entirely natural. This dynamic pushes the generator's quality far beyond what standard supervised training alone can achieve.

## **Part 1: Requirements & System Setup**

### **1.1. Python Libraries**

Ensure you have the necessary libraries installed.  
pip install torch numpy transformers datasets tiktoken tqdm

### **1.2. File Structure**

Your project directory should be organized as follows. We will be modifying model.py and creating a new script, prepare\_reward\_data.py.  
/your\_project/  
|  
|-- model.py             \# (Will be modified)  
|-- train.py             \# Main script for standard LM training  
|-- train\_reward\_model.py \# New script for training the reward model  
|-- train\_grpo.py        \# New script for the GRPO reinforcement learning step  
|  
|-- data/  
|   |-- shakespeare\_char/  
|   |   |-- input.txt  
|   |   |-- prepare.py  
|   |  
|   |-- reward\_dataset/     \# Directory for the reward model's data  
|       |-- train.bin  
|       |-- val.bin  
|  
|-- prepare\_reward\_data.py \# New script to generate the reward dataset

## **Part 2: Model Architecture Modifications (model.py)**

To avoid maintaining two separate large models, we will modify the GPT class to operate in one of two modes: generator or reward. Both modes will share the vast majority of their weights (the "trunk"), but will have different final layers (the "head") specific to their task.

### **2.1. Update GPTConfig**

Add a mode parameter to the configuration class to specify which model to build.  
\# In model.py

from dataclasses import dataclass

@dataclass  
class GPTConfig:  
    block\_size: int \= 1024  
    vocab\_size: int \= 50304  
    n\_layer: int \= 12  
    n\_head: int \= 12  
    n\_embd: int \= 768  
    dropout: float \= 0.0  
    bias: bool \= True  
    \# \--- ADD THIS LINE \---  
    mode: str \= 'generator' \# Can be 'generator' or 'reward'

### **2.2. Update GPT Class**

The \_\_init\_\_ and forward methods will be updated to handle the two modes.  
**Key Changes:**

1. **Shared Trunk:** The transformer module dictionary (containing embeddings, dropout, transformer blocks, and final layer norm) is created for both modes. This is the core of the model that will be shared.  
2. **Conditional Heads:**  
   * If config.mode \== 'generator', we create the standard lm\_head for predicting the next token.  
   * If config.mode \== 'reward', we create a new reward\_head. This head is a simple MLP that takes the final transformer output, pools it, and classifies it.  
3. **Pooling \+ MLP \+ Softmax:** The reward\_head takes the hidden state of the **last token** in the sequence as a summary of the entire sequence (Pooling). It then passes this summary through a small MLP which outputs two values. A Softmax layer converts these values into a probability distribution \[P(natural), P(synthetic)\].  
4. **Forward Pass Logic:** The forward method first computes the output of the shared trunk (x). Then, it branches based on the mode, passing x to the appropriate head to get either logits (for the generator) or probabilities (for the reward model).

Here is the implementation within the GPT class:  
\# In model.py, inside the GPT class

class GPT(nn.Module):  
    def \_\_init\_\_(self, config):  
        super().\_\_init\_\_()  
        assert config.vocab\_size is not None  
        assert config.block\_size is not None  
        assert config.mode in \['generator', 'reward'\], "mode must be 'generator' or 'reward'"  
        self.config \= config

        \# Shared "trunk" of the model  
        self.transformer \= nn.ModuleDict(dict(  
            wte \= nn.Embedding(config.vocab\_size, config.n\_embd),  
            drop \= nn.Dropout(config.dropout),  
            h \= nn.ModuleList(\[Block(config) for \_ in range(config.n\_layer)\]),  
            ln\_f \= LayerNorm(config.n\_embd, bias=config.bias),  
        ))

        \# Task-specific "heads"  
        if self.config.mode \== 'generator':  
            self.lm\_head \= nn.Linear(config.n\_embd, config.vocab\_size, bias=False)  
            self.transformer.wte.weight \= self.lm\_head.weight  
        elif self.config.mode \== 'reward':  
            self.reward\_head \= nn.Sequential(  
                nn.Linear(config.n\_embd, 256),  
                nn.ReLU(),  
                nn.Linear(256, 2), \# Outputs 2 raw scores for \[natural, synthetic\]  
                nn.Softmax(dim=-1)  \# Converts scores to probabilities  
            )

        \# ... (rest of \_\_init\_\_, including weight initialization)

    def forward(self, idx, targets=None):  
        \# ... (forward pass through the shared trunk to get \`x\`)  
        tok\_emb \= self.transformer.wte(idx)  
        x \= self.transformer.drop(tok\_emb)  
        for block in self.transformer.h:  
            x \= block(x)  
        x \= self.transformer.ln\_f(x)

        \# Branching logic for the specific head  
        if self.config.mode \== 'generator':  
            if targets is not None:  
                logits \= self.lm\_head(x)  
                loss \= F.cross\_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore\_index=-1)  
            else:  
                logits \= self.lm\_head(x\[:, \[-1\], :\])  
                loss \= None  
            return logits, loss

        elif self.config.mode \== 'reward':  
            \# Pool by taking the hidden state of the last token  
            pooled\_output \= x\[:, \-1, :\] \# Shape: (batch\_size, n\_embd)  
            \# Get probabilities from the reward head  
            probabilities \= self.reward\_head(pooled\_output) \# Shape: (batch\_size, 2\)  
              
            \# For reward model training, loss is calculated in the training script  
            loss \= None  
            if targets is not None:  
                \# Example loss: Mean Squared Error  
                loss \= F.mse\_loss(probabilities, targets)  
              
            return probabilities, loss

## **Part 3: Preparing Data for the Reward Model**

This step is crucial. We will create a script prepare\_reward\_data.py that generates a labeled dataset for training the Reward Model. **To prevent data contamination, the reward model's training and validation sets must be generated from the base model's corresponding data splits.**

### **prepare\_reward\_data.py Algorithm:**

1. **Inputs:**  
   * Path to the raw text data (input.txt).  
   * Path to a pre-trained Base Model checkpoint (.pt file).  
   * The block\_size to use for samples.  
   * The train/validation split ratio (e.g., 0.9).  
2. **Process:**  
   * Load the tokenizer (e.g., tiktoken).  
   * Load the pre-trained Base Model and set it to evaluation mode (model.eval()).  
   * Read the entire raw text dataset and tokenize it.  
   * **Crucially, split the tokenized data into a training set and a validation set *before* generating any samples.** This split must be identical to the one used for the Base Model's training to ensure the datasets are perfectly aligned and independent.  
   * **Generate Training Data:**  
     * Iterate through the *training split* of the tokenized data.  
     * For each sequence, perform the K crossover generation process to create (X\_train, Y\_train) pairs.  
   * **Generate Validation Data:**  
     * Iterate through the *validation split* of the tokenized data.  
     * For each sequence, perform the K crossover generation process to create (X\_val, Y\_val) pairs.  
3. **Output:**  
   * Save the training pairs to a train.bin file in the reward\_dataset directory.  
   * Save the validation pairs to a val.bin file in the reward\_dataset directory. A simple approach is to write the block\_size integers for X, followed by the 2 floats for Y, and repeat for all samples.

## **Part 4: The Iterative Training Cycle**

This is the high-level orchestration of the entire training process.

### **Cycle 0: Initial Base Model Training**

* **Goal:** Train an initial, competent Base Model.  
* **Action:** Run your standard train.py script on the original dataset (e.g., Shakespeare).  
* **Stop Condition:** Employ an early stopping strategy based on validation loss dynamics to prevent overfitting and save computational resources. A robust set of criteria is:  
  * **Early Stopping with Patience:** Don't stop at the first sign of an increase in validation loss. Instead, track the best validation loss achieved so far. Stop training only if the validation loss has failed to improve (i.e., decrease) for a specified number of evaluation steps, known as "patience" (e.g., 5-10 evaluations).  
  * **Minimum Improvement Threshold (min\_delta):** To avoid stopping due to insignificant fluctuations, consider an improvement valid only if the validation loss decreases by at least a small amount, or min\_delta (e.g., 0.001).  
  * **Practical Rule:** A good combined rule is: "Stop training if the validation loss has not decreased by at least min\_delta over the last patience evaluation steps." This ensures the model has truly plateaued before training is halted.  
* **Output:** A checkpoint file, base\_model\_v1.pt.

### **The Main Loop: Cycles 1, 2, 3...**

This loop iterates through four distinct steps. Let's assume we just finished Cycle N and have base\_model\_v(N+1).pt. We now begin Cycle N+1.

#### **Step 1: Generate Reward Dataset**

* **Goal:** Create a new dataset that challenges the *current* Base Model.  
* **Action:**  
  1. Take the latest Base Model checkpoint: base\_model\_v(N+1).pt.  
  2. Run the prepare\_reward\_data.py script. It will use this model to generate the "synthetic" portions of the data, ensuring the train/val split of the source text is respected.  
* **Output:** A new reward dataset (reward\_data\_v(N+1)/).

#### **Step 2: Train the Reward Model**

* **Goal:** Train a new Reward Model to get good at discriminating the outputs of the latest Base Model.  
* **Action:**  
  1. Write a new script train\_reward\_model.py.  
  2. In this script, instantiate the GPT model with config.mode \= 'reward'.  
  3. Load the weights from base\_model\_v(N+1).pt. PyTorch will load the shared trunk weights and randomly initialize the new reward\_head.  
  4. **Freeze the shared trunk weights.** This is critical. We want to use the powerful, pre-trained language features from the Base Model as a feature extractor without altering them. Only the new reward\_head should be trained. This is faster, more stable, and prevents overfitting.  
     \# In train\_reward\_model.py, after loading the state dict  
     for param in model.transformer.parameters():  
         param.requires\_grad \= False

  5. Train this model on the dataset from Step 1\. The optimizer will now only update the weights of the reward\_head. The loss function should compare the model's output probabilities with the ground-truth ratios (e.g., using Mean Squared Error).  
* **Stop Condition:** Train until validation loss on the reward dataset stops improving, using the same "patience" and "min\_delta" criteria from Cycle 0\.  
* **Output:** A new Reward Model checkpoint, reward\_model\_v(N+1).pt.

#### **Step 3: GRPO Training of the Base Model**

* **Goal:** Use the new Reward Model to improve the Base Model via reinforcement learning.  
* **Action:**  
  1. Write a new script train\_grpo.py.  
  2. Load two models:  
     * The **Base Model** (base\_model\_v(N+1).pt) in 'generator' mode. This is the policy we are updating.  
     * The **Reward Model** (reward\_model\_v(N+1).pt) in 'reward' mode. Freeze its weights (.eval()).  
  3. The GRPO loop:  
     * **Sample a variable-length prompt:**  
       1. First, sample a random prompt length, P\_len, from a distribution (e.g., uniformly from \[1, block\_size / 2\]). This ensures the model is trained on a variety of prompt lengths.  
       2. Then, sample a prompt of P\_len tokens from the original dataset.  
     * Generate a full-length completion (up to block\_size) using the Base Model, conditioned on the sampled prompt.  
     * Pass the full completion to the frozen Reward Model to get a probability distribution. The reward is the first value: P(natural).  
     * Use this reward signal to update the Base Model's weights using a policy gradient algorithm (GRPO/PPO). The objective is to maximize the reward.  
* **When to Stop This Step?** This is a critical decision. You stop GRPO training and start a new cycle when the Base Model has effectively "solved" the current Reward Model.  
  * **Primary Metric:** The average reward. When the average P(natural) score that the Base Model achieves from the Reward Model **plateaus or exceeds a high threshold (e.g., 0.9-0.95)**, it means the generator can consistently fool this version of the discriminator. It's time to build a better discriminator.  
  * **Secondary Metric:** KL divergence between the policy (Base Model) before and after an update batch. If this divergence grows too large, the policy is changing too drastically, which can lead to instability. Capping it is a core part of PPO/GRPO.  
* **Output:** An improved Base Model, base\_model\_v(N+2).pt.

#### **Step 4: Loop or Terminate?**

* **Go to Step 1:** You now have a new, improved base model. Repeat the cycle by using it to generate a new, harder reward dataset.  
* **When to Break Execution?** The entire process can be stopped when:  
  1. You complete a pre-defined number of cycles (e.g., 5-10 cycles).  
  2. The overall improvement between cycles diminishes. You can measure this by tracking the perplexity of the Base Model on a held-out, global validation set. If perplexity stops improving cycle-over-cycle, the model has likely converged.  
  3. Human evaluation. Ultimately, the goal is better text. Periodically generate samples and have humans rate them. If the quality is no longer improving, stop the process.

##### **How to Measure Perplexity**

Perplexity is a core metric for evaluating language models. It measures how "surprised" a model is by a sequence of text. A lower score is better, indicating the model was more confident in its predictions.  
**The Mathematical Connection:** Perplexity is simply the exponentiation of the cross-entropy loss you already use for training.  
Perplexity=ecross−entropy\_loss  
**Calculation Steps:**

1. **Load Model and Data:** Load your Base Model in 'generator' mode and put it in evaluation mode (model.eval()). Use a data loader for your global validation set (the one held out from the very beginning).  
2. **Calculate Average Loss:** Iterate through the entire validation set without updating gradients (with torch.no\_grad():). For each batch, calculate the cross-entropy loss and keep a running average.  
3. **Exponentiate:** Once you have the final average loss over the entire validation set, calculate math.exp(average\_loss) to get the perplexity score.

Here is a sample implementation:  
import torch  
import math

@torch.no\_grad()  
def calculate\_perplexity(model, data\_loader, device):  
    """  
    Calculates the perplexity of a model on a given dataset.  
    """  
    model.eval()  \# Set the model to evaluation mode  
    total\_loss \= 0  
    num\_batches \= 0

    for batch in data\_loader:  
        inputs, targets \= batch  
        inputs \= inputs.to(device)  
        targets \= targets.to(device)

        \# Forward pass to get the loss  
        \_, loss \= model(inputs, targets)

        if loss is not None:  
            total\_loss \+= loss.item()  
            num\_batches \+= 1

    model.train()  \# Set the model back to training mode

    if num\_batches \== 0:  
        return float('inf')

    \# Calculate the average loss and then perplexity  
    average\_loss \= total\_loss / num\_batches  
    perplexity \= math.exp(average\_loss)  
      
    return perplexity

This iterative, adversarial process provides a robust path toward improving your language model's generation quality without needing a predefined, explicit reward function.