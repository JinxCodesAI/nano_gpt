_wandb:
    value:
        cli_version: 0.21.0
        e:
            5dhcuk297t4u0nsc3te4pz0qe5n5c122:
                args:
                    - config/L4_dynamic_start.py
                codePath: train.py
                codePathLocal: train.py
                cpu_count: 4
                cpu_count_logical: 8
                cudaVersion: "12.2"
                disk:
                    /:
                        total: "395184570368"
                        used: "45133344768"
                email: adamskrodzki@gmail.com
                executable: /home/zeus/miniconda3/envs/cloudspace/bin/python
                git:
                    commit: c576328aa177909fc0c5b982cbe43318286c447a
                    remote: https://github.com/JinxCodesAI/nano_gpt.git
                gpu: NVIDIA L4
                gpu_count: 1
                gpu_nvidia:
                    - architecture: Ada
                      cudaCores: 7424
                      memoryTotal: "24152899584"
                      name: NVIDIA L4
                      uuid: GPU-f542fea8-f395-782e-3871-0bf7d1f641a1
                host: cs-01k0mr1mpxb7hyms6a4hqqskmb
                memory:
                    total: "33652191232"
                os: Linux-6.8.0-1032-gcp-x86_64-with-glibc2.31
                program: /teamspace/studios/this_studio/nanoGPT/train.py
                python: CPython 3.10.10
                root: /teamspace/studios/this_studio/nanoGPT
                startedAt: "2025-07-20T21:06:05.749633Z"
                writerId: 5dhcuk297t4u0nsc3te4pz0qe5n5c122
        m: []
        python_version: 3.10.10
        t:
            "1":
                - 1
            "2":
                - 1
            "3":
                - 13
                - 16
            "4": 3.10.10
            "5": 0.21.0
            "12": 0.21.0
            "13": linux-x86_64
always_save_checkpoint:
    value: false
attn_lora_rank:
    value: 0
attn_lora_rank_divisor:
    value: 0
backend:
    value: nccl
batch_size:
    value: 32
batch_size_multiplier:
    value: 1
beta1:
    value: 0.9
beta2:
    value: 0.95
bias:
    value: false
block_size:
    value: 1024
compile:
    value: true
dataset:
    value: fineweb10B
decay_lr:
    value: true
device:
    value: cuda
dropout:
    value: 0
dtype:
    value: bfloat16
embedding_mode:
    value: lora
embedding_rank:
    value: 0
eval_interval:
    value: 200
eval_interval_multiplier:
    value: 1
eval_iters:
    value: 1
eval_iters_multiplier:
    value: 1
eval_only:
    value: false
file_logging:
    value: true
grad_accum_multiplier:
    value: 1
grad_clip:
    value: 1
gradient_accumulation_steps:
    value: 2
init_from:
    value: scratch
learning_rate:
    value: 0.0001
log_dir:
    value: logs
log_interval:
    value: 10
lora_alpha:
    value: 1
lora_alpha_multiplier:
    value: 1
lr_decay_iters:
    value: 600000
lr_multiplier:
    value: 1
max_iters:
    value: 600000
min_lr:
    value: 6e-05
n_embd:
    value: 768
n_head:
    value: 12
n_hidden_divisor:
    value: 4
n_layer:
    value: 1
n_layer_divisor:
    value: 12
out_dir:
    value: out
rotary_base:
    value: 10000
rotary_max_position_embeddings:
    value: 2048
use_rotary_embeddings:
    value: true
vocab_lora_rank_divisor:
    value: 0
wandb_log:
    value: true
wandb_project:
    value: owt
wandb_run_name:
    value: gpt2
warmup_iters:
    value: 200
warmup_iters_multiplier:
    value: 1
weight_decay:
    value: 0.1
