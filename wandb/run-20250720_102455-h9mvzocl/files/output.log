
Detailed parameter count:
  total                  | Total:   52,149,504 | Trainable:    8,207,616
  token_embeddings       | Total:    2,451,456 | Trainable:    2,451,456
  position_embeddings    | Total:            0 | Trainable:            0
  attention_layers       | Total:    7,520,256 | Trainable:    2,211,840
  feed_forward_layers    | Total:    3,538,944 | Trainable:    3,538,944
  layer_norms            | Total:        4,608 | Trainable:        4,608
  final_layer_norm       | Total:          768 | Trainable:          768
  language_model_head    | Total:   38,633,472 | Trainable:            0
------------------------------------------------------------
step 0: train loss 10.9667, val loss 10.9694
iter 0: loss 10.9647, lr 0.00005, time 7127.52ms, mfu -100.00%
iter 10: loss 9.6120, lr 0.00055, time 563.48ms, mfu 32.80%
iter 20: loss 9.4854, lr 0.00104, time 556.90ms, mfu 32.84%
iter 30: loss 9.2717, lr 0.00154, time 574.19ms, mfu 32.77%
iter 40: loss 9.1209, lr 0.00204, time 577.54ms, mfu 32.69%
iter 50: loss 8.9693, lr 0.00254, time 578.67ms, mfu 32.62%
iter 60: loss 8.8538, lr 0.00303, time 581.58ms, mfu 32.53%
iter 70: loss 8.8489, lr 0.00353, time 578.20ms, mfu 32.48%
iter 80: loss 8.7509, lr 0.00403, time 576.64ms, mfu 32.43%
iter 90: loss 8.6250, lr 0.00453, time 575.66ms, mfu 32.40%
step 100: train loss 8.5557, val loss 8.5885
saving checkpoint to out

=== SCALING OPERATION TRIGGERED (DDP SYNC) ===
Operation: merge_lora_weights
Trigger reason: Timeout
Current val loss: 8.5885, Trigger loss: 1.0000
Iterations since last op: 100, Max wait: 100
Executing operation: merge_lora_weights  with value: None
Performing architectural operation: merge_lora_weights
Merging LoRA weights into main weights...
LoRA weights merged and reset.

Detailed parameter count:
  total                  | Total:   52,149,504 | Trainable:    8,207,616
  token_embeddings       | Total:    2,451,456 | Trainable:    2,451,456
  position_embeddings    | Total:            0 | Trainable:            0
  attention_layers       | Total:    7,520,256 | Trainable:    2,211,840
  feed_forward_layers    | Total:    3,538,944 | Trainable:    3,538,944
  layer_norms            | Total:        4,608 | Trainable:        4,608
  final_layer_norm       | Total:          768 | Trainable:          768
  language_model_head    | Total:   38,633,472 | Trainable:            0
------------------------------------------------------------
Re-configuring optimizer after architectural change...
num decayed parameter tensors: 17, with 8,202,240 parameters
num non-decayed parameter tensors: 7, with 5,376 parameters
using fused AdamW: True
Transferring optimizer state for existing parameters...
Transferred optimizer state for 0 / 28 parameters
Re-compiling the model...
Architectural operation completed successfully.
=== SCALING OPERATION COMPLETE ===


=== SCALING OPERATION TRIGGERED (DDP SYNC) ===
Operation: change_lr
Trigger reason: Loss threshold
Current val loss: 8.5885, Trigger loss: 100.0000
Iterations since last op: 0, Max wait: 1
Executing operation: change_lr  with value: 0.5
LR multiplier: 10.0000 -> 5.0000
=== SCALING OPERATION COMPLETE ===


=== SCALING OPERATION TRIGGERED (DDP SYNC) ===
Operation: reset_lr_schedule
Trigger reason: Loss threshold
Current val loss: 8.5885, Trigger loss: 100.0000
Iterations since last op: 0, Max wait: 1
Executing operation: reset_lr_schedule  with value: None
LR schedule offset: 0 -> 100
=== SCALING OPERATION COMPLETE ===

iter 100: loss 8.5726, lr 0.00502, time 4860.74ms, mfu 29.54%
Traceback (most recent call last):
  File "/teamspace/studios/this_studio/nanoGPT/train.py", line 654, in <module>
    scaler.scale(loss).backward()
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/_tensor.py", line 648, in backward
    torch.autograd.backward(
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/autograd/__init__.py", line 353, in backward
    _engine_run_backward(
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/autograd/graph.py", line 824, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
KeyboardInterrupt
Traceback (most recent call last):
  File "/teamspace/studios/this_studio/nanoGPT/train.py", line 654, in <module>
    scaler.scale(loss).backward()
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/_tensor.py", line 648, in backward
    torch.autograd.backward(
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/autograd/__init__.py", line 353, in backward
    _engine_run_backward(
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/autograd/graph.py", line 824, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
KeyboardInterrupt
