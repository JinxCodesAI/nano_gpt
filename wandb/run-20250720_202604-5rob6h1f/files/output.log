Calculating target architecture based on schedule...

============================================================
       TARGET MODEL ARCHITECTURE (at end of schedule)
============================================================
  n_layer                | 2
  n_head                 | 12
  n_embd                 | 768
  n_hidden               | 1536
  block_size             | 1024
  vocab_size             | 50304
  dropout                | 0.0
  bias                   | False
  embedding_mode         | standard
  attn_lora_rank         | 0
  embedding_rank         | 0
  lora_alpha             | 0.0
============================================================


============================================================
           INITIAL MODEL ARCHITECTURE (at Iter 0)
============================================================
  n_layer                | 1
  n_head                 | 12
  n_embd                 | 768
  n_hidden               | 768
  block_size             | 1024
  vocab_size             | 50304
  dropout                | 0.0
  bias                   | False
  embedding_mode         | lora
  attn_lora_rank         | 48
  embedding_rank         | 48
  lora_alpha             | 1.0
============================================================


Detailed parameter count:
  total                  | Total:   44,773,632 | Trainable:    4,370,688
  token_embeddings       | Total:    2,451,456 | Trainable:    2,451,456
  position_embeddings    | Total:            0 | Trainable:            0
  attention_layers       | Total:    2,506,752 | Trainable:      737,280
  feed_forward_layers    | Total:    1,179,648 | Trainable:    1,179,648
  layer_norms            | Total:        1,536 | Trainable:        1,536
  final_layer_norm       | Total:          768 | Trainable:          768
  language_model_head    | Total:   38,633,472 | Trainable:            0
------------------------------------------------------------
eval every:200
W0720 20:26:10.011000 3503 /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/_inductor/utils.py:1250] [0/0] Not enough SMs to use max_autotune_gemm mode
step 0: train loss 10.9500, val loss 10.9511
  MLP Rank Utilization (L0): 71.09% (546/768)
Attn LoRA    | 47/48           | 97.92%
--- Model Analysis ---
  Embedding Utilization (L0): 97.53% (749/768)
Embed LoRA   | 47/48           | 97.92%
  Average Attention Entropy:  5.8911
----------------------
merge_lora_weights 10.951141357421875 4.0 10500
iter 0: loss 10.9507, lr 0.00000, time 38899.26ms, mfu -100.00%
iter 10: loss 10.8312, lr 0.00001, time 5.63ms, mfu 1337.48%
iter 20: loss 10.5151, lr 0.00002, time 9.15ms, mfu 1286.05%
iter 30: loss 10.1573, lr 0.00003, time 5.78ms, mfu 1287.75%
iter 40: loss 9.8816, lr 0.00004, time 5.62ms, mfu 1292.92%
iter 50: loss 9.7337, lr 0.00005, time 5.61ms, mfu 1297.95%
iter 60: loss 9.6334, lr 0.00006, time 5.59ms, mfu 1302.98%
iter 70: loss 9.6713, lr 0.00007, time 6.16ms, mfu 1294.86%
iter 80: loss 9.6016, lr 0.00008, time 5.71ms, mfu 1297.25%
iter 90: loss 9.5611, lr 0.00009, time 5.96ms, mfu 1293.85%
iter 100: loss 9.5283, lr 0.00010, time 5.67ms, mfu 1297.37%
iter 110: loss 9.4467, lr 0.00011, time 11.00ms, mfu 1236.10%
iter 120: loss 9.3353, lr 0.00012, time 5.56ms, mfu 1247.95%
iter 130: loss 9.3221, lr 0.00013, time 9.92ms, mfu 1199.09%
iter 140: loss 9.2863, lr 0.00014, time 5.94ms, mfu 1205.87%
iter 150: loss 9.2580, lr 0.00015, time 5.60ms, mfu 1219.66%
Traceback (most recent call last):
  File "/teamspace/studios/this_studio/nanoGPT/train.py", line 865, in <module>
    lossf = loss.item() * gradient_accumulation_steps
KeyboardInterrupt
Traceback (most recent call last):
  File "/teamspace/studios/this_studio/nanoGPT/train.py", line 865, in <module>
    lossf = loss.item() * gradient_accumulation_steps
KeyboardInterrupt
Exception ignored in atexit callback: <function _start_and_connect_service.<locals>.teardown_atexit at 0x78d04b979cf0>
Traceback (most recent call last):
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/wandb/sdk/lib/service/service_connection.py", line 54, in teardown_atexit
    conn.teardown(hooks.exit_code)
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/wandb/sdk/lib/service/service_connection.py", line 182, in teardown
    self._router.join()
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/wandb/sdk/interface/router.py", line 75, in join
    self._thread.join()
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/threading.py", line 1096, in join
    self._wait_for_tstate_lock()
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/threading.py", line 1116, in _wait_for_tstate_lock
    if lock.acquire(block, timeout):
KeyboardInterrupt:
Exception ignored in atexit callback: <function shutdown_compile_workers at 0x78d06c5e5b40>
Traceback (most recent call last):
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/_inductor/async_compile.py", line 113, in shutdown_compile_workers
    pool.shutdown()
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/_inductor/compile_worker/subproc_pool.py", line 239, in shutdown
    self.process.wait(300)
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/subprocess.py", line 1209, in wait
    return self._wait(timeout=timeout)
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/subprocess.py", line 1937, in _wait
    time.sleep(delay)
KeyboardInterrupt:
