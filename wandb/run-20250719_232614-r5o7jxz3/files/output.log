
Detailed parameter count:
  total                  | Total:   52,149,504 | Trainable:    8,207,616
  token_embeddings       | Total:    2,451,456 | Trainable:    2,451,456
  position_embeddings    | Total:            0 | Trainable:            0
  attention_layers       | Total:    7,520,256 | Trainable:    2,211,840
  feed_forward_layers    | Total:    3,538,944 | Trainable:    3,538,944
  layer_norms            | Total:        4,608 | Trainable:        4,608
  final_layer_norm       | Total:          768 | Trainable:          768
  language_model_head    | Total:   38,633,472 | Trainable:            0
------------------------------------------------------------
step 0: train loss 10.9682, val loss 10.9694
iter 0: loss 10.9683, lr 0.00005, time 7079.31ms, mfu -100.00%
iter 10: loss 9.5680, lr 0.00055, time 554.66ms, mfu 33.32%
iter 20: loss 9.3971, lr 0.00104, time 560.90ms, mfu 33.28%
iter 30: loss 9.3252, lr 0.00154, time 567.68ms, mfu 33.21%
iter 40: loss 9.2079, lr 0.00204, time 562.94ms, mfu 33.17%
iter 50: loss 9.0187, lr 0.00254, time 578.83ms, mfu 33.05%
iter 60: loss 8.9538, lr 0.00303, time 576.67ms, mfu 32.95%
iter 70: loss 8.7956, lr 0.00353, time 585.15ms, mfu 32.81%
iter 80: loss 8.6762, lr 0.00403, time 596.22ms, mfu 32.63%
iter 90: loss 8.6379, lr 0.00453, time 595.75ms, mfu 32.47%
step 100: train loss 8.5565, val loss 8.5895
saving checkpoint to out

=== SCALING OPERATION TRIGGERED (DDP SYNC) ===
Operation: merge_lora_weights
Trigger reason: Timeout
Current val loss: 8.5895, Trigger loss: 1.0000
Iterations since last op: 100, Max wait: 100
Executing operation: merge_lora_weights with value: None
Performing architectural operation: merge_lora_weights
Merging LoRA weights into main weights...
LoRA weights merged and reset.

Detailed parameter count:
  total                  | Total:   52,149,504 | Trainable:    8,207,616
  token_embeddings       | Total:    2,451,456 | Trainable:    2,451,456
  position_embeddings    | Total:            0 | Trainable:            0
  attention_layers       | Total:    7,520,256 | Trainable:    2,211,840
  feed_forward_layers    | Total:    3,538,944 | Trainable:    3,538,944
  layer_norms            | Total:        4,608 | Trainable:        4,608
  final_layer_norm       | Total:          768 | Trainable:          768
  language_model_head    | Total:   38,633,472 | Trainable:            0
------------------------------------------------------------
Re-configuring optimizer after architectural change...
num decayed parameter tensors: 17, with 8,202,240 parameters
num non-decayed parameter tensors: 7, with 5,376 parameters
using fused AdamW: True
Transferring optimizer state for existing parameters...
Transferred optimizer state for 0 / 28 parameters
Re-compiling the model...
Architectural operation completed successfully.
Re-evaluating validation loss after operation...
New val loss after operation: 8.6231
iter 100: loss 8.6154, lr 0.00502, time 7032.19ms, mfu 29.48%
iter 110: loss 8.6590, lr 0.00552, time 580.03ms, mfu 29.72%
iter 120: loss 8.6352, lr 0.00602, time 568.44ms, mfu 30.00%
iter 130: loss 8.4137, lr 0.00652, time 578.94ms, mfu 30.19%
iter 140: loss 8.2934, lr 0.00701, time 580.54ms, mfu 30.36%
iter 150: loss 8.3744, lr 0.00751, time 577.80ms, mfu 30.52%
iter 160: loss 8.1393, lr 0.00801, time 580.89ms, mfu 30.65%
iter 170: loss 8.1873, lr 0.00851, time 578.61ms, mfu 30.78%
iter 180: loss 8.0579, lr 0.00900, time 580.88ms, mfu 30.88%
iter 190: loss 8.0755, lr 0.00950, time 583.41ms, mfu 30.96%
step 200: train loss 7.9435, val loss 7.9870
saving checkpoint to out

=== SCALING OPERATION TRIGGERED (DDP SYNC) ===
Operation: change_lr
Trigger reason: Loss threshold
Current val loss: 7.9870, Trigger loss: 100.0000
Iterations since last op: 100, Max wait: 1
Executing operation: change_lr with value: 0.5
LR multiplier: 10.0000 -> 5.0000
Re-evaluating validation loss after operation...
New val loss after operation: 7.9707
iter 200: loss 7.8728, lr 0.01000, time 7151.42ms, mfu 28.12%
iter 210: loss 8.0268, lr 0.00500, time 586.41ms, mfu 28.46%
iter 220: loss 7.9496, lr 0.00500, time 594.94ms, mfu 28.72%
iter 230: loss 7.7885, lr 0.00500, time 592.15ms, mfu 28.97%
iter 240: loss 7.8035, lr 0.00500, time 585.46ms, mfu 29.23%
iter 250: loss 7.6687, lr 0.00500, time 593.39ms, mfu 29.42%
iter 260: loss 7.8760, lr 0.00500, time 592.70ms, mfu 29.60%
iter 270: loss 7.8296, lr 0.00500, time 589.86ms, mfu 29.77%
iter 280: loss 7.6457, lr 0.00500, time 593.12ms, mfu 29.91%
iter 290: loss 7.6092, lr 0.00500, time 590.31ms, mfu 30.05%
step 300: train loss 7.5902, val loss 7.6125
saving checkpoint to out

=== SCALING OPERATION TRIGGERED (DDP SYNC) ===
Operation: reset_lr_schedule
Trigger reason: Loss threshold
Current val loss: 7.6125, Trigger loss: 100.0000
Iterations since last op: 100, Max wait: 1
Executing operation: reset_lr_schedule with value: None
LR schedule offset: 0 -> 300
=== SCALING OPERATION COMPLETE ===

iter 300: loss 7.5300, lr 0.00500, time 4953.49ms, mfu 27.42%
iter 310: loss 7.5953, lr 0.00027, time 575.99ms, mfu 27.88%
iter 320: loss 7.5705, lr 0.00052, time 582.26ms, mfu 28.27%
iter 330: loss 7.5358, lr 0.00077, time 582.47ms, mfu 28.62%
iter 340: loss 7.4840, lr 0.00102, time 584.34ms, mfu 28.92%
iter 350: loss 7.6060, lr 0.00127, time 579.10ms, mfu 29.22%
Traceback (most recent call last):
  File "/teamspace/studios/this_studio/nanoGPT/train.py", line 635, in <module>
    scaler.scale(loss).backward()
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/_tensor.py", line 648, in backward
    torch.autograd.backward(
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/autograd/__init__.py", line 353, in backward
    _engine_run_backward(
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/autograd/graph.py", line 824, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
KeyboardInterrupt
Traceback (most recent call last):
  File "/teamspace/studios/this_studio/nanoGPT/train.py", line 635, in <module>
    scaler.scale(loss).backward()
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/_tensor.py", line 648, in backward
    torch.autograd.backward(
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/autograd/__init__.py", line 353, in backward
    _engine_run_backward(
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/autograd/graph.py", line 824, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
KeyboardInterrupt
