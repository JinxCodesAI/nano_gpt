Calculating target architecture based on schedule...

============================================================
       TARGET MODEL ARCHITECTURE (at end of schedule)
============================================================
  n_layer                | 2
  n_head                 | 12
  n_embd                 | 768
  n_hidden               | 1536
  block_size             | 1024
  vocab_size             | 50304
  dropout                | 0.0
  bias                   | False
  embedding_mode         | standard
  attn_lora_rank         | 0
  embedding_rank         | 0
  lora_alpha             | 0.0
============================================================


============================================================
           INITIAL MODEL ARCHITECTURE (at Iter 0)
============================================================
  n_layer                | 1
  n_head                 | 12
  n_embd                 | 768
  n_hidden               | 768
  block_size             | 1024
  vocab_size             | 50304
  dropout                | 0.0
  bias                   | False
  embedding_mode         | lora
  attn_lora_rank         | 48
  embedding_rank         | 48
  lora_alpha             | 1.0
============================================================


Detailed parameter count:
  total                  | Total:   44,773,632 | Trainable:    4,370,688
  token_embeddings       | Total:    2,451,456 | Trainable:    2,451,456
  position_embeddings    | Total:            0 | Trainable:            0
  attention_layers       | Total:    2,506,752 | Trainable:      737,280
  feed_forward_layers    | Total:    1,179,648 | Trainable:    1,179,648
  layer_norms            | Total:        1,536 | Trainable:        1,536
  final_layer_norm       | Total:          768 | Trainable:          768
  language_model_head    | Total:   38,633,472 | Trainable:            0
------------------------------------------------------------
eval every:200
step 1800: train loss 8.1403, val loss 8.1915
  MLP Rank Utilization (L0): 68.62% (527/768)
Attn LoRA    | 2/48            | 4.17%
--- Model Analysis ---
  Embedding Utilization (L0): 97.53% (749/768)
Embed LoRA   | 2/48            | 4.17%
  Average Attention Entropy:  5.7120
----------------------
merge_lora_weights 8.191465377807617 4.0 10500
iter 1800: loss 8.2195, lr 0.00002, time 7199.21ms, mfu -100.00%
iter 1810: loss 8.1869, lr 0.00002, time 1744.59ms, mfu 34.53%
iter 1820: loss 8.1473, lr 0.00002, time 1730.34ms, mfu 34.56%
iter 1830: loss 8.2063, lr 0.00002, time 1725.31ms, mfu 34.60%
iter 1840: loss 8.1945, lr 0.00002, time 1713.28ms, mfu 34.65%
iter 1850: loss 8.1286, lr 0.00002, time 1705.86ms, mfu 34.72%
iter 1860: loss 8.0756, lr 0.00002, time 1722.46ms, mfu 34.75%
iter 1870: loss 8.1870, lr 0.00002, time 1728.20ms, mfu 34.76%
iter 1880: loss 8.2768, lr 0.00002, time 1726.70ms, mfu 34.77%
iter 1890: loss 8.1083, lr 0.00002, time 1725.30ms, mfu 34.79%
iter 1900: loss 8.1326, lr 0.00002, time 1720.50ms, mfu 34.81%
iter 1910: loss 8.1685, lr 0.00002, time 1719.56ms, mfu 34.83%
iter 1920: loss 8.2531, lr 0.00002, time 1722.17ms, mfu 34.85%
iter 1930: loss 8.1811, lr 0.00002, time 1726.30ms, mfu 34.85%
iter 1940: loss 8.2110, lr 0.00002, time 1726.55ms, mfu 34.86%
iter 1950: loss 8.1417, lr 0.00002, time 1721.76ms, mfu 34.87%
iter 1960: loss 8.3570, lr 0.00002, time 1728.00ms, mfu 34.87%
iter 1970: loss 8.1400, lr 0.00002, time 1725.02ms, mfu 34.87%
iter 1980: loss 8.1767, lr 0.00002, time 1721.48ms, mfu 34.89%
iter 1990: loss 8.1089, lr 0.00002, time 1722.32ms, mfu 34.90%
step 2000: train loss 8.1902, val loss 8.1756
  MLP Rank Utilization (L0): 68.62% (527/768)
Attn LoRA    | 2/48            | 4.17%
--- Model Analysis ---
  Embedding Utilization (L0): 97.53% (749/768)
Embed LoRA   | 2/48            | 4.17%
  Average Attention Entropy:  5.6675
----------------------
merge_lora_weights 8.175553321838379 4.0 10500
iter 2000: loss 8.2479, lr 0.00002, time 3278.17ms, mfu 33.24%
iter 2010: loss 8.2136, lr 0.00002, time 1723.13ms, mfu 33.42%
Traceback (most recent call last):
  File "/teamspace/studios/this_studio/nanoGPT/train.py", line 856, in <module>
    torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/nn/utils/clip_grad.py", line 38, in _no_grad_wrapper
    return func(*args, **kwargs)
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/nn/utils/clip_grad.py", line 220, in clip_grad_norm_
    _clip_grads_with_norm_(parameters, max_norm, total_norm, foreach)
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/nn/utils/clip_grad.py", line 38, in _no_grad_wrapper
    return func(*args, **kwargs)
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/nn/utils/clip_grad.py", line 168, in _clip_grads_with_norm_
    torch._foreach_mul_(device_grads, clip_coef_clamped.to(device))
KeyboardInterrupt
Traceback (most recent call last):
  File "/teamspace/studios/this_studio/nanoGPT/train.py", line 856, in <module>
    torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/nn/utils/clip_grad.py", line 38, in _no_grad_wrapper
    return func(*args, **kwargs)
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/nn/utils/clip_grad.py", line 220, in clip_grad_norm_
    _clip_grads_with_norm_(parameters, max_norm, total_norm, foreach)
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/nn/utils/clip_grad.py", line 38, in _no_grad_wrapper
    return func(*args, **kwargs)
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/nn/utils/clip_grad.py", line 168, in _clip_grads_with_norm_
    torch._foreach_mul_(device_grads, clip_coef_clamped.to(device))
KeyboardInterrupt
