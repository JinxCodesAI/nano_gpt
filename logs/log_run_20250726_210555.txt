[2025-07-26 21:05:55] ================================================================================

[2025-07-26 21:05:55] Training run started at 2025-07-26 21:05:55

[2025-07-26 21:05:55] ================================================================================

      Configuration:

      ----------------------------------------

      RARE_TOKEN_ID: None

      always_save_checkpoint: True

      attn_lora_rank: 0

      backend: nccl

      batch_size: 8

      beta1: 0.9

      beta2: 0.95

      bias: False

      block_size: 64

      compile: False

      dataset: fineweb10B

      decay_lr: True

      device: cpu

      dropout: 0.0

      dtype: bfloat16

      embedding_mode: standard

      embedding_rank: 0

      eval_interval: 50

      eval_iters: 10

      eval_only: False

      file_logging: True

      grad_clip: 1.0

      gradient_accumulation_steps: 4

      ignored_outlayers_sum: 0.01

      init_from: scratch

      learning_rate: 0.0006

      log_dir: logs

      log_interval: 5

      lora_alpha: 1.0

      lr_decay_iters: 300

      max_iters: 300

      min_lr: 6e-05

      n_embd: 128

      n_head: 4

      n_hidden: 256

      n_layer: 1

      num_train_shards: 103

      out_dir: test-base

      rotary_base: 10000.0

      rotary_max_position_embeddings: 2048

      scaling_schedule: []

      scaling_schedule_file: configs/test_base_example.json

      shrunken_vocab_size: None

      target_architecture_config: None

      train_shard_filenames: ['fineweb_train_000001.bin', 'fineweb_train_000002.bin', 'fineweb_train_000003.bin', 'fineweb_train_000004.bin', 'fineweb_train_000005.bin', 'fineweb_train_000006.bin', 'fineweb_train_000007.bin', 'fineweb_train_000008.bin', 'fineweb_train_000009.bin', 'fineweb_train_000010.bin', 'fineweb_train_000011.bin', 'fineweb_train_000012.bin', 'fineweb_train_000013.bin', 'fineweb_train_000014.bin', 'fineweb_train_000015.bin', 'fineweb_train_000016.bin', 'fineweb_train_000017.bin', 'fineweb_train_000018.bin', 'fineweb_train_000019.bin', 'fineweb_train_000020.bin', 'fineweb_train_000021.bin', 'fineweb_train_000022.bin', 'fineweb_train_000023.bin', 'fineweb_train_000024.bin', 'fineweb_train_000025.bin', 'fineweb_train_000026.bin', 'fineweb_train_000027.bin', 'fineweb_train_000028.bin', 'fineweb_train_000029.bin', 'fineweb_train_000030.bin', 'fineweb_train_000031.bin', 'fineweb_train_000032.bin', 'fineweb_train_000033.bin', 'fineweb_train_000034.bin', 'fineweb_train_000035.bin', 'fineweb_train_000036.bin', 'fineweb_train_000037.bin', 'fineweb_train_000038.bin', 'fineweb_train_000039.bin', 'fineweb_train_000040.bin', 'fineweb_train_000041.bin', 'fineweb_train_000042.bin', 'fineweb_train_000043.bin', 'fineweb_train_000044.bin', 'fineweb_train_000045.bin', 'fineweb_train_000046.bin', 'fineweb_train_000047.bin', 'fineweb_train_000048.bin', 'fineweb_train_000049.bin', 'fineweb_train_000050.bin', 'fineweb_train_000051.bin', 'fineweb_train_000052.bin', 'fineweb_train_000053.bin', 'fineweb_train_000054.bin', 'fineweb_train_000055.bin', 'fineweb_train_000056.bin', 'fineweb_train_000057.bin', 'fineweb_train_000058.bin', 'fineweb_train_000059.bin', 'fineweb_train_000060.bin', 'fineweb_train_000061.bin', 'fineweb_train_000062.bin', 'fineweb_train_000063.bin', 'fineweb_train_000064.bin', 'fineweb_train_000065.bin', 'fineweb_train_000066.bin', 'fineweb_train_000067.bin', 'fineweb_train_000068.bin', 'fineweb_train_000069.bin', 'fineweb_train_000070.bin', 'fineweb_train_000071.bin', 'fineweb_train_000072.bin', 'fineweb_train_000073.bin', 'fineweb_train_000074.bin', 'fineweb_train_000075.bin', 'fineweb_train_000076.bin', 'fineweb_train_000077.bin', 'fineweb_train_000078.bin', 'fineweb_train_000079.bin', 'fineweb_train_000080.bin', 'fineweb_train_000081.bin', 'fineweb_train_000082.bin', 'fineweb_train_000083.bin', 'fineweb_train_000084.bin', 'fineweb_train_000085.bin', 'fineweb_train_000086.bin', 'fineweb_train_000087.bin', 'fineweb_train_000088.bin', 'fineweb_train_000089.bin', 'fineweb_train_000090.bin', 'fineweb_train_000091.bin', 'fineweb_train_000092.bin', 'fineweb_train_000093.bin', 'fineweb_train_000094.bin', 'fineweb_train_000095.bin', 'fineweb_train_000096.bin', 'fineweb_train_000097.bin', 'fineweb_train_000098.bin', 'fineweb_train_000099.bin', 'fineweb_train_000100.bin', 'fineweb_train_000101.bin', 'fineweb_train_000102.bin', 'fineweb_train_000103.bin']

      use_rotary_embeddings: False

      vocab_remapping_file: None

      wandb_log: False

      wandb_project: owt-test

      wandb_run_name: orchestrator-test

      warmup_iters: 2000

      weight_decay: 0.1

      ----------------------------------------

[2025-07-26 21:05:59] step 0: train loss 10.8484, val loss 10.8494, tokens/sec 1577
[2025-07-26 21:06:00] --- ASYNC ANALYSIS RESULTS FOR ITERATION 0 ---
[2025-07-26 21:06:00]   [Embeddings Geometry] Avg Neighbors: 0.00 10th-90th-99th Percentile: 0.0000 - 0.0000 - 0.0000 
[2025-07-26 21:06:00]   [Embeddings Geometry] Mean Similarity: -0.0005 Std Similarity: 0.0909 | 10th-90th Percentile: -0.0973 - 0.1259
[2025-07-26 21:06:00]   [Embeddings Analysis] Analyzed: 2110/50304 embeddings | Filtered: True
[2025-07-26 21:06:00]   [FFN Rank L0] Utilization: 88.28% (113/128)
[2025-07-26 21:06:00]   [Attn Q L0] Utilization: 71.88% (92/128)
[2025-07-26 21:06:00]   [Attn K L0] Utilization: 71.88% (92/128)
[2025-07-26 21:06:00]   [Attn V L0] Utilization: 70.31% (90/128)
[2025-07-26 21:06:00] --- END OF ASYNC ANALYSIS RESULTS ---
[2025-07-26 21:06:03] iter 0: loss 10.8511, lr 0.00000, time 1468.81ms, mfu -100.00%, VRAM 0.0/0.0GB (0.0%)
[2025-07-26 21:06:20] iter 5: loss 10.8793, lr 0.00000, time 3260.55ms, mfu 0.02%, VRAM 0.0/0.0GB (0.0%)
[2025-07-26 21:06:34] iter 10: loss 10.8493, lr 0.00000, time 2941.63ms, mfu 0.02%, VRAM 0.0/0.0GB (0.0%)
[2025-07-26 21:06:49] iter 15: loss 10.8354, lr 0.00000, time 3014.53ms, mfu 0.02%, VRAM 0.0/0.0GB (0.0%)
[2025-07-26 21:07:04] iter 20: loss 10.8341, lr 0.00001, time 2862.74ms, mfu 0.02%, VRAM 0.0/0.0GB (0.0%)
[2025-07-26 21:07:18] iter 25: loss 10.8375, lr 0.00001, time 2927.37ms, mfu 0.02%, VRAM 0.0/0.0GB (0.0%)
[2025-07-26 21:07:33] iter 30: loss 10.8321, lr 0.00001, time 2909.70ms, mfu 0.02%, VRAM 0.0/0.0GB (0.0%)
[2025-07-26 21:07:47] iter 35: loss 10.8181, lr 0.00001, time 2887.51ms, mfu 0.02%, VRAM 0.0/0.0GB (0.0%)
[2025-07-26 21:08:02] iter 40: loss 10.8341, lr 0.00001, time 2892.36ms, mfu 0.02%, VRAM 0.0/0.0GB (0.0%)
[2025-07-26 21:08:16] iter 45: loss 10.8129, lr 0.00001, time 2907.27ms, mfu 0.02%, VRAM 0.0/0.0GB (0.0%)
[2025-07-26 21:08:31] step 50: train loss 10.8198, val loss 10.8145, tokens/sec 735
[2025-07-26 21:08:31]   timing breakdown (avg last 50): evaluation 54.8%, operations 22.7%, forward_backward 22.0%, grad_clip 0.1%, optimizer_step 0.4%
[2025-07-26 21:08:37] --- ASYNC ANALYSIS RESULTS FOR ITERATION 50 ---
[2025-07-26 21:08:37]   [Embeddings Geometry] Avg Neighbors: 0.00 10th-90th-99th Percentile: 0.0000 - 0.0000 - 0.0000 
[2025-07-26 21:08:37]   [Embeddings Geometry] Mean Similarity: -0.0000 Std Similarity: 0.0888 | 10th-90th Percentile: -0.1048 - 0.1088
[2025-07-26 21:08:37]   [Embeddings Analysis] Analyzed: 15275/50304 embeddings | Filtered: True
[2025-07-26 21:08:37]   [FFN Rank L0] Utilization: 88.28% (113/128)
[2025-07-26 21:08:37]   [Attn Q L0] Utilization: 71.88% (92/128)
[2025-07-26 21:08:37]   [Attn K L0] Utilization: 71.88% (92/128)
[2025-07-26 21:08:37]   [Attn V L0] Utilization: 70.31% (90/128)
[2025-07-26 21:08:37] --- END OF ASYNC ANALYSIS RESULTS ---
[2025-07-26 21:08:38] iter 50: loss 10.8093, lr 0.00001, time 4293.85ms, mfu 0.02%, VRAM 0.0/0.0GB (0.0%)
[2025-07-26 21:08:54] iter 55: loss 10.7997, lr 0.00002, time 3172.88ms, mfu 0.02%, VRAM 0.0/0.0GB (0.0%)
[2025-07-26 21:09:08] iter 60: loss 10.7852, lr 0.00002, time 2765.88ms, mfu 0.02%, VRAM 0.0/0.0GB (0.0%)
[2025-07-26 21:09:21] iter 65: loss 10.8000, lr 0.00002, time 2712.00ms, mfu 0.02%, VRAM 0.0/0.0GB (0.0%)
[2025-07-26 21:09:35] iter 70: loss 10.7635, lr 0.00002, time 2728.20ms, mfu 0.02%, VRAM 0.0/0.0GB (0.0%)
[2025-07-26 21:09:48] iter 75: loss 10.7288, lr 0.00002, time 2757.38ms, mfu 0.02%, VRAM 0.0/0.0GB (0.0%)
[2025-07-26 21:10:03] iter 80: loss 10.7109, lr 0.00002, time 2844.36ms, mfu 0.02%, VRAM 0.0/0.0GB (0.0%)
[2025-07-26 21:10:16] iter 85: loss 10.6483, lr 0.00003, time 2752.29ms, mfu 0.02%, VRAM 0.0/0.0GB (0.0%)
[2025-07-26 21:10:30] iter 90: loss 10.6675, lr 0.00003, time 2800.60ms, mfu 0.02%, VRAM 0.0/0.0GB (0.0%)
[2025-07-26 21:10:44] iter 95: loss 10.6436, lr 0.00003, time 2726.56ms, mfu 0.02%, VRAM 0.0/0.0GB (0.0%)
[2025-07-26 21:10:58] step 100: train loss 10.6038, val loss 10.6115, tokens/sec 1480
[2025-07-26 21:10:58]   timing breakdown (avg last 50): evaluation 55.9%, operations 22.0%, forward_backward 21.6%, grad_clip 0.1%, optimizer_step 0.4%
[2025-07-26 21:11:03] iter 100: loss 10.5947, lr 0.00003, time 3682.27ms, mfu 0.02%, VRAM 0.0/0.0GB (0.0%)
[2025-07-26 21:11:04] --- ASYNC ANALYSIS RESULTS FOR ITERATION 101 ---
[2025-07-26 21:11:04]   [Embeddings Geometry] Avg Neighbors: 0.00 10th-90th-99th Percentile: 0.0000 - 0.0000 - 0.0000 
[2025-07-26 21:11:04]   [Embeddings Geometry] Mean Similarity: 0.0004 Std Similarity: 0.0887 | 10th-90th Percentile: -0.1059 - 0.1170
[2025-07-26 21:11:04]   [Embeddings Analysis] Analyzed: 20787/50304 embeddings | Filtered: True
[2025-07-26 21:11:04]   [FFN Rank L0] Utilization: 88.28% (113/128)
[2025-07-26 21:11:04]   [Attn Q L0] Utilization: 71.88% (92/128)
[2025-07-26 21:11:04]   [Attn K L0] Utilization: 71.88% (92/128)
[2025-07-26 21:11:04]   [Attn V L0] Utilization: 70.31% (90/128)
[2025-07-26 21:11:04] --- END OF ASYNC ANALYSIS RESULTS ---
[2025-07-26 21:11:19] iter 105: loss 10.5932, lr 0.00003, time 3373.25ms, mfu 0.02%, VRAM 0.0/0.0GB (0.0%)
[2025-07-26 21:11:33] iter 110: loss 10.5300, lr 0.00003, time 2786.41ms, mfu 0.02%, VRAM 0.0/0.0GB (0.0%)
[2025-07-26 21:11:47] iter 115: loss 10.4759, lr 0.00003, time 2821.47ms, mfu 0.02%, VRAM 0.0/0.0GB (0.0%)
[2025-07-26 21:12:01] iter 120: loss 10.4176, lr 0.00004, time 2738.22ms, mfu 0.02%, VRAM 0.0/0.0GB (0.0%)
