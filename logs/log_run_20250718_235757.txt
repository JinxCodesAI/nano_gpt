================================================================================
Training run started at 2025-07-18 23:57:57
================================================================================
Configuration:
----------------------------------------
always_save_checkpoint: True
backend: nccl
batch_size: 12
beta1: 0.9
beta2: 0.95
bias: False
block_size: 1024
compile: True
dataset: fineweb10B
decay_lr: True
device: cuda
dropout: 0.0
dtype: bfloat16
eval_interval: 50
eval_iters: 1
eval_only: False
file_logging: True
grad_clip: 1.0
gradient_accumulation_steps: 5
init_from: scratch
learning_rate: 0.0006
log_dir: logs
log_interval: 10
lr_decay_iters: 600000
max_iters: 600000
min_lr: 6e-05
n_embd: 192
n_head: 12
n_layer: 12
out_dir: out
rotary_base: 10000.0
rotary_max_position_embeddings: 2048
use_rotary_embeddings: True
wandb_log: True
wandb_project: owt
wandb_run_name: gpt2-124M
warmup_iters: 20
weight_decay: 0.1
----------------------------------------
[2025-07-18 23:58:17] step 0: train loss 10.8443, val loss 10.8523
[2025-07-18 23:59:00] step 50: train loss 7.4399, val loss 7.3239
[2025-07-18 23:59:19] step 100: train loss 6.9632, val loss 6.9664
[2025-07-18 23:59:38] step 150: train loss 6.7143, val loss 6.8364
[2025-07-18 23:59:57] step 200: train loss 6.3816, val loss 6.1651
[2025-07-19 00:00:17] step 250: train loss 5.9638, val loss 6.3297
[2025-07-19 00:00:37] step 300: train loss 6.1101, val loss 6.2056
