================================================================================
Training run started at 2025-07-20 21:29:20
================================================================================
Configuration:
----------------------------------------
always_save_checkpoint: False
attn_lora_rank: 0
attn_lora_rank_divisor: 0
backend: nccl
batch_size: 32
batch_size_multiplier: 1.0
beta1: 0.9
beta2: 0.95
bias: False
block_size: 1024
compile: True
dataset: fineweb10B
decay_lr: True
device: cuda
dropout: 0.0
dtype: bfloat16
embedding_mode: lora
embedding_rank: 0
eval_interval: 200
eval_interval_multiplier: 1.0
eval_iters: 1
eval_iters_multiplier: 1.0
eval_only: False
file_logging: True
grad_accum_multiplier: 1.0
grad_clip: 1.0
gradient_accumulation_steps: 2
init_from: resume
learning_rate: 0.0001
log_dir: logs
log_interval: 10
lora_alpha: 1.0
lora_alpha_multiplier: 1.0
lr_decay_iters: 600000
lr_multiplier: 1
max_iters: 600000
min_lr: 6e-05
n_embd: 768
n_head: 12
n_hidden_divisor: 0
n_layer: 1
n_layer_divisor: 0
out_dir: out
rotary_base: 10000.0
rotary_max_position_embeddings: 2048
use_rotary_embeddings: True
vocab_lora_rank_divisor: 0
wandb_log: True
wandb_project: owt
wandb_run_name: gpt2
warmup_iters: 200
warmup_iters_multiplier: 1.0
weight_decay: 0.1
----------------------------------------
[2025-07-20 21:29:29] step 400: train loss 6.4513, val loss 6.3592
[2025-07-20 21:29:30] OPERATION_START: reset_lr_schedule  | iter=400 | value=None | trigger=Loss threshold | val_loss=6.3592 | trigger_loss=100.0000 | max_wait=1
[2025-07-20 21:29:30] OPERATION_SUCCESS: reset_lr_schedule | iter=400 | old=0 | new=400
[2025-07-20 21:29:30] OPERATION_START: change_batch_size  | iter=400 | value=0.5 | trigger=Loss threshold | val_loss=6.3592 | trigger_loss=100.0000 | max_wait=1
[2025-07-20 21:29:30] OPERATION_SUCCESS: change_batch_size | iter=400 | old=32 | new=16
[2025-07-20 21:29:30] OPERATION_START: change_grad_accum  | iter=400 | value=2.0 | trigger=Loss threshold | val_loss=6.3592 | trigger_loss=100.0000 | max_wait=1
[2025-07-20 21:29:30] OPERATION_SUCCESS: change_grad_accum | iter=400 | old=2 | new=4
[2025-07-20 21:29:30] OPERATION_START: stack_layers first burn | iter=400 | value=2 | trigger=Loss threshold | val_loss=6.3592 | trigger_loss=100.0000 | max_wait=1
[2025-07-20 21:29:30] OPERATION_SUCCESS: stack_layers | iter=400 | old_divisor=0 | new_divisor=0.0 | new_layers=2
[2025-07-20 21:29:38] OPERATION_REEVALUATION: stack_layers | iter=400 | old_val_loss=6.3592 | new_val_loss=6.4803 | change=+0.1212
[2025-07-20 21:29:38] step 400: train loss 6.7073, val loss 6.4803
[2025-07-20 21:31:57] step 600: train loss 6.1310, val loss 6.3114
[2025-07-20 21:31:58] OPERATION_START: change_lr  | iter=600 | value=0.5 | trigger=Loss threshold | val_loss=6.3114 | trigger_loss=100.0000 | max_wait=1
[2025-07-20 21:31:58] OPERATION_SUCCESS: change_lr | iter=600 | old=1 | new=0.5
[2025-07-20 21:31:58] OPERATION_START: reset_lr_schedule  | iter=600 | value=None | trigger=Loss threshold | val_loss=6.3114 | trigger_loss=100.0000 | max_wait=1
[2025-07-20 21:31:58] OPERATION_SUCCESS: reset_lr_schedule | iter=600 | old=400 | new=600
[2025-07-20 21:34:00] step 800: train loss 6.0283, val loss 5.9134
[2025-07-20 21:36:03] step 1000: train loss 5.9325, val loss 5.9461
