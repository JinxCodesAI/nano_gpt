
Detailed parameter count:
  total                  | Total:   52,149,504 | Trainable:    8,207,616
  token_embeddings       | Total:    2,451,456 | Trainable:    2,451,456
  position_embeddings    | Total:            0 | Trainable:            0
  attention_layers       | Total:    7,520,256 | Trainable:    2,211,840
  feed_forward_layers    | Total:    3,538,944 | Trainable:    3,538,944
  layer_norms            | Total:        4,608 | Trainable:        4,608
  final_layer_norm       | Total:          768 | Trainable:          768
  language_model_head    | Total:   38,633,472 | Trainable:            0
------------------------------------------------------------
step 0: train loss 10.9662, val loss 10.9694
iter 0: loss 10.9692, lr 0.00005, time 7305.77ms, mfu -100.00%
iter 10: loss 9.5761, lr 0.00055, time 570.79ms, mfu 32.38%
iter 20: loss 9.4952, lr 0.00104, time 570.81ms, mfu 32.38%
iter 30: loss 9.2980, lr 0.00154, time 582.02ms, mfu 32.31%
iter 40: loss 9.2473, lr 0.00204, time 581.53ms, mfu 32.26%
iter 50: loss 8.9699, lr 0.00254, time 590.23ms, mfu 32.17%
iter 60: loss 8.9511, lr 0.00303, time 589.17ms, mfu 32.09%
iter 70: loss 8.6607, lr 0.00353, time 593.11ms, mfu 31.99%
iter 80: loss 8.7721, lr 0.00403, time 581.73ms, mfu 31.97%
iter 90: loss 8.6106, lr 0.00453, time 579.95ms, mfu 31.96%
step 100: train loss 8.5652, val loss 8.5827
saving checkpoint to out

=== SCALING OPERATION TRIGGERED (DDP SYNC) ===
Operation: merge_lora_weights
Trigger reason: Timeout
Current val loss: 8.5827, Trigger loss: 1.0000
Iterations since last op: 100, Max wait: 100
Executing operation: merge_lora_weights with value: None
Performing architectural operation: merge_lora_weights
Merging LoRA weights into main weights...
LoRA weights merged and reset.

Detailed parameter count:
  total                  | Total:   52,149,504 | Trainable:    8,207,616
  token_embeddings       | Total:    2,451,456 | Trainable:    2,451,456
  position_embeddings    | Total:            0 | Trainable:            0
  attention_layers       | Total:    7,520,256 | Trainable:    2,211,840
  feed_forward_layers    | Total:    3,538,944 | Trainable:    3,538,944
  layer_norms            | Total:        4,608 | Trainable:        4,608
  final_layer_norm       | Total:          768 | Trainable:          768
  language_model_head    | Total:   38,633,472 | Trainable:            0
------------------------------------------------------------
Re-configuring optimizer after architectural change...
num decayed parameter tensors: 17, with 8,202,240 parameters
num non-decayed parameter tensors: 7, with 5,376 parameters
using fused AdamW: True
Transferring optimizer state for existing parameters...
Transferred optimizer state for 0 / 28 parameters
Re-compiling the model...
Architectural operation completed successfully.
=== SCALING OPERATION COMPLETE ===


=== SCALING OPERATION TRIGGERED (DDP SYNC) ===
Operation: change_lr
Trigger reason: Loss threshold
Current val loss: 8.5827, Trigger loss: 100.0000
Iterations since last op: 0, Max wait: 1
Executing operation: change_lr with value: 0.5
LR multiplier: 10.0000 -> 5.0000
=== SCALING OPERATION COMPLETE ===


=== SCALING OPERATION TRIGGERED (DDP SYNC) ===
Operation: reset_lr_schedule
Trigger reason: Loss threshold
Current val loss: 8.5827, Trigger loss: 100.0000
Iterations since last op: 0, Max wait: 1
Executing operation: reset_lr_schedule with value: None
LR schedule offset: 0 -> 100
=== SCALING OPERATION COMPLETE ===

iter 100: loss 8.5725, lr 0.00502, time 4956.82ms, mfu 29.14%
iter 110: loss 8.5524, lr 0.00027, time 573.57ms, mfu 29.45%
iter 120: loss 8.5514, lr 0.00052, time 579.28ms, mfu 29.69%
iter 130: loss 8.5690, lr 0.00077, time 573.60ms, mfu 29.94%
iter 140: loss 8.3688, lr 0.00102, time 576.11ms, mfu 30.16%
iter 150: loss 8.3177, lr 0.00127, time 577.44ms, mfu 30.34%
iter 160: loss 8.3986, lr 0.00152, time 576.85ms, mfu 30.51%
iter 170: loss 8.3490, lr 0.00177, time 582.59ms, mfu 30.63%
iter 180: loss 8.3795, lr 0.00201, time 585.39ms, mfu 30.73%
iter 190: loss 8.1805, lr 0.00226, time 584.34ms, mfu 30.82%
step 200: train loss 8.1466, val loss 8.1715
saving checkpoint to out

=== SCALING OPERATION TRIGGERED (DDP SYNC) ===
Operation: merge_lora_weights
Trigger reason: Timeout
Current val loss: 8.1715, Trigger loss: 1.0000
Iterations since last op: 100, Max wait: 100
Executing operation: merge_lora_weights with value: None
Performing architectural operation: merge_lora_weights
Merging LoRA weights into main weights...
LoRA weights merged and reset.

Detailed parameter count:
  total                  | Total:   52,149,504 | Trainable:    8,207,616
  token_embeddings       | Total:    2,451,456 | Trainable:    2,451,456
  position_embeddings    | Total:            0 | Trainable:            0
  attention_layers       | Total:    7,520,256 | Trainable:    2,211,840
  feed_forward_layers    | Total:    3,538,944 | Trainable:    3,538,944
  layer_norms            | Total:        4,608 | Trainable:        4,608
  final_layer_norm       | Total:          768 | Trainable:          768
  language_model_head    | Total:   38,633,472 | Trainable:            0
------------------------------------------------------------
Re-configuring optimizer after architectural change...
num decayed parameter tensors: 17, with 8,202,240 parameters
num non-decayed parameter tensors: 7, with 5,376 parameters
using fused AdamW: True
Transferring optimizer state for existing parameters...
Transferred optimizer state for 0 / 28 parameters
Re-compiling the model...
Architectural operation completed successfully.
=== SCALING OPERATION COMPLETE ===


=== SCALING OPERATION TRIGGERED (DDP SYNC) ===
Operation: change_warmup_iters
Trigger reason: Loss threshold
Current val loss: 8.1715, Trigger loss: 100.0000
Iterations since last op: 0, Max wait: 1
Executing operation: change_warmup_iters with value: 2.0
Warmup iters multiplier: 1.0000 -> 2.0000
=== SCALING OPERATION COMPLETE ===


=== SCALING OPERATION TRIGGERED (DDP SYNC) ===
Operation: reset_lr_schedule
Trigger reason: Loss threshold
Current val loss: 8.1715, Trigger loss: 100.0000
Iterations since last op: 0, Max wait: 1
Executing operation: reset_lr_schedule with value: None
LR schedule offset: 100 -> 200
=== SCALING OPERATION COMPLETE ===


=== SCALING OPERATION TRIGGERED (DDP SYNC) ===
Operation: change_batch_size
Trigger reason: Loss threshold
Current val loss: 8.1715, Trigger loss: 100.0000
Iterations since last op: 0, Max wait: 1
Executing operation: change_batch_size with value: 0.5
Batch size: 32 -> 16
=== SCALING OPERATION COMPLETE ===


=== SCALING OPERATION TRIGGERED (DDP SYNC) ===
Operation: change_grad_accum
Trigger reason: Loss threshold
Current val loss: 8.1715, Trigger loss: 100.0000
Iterations since last op: 0, Max wait: 1
Executing operation: change_grad_accum with value: 2.0
Grad accum steps: 2 -> 4
=== SCALING OPERATION COMPLETE ===


=== SCALING OPERATION TRIGGERED (DDP SYNC) ===
Operation: stack_layers
Trigger reason: Loss threshold
Current val loss: 8.1715, Trigger loss: 100.0000
Iterations since last op: 0, Max wait: 1
Executing operation: stack_layers with value: 2
Performing architectural operation: stack_layers
Stacking layers: current depth 3, creating 6 total layers.
Model now has 6 layers.

Detailed parameter count:
  total                  | Total:   63,213,312 | Trainable:   13,963,008
  token_embeddings       | Total:    2,451,456 | Trainable:    2,451,456
  position_embeddings    | Total:            0 | Trainable:            0
  attention_layers       | Total:   15,040,512 | Trainable:    4,423,680
  feed_forward_layers    | Total:    7,077,888 | Trainable:    7,077,888
  layer_norms            | Total:        9,216 | Trainable:        9,216
  final_layer_norm       | Total:          768 | Trainable:          768
  language_model_head    | Total:   38,633,472 | Trainable:            0
------------------------------------------------------------
Re-configuring optimizer after architectural change...
num decayed parameter tensors: 32, with 13,953,024 parameters
num non-decayed parameter tensors: 13, with 9,984 parameters
using fused AdamW: True
Transferring optimizer state for existing parameters...
Transferred optimizer state for 0 / 52 parameters
Re-compiling the model...
Architectural operation completed successfully.
Re-evaluating validation loss after operation...
W0719 23:46:42.502000 268745 /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/_inductor/utils.py:1250] [0/2] Not enough SMs to use max_autotune_gemm mode
New val loss after operation: 8.9241
iter 200: loss 8.8080, lr 0.00251, time 44788.99ms, mfu 27.79%
iter 210: loss 8.2913, lr 0.00014, time 777.24ms, mfu 28.05%
iter 220: loss 8.2776, lr 0.00026, time 779.98ms, mfu 28.27%
iter 230: loss 8.2297, lr 0.00039, time 787.42ms, mfu 28.44%
iter 240: loss 8.1911, lr 0.00051, time 778.77ms, mfu 28.63%
iter 250: loss 8.0272, lr 0.00064, time 774.90ms, mfu 28.81%
iter 260: loss 8.0548, lr 0.00076, time 766.35ms, mfu 29.01%
iter 270: loss 8.0372, lr 0.00089, time 760.08ms, mfu 29.22%
iter 280: loss 7.7761, lr 0.00101, time 765.52ms, mfu 29.38%
iter 290: loss 7.9323, lr 0.00113, time 761.83ms, mfu 29.54%
step 300: train loss 7.9452, val loss 8.0036
saving checkpoint to out

=== SCALING OPERATION TRIGGERED (DDP SYNC) ===
Operation: merge_lora_weights
Trigger reason: Timeout
Current val loss: 8.0036, Trigger loss: 1.0000
Iterations since last op: 100, Max wait: 100
Executing operation: merge_lora_weights with value: None
Performing architectural operation: merge_lora_weights
Merging LoRA weights into main weights...
LoRA weights merged and reset.

Detailed parameter count:
  total                  | Total:   63,213,312 | Trainable:   13,963,008
  token_embeddings       | Total:    2,451,456 | Trainable:    2,451,456
  position_embeddings    | Total:            0 | Trainable:            0
  attention_layers       | Total:   15,040,512 | Trainable:    4,423,680
  feed_forward_layers    | Total:    7,077,888 | Trainable:    7,077,888
  layer_norms            | Total:        9,216 | Trainable:        9,216
  final_layer_norm       | Total:          768 | Trainable:          768
  language_model_head    | Total:   38,633,472 | Trainable:            0
------------------------------------------------------------
Re-configuring optimizer after architectural change...
num decayed parameter tensors: 32, with 13,953,024 parameters
num non-decayed parameter tensors: 13, with 9,984 parameters
using fused AdamW: True
Transferring optimizer state for existing parameters...
Transferred optimizer state for 0 / 52 parameters
Re-compiling the model...
Architectural operation completed successfully.
=== SCALING OPERATION COMPLETE ===

iter 300: loss 7.8787, lr 0.00126, time 3524.68ms, mfu 27.26%
iter 310: loss 8.1439, lr 0.00138, time 766.00ms, mfu 27.61%
iter 320: loss 7.8876, lr 0.00151, time 771.71ms, mfu 27.91%
iter 330: loss 7.9077, lr 0.00163, time 774.56ms, mfu 28.17%
iter 340: loss 7.8270, lr 0.00176, time 779.24ms, mfu 28.38%
iter 350: loss 7.7995, lr 0.00188, time 781.36ms, mfu 28.56%
iter 360: loss 7.8629, lr 0.00201, time 778.31ms, mfu 28.74%
iter 370: loss 7.8903, lr 0.00213, time 780.33ms, mfu 28.89%
iter 380: loss 7.7322, lr 0.00226, time 775.74ms, mfu 29.05%
iter 390: loss 7.9092, lr 0.00238, time 772.53ms, mfu 29.20%
step 400: train loss 7.7196, val loss 7.6987
saving checkpoint to out

=== SCALING OPERATION TRIGGERED (DDP SYNC) ===
Operation: merge_lora_weights
Trigger reason: Timeout
Current val loss: 7.6987, Trigger loss: 1.0000
Iterations since last op: 100, Max wait: 100
Executing operation: merge_lora_weights with value: None
Performing architectural operation: merge_lora_weights
Merging LoRA weights into main weights...
LoRA weights merged and reset.

Detailed parameter count:
  total                  | Total:   63,213,312 | Trainable:   13,963,008
  token_embeddings       | Total:    2,451,456 | Trainable:    2,451,456
  position_embeddings    | Total:            0 | Trainable:            0
  attention_layers       | Total:   15,040,512 | Trainable:    4,423,680
  feed_forward_layers    | Total:    7,077,888 | Trainable:    7,077,888
  layer_norms            | Total:        9,216 | Trainable:        9,216
  final_layer_norm       | Total:          768 | Trainable:          768
  language_model_head    | Total:   38,633,472 | Trainable:            0
------------------------------------------------------------
Re-configuring optimizer after architectural change...
num decayed parameter tensors: 32, with 13,953,024 parameters
num non-decayed parameter tensors: 13, with 9,984 parameters
using fused AdamW: True
Transferring optimizer state for existing parameters...
Transferred optimizer state for 0 / 52 parameters
Re-compiling the model...
Architectural operation completed successfully.
=== SCALING OPERATION COMPLETE ===


=== SCALING OPERATION TRIGGERED (DDP SYNC) ===
Operation: change_batch_size
Trigger reason: Loss threshold
Current val loss: 7.6987, Trigger loss: 100.0000
Iterations since last op: 0, Max wait: 1
Executing operation: change_batch_size with value: 0.5
Batch size: 16 -> 8
=== SCALING OPERATION COMPLETE ===


=== SCALING OPERATION TRIGGERED (DDP SYNC) ===
Operation: change_grad_accum
Trigger reason: Loss threshold
Current val loss: 7.6987, Trigger loss: 100.0000
Iterations since last op: 0, Max wait: 1
Executing operation: change_grad_accum with value: 2.0
Grad accum steps: 4 -> 8
=== SCALING OPERATION COMPLETE ===


=== SCALING OPERATION TRIGGERED (DDP SYNC) ===
Operation: stack_layers
Trigger reason: Loss threshold
Current val loss: 7.6987, Trigger loss: 100.0000
Iterations since last op: 0, Max wait: 1
Executing operation: stack_layers with value: 2
Performing architectural operation: stack_layers
Stacking layers: current depth 6, creating 12 total layers.
Model now has 12 layers.

Detailed parameter count:
  total                  | Total:   85,340,928 | Trainable:   25,473,792
  token_embeddings       | Total:    2,451,456 | Trainable:    2,451,456
  position_embeddings    | Total:            0 | Trainable:            0
  attention_layers       | Total:   30,081,024 | Trainable:    8,847,360
  feed_forward_layers    | Total:   14,155,776 | Trainable:   14,155,776
  layer_norms            | Total:       18,432 | Trainable:       18,432
  final_layer_norm       | Total:          768 | Trainable:          768
  language_model_head    | Total:   38,633,472 | Trainable:            0
------------------------------------------------------------
Re-configuring optimizer after architectural change...
num decayed parameter tensors: 62, with 25,454,592 parameters
num non-decayed parameter tensors: 25, with 19,200 parameters
using fused AdamW: True
Transferring optimizer state for existing parameters...
Transferred optimizer state for 0 / 100 parameters
Re-compiling the model...
Architectural operation completed successfully.
Re-evaluating validation loss after operation...
New val loss after operation: 7.9785
iter 400: loss 8.1899, lr 0.00251, time 67817.28ms, mfu 26.33%
iter 410: loss 8.2000, lr 0.00263, time 1184.53ms, mfu 26.55%
iter 420: loss 7.5229, lr 0.00276, time 1190.91ms, mfu 26.74%
iter 430: loss 7.9107, lr 0.00288, time 1186.19ms, mfu 26.92%
iter 440: loss 7.5687, lr 0.00300, time 1158.94ms, mfu 27.15%
iter 450: loss 7.4563, lr 0.00313, time 1149.11ms, mfu 27.39%
iter 460: loss 7.5367, lr 0.00325, time 1150.41ms, mfu 27.59%
iter 470: loss 7.4667, lr 0.00338, time 1150.20ms, mfu 27.78%
iter 480: loss 7.7070, lr 0.00350, time 1160.40ms, mfu 27.92%
iter 490: loss 7.6118, lr 0.00363, time 1160.41ms, mfu 28.04%
step 500: train loss 7.5576, val loss 7.5626
saving checkpoint to out

=== SCALING OPERATION TRIGGERED (DDP SYNC) ===
Operation: merge_lora_weights
Trigger reason: Timeout
Current val loss: 7.5626, Trigger loss: 1.0000
Iterations since last op: 100, Max wait: 100
Executing operation: merge_lora_weights with value: None
Performing architectural operation: merge_lora_weights
Merging LoRA weights into main weights...
LoRA weights merged and reset.

Detailed parameter count:
  total                  | Total:   85,340,928 | Trainable:   25,473,792
  token_embeddings       | Total:    2,451,456 | Trainable:    2,451,456
  position_embeddings    | Total:            0 | Trainable:            0
  attention_layers       | Total:   30,081,024 | Trainable:    8,847,360
  feed_forward_layers    | Total:   14,155,776 | Trainable:   14,155,776
  layer_norms            | Total:       18,432 | Trainable:       18,432
  final_layer_norm       | Total:          768 | Trainable:          768
  language_model_head    | Total:   38,633,472 | Trainable:            0
------------------------------------------------------------
Re-configuring optimizer after architectural change...
num decayed parameter tensors: 62, with 25,454,592 parameters
num non-decayed parameter tensors: 25, with 19,200 parameters
using fused AdamW: True
Transferring optimizer state for existing parameters...
Transferred optimizer state for 0 / 100 parameters
Re-compiling the model...
Architectural operation completed successfully.
Re-evaluating validation loss after operation...
New val loss after operation: 7.5002
iter 500: loss 7.2856, lr 0.00375, time 5553.37ms, mfu 25.85%
iter 510: loss 7.9064, lr 0.00388, time 1178.67ms, mfu 26.14%
iter 520: loss 7.2993, lr 0.00400, time 1178.59ms, mfu 26.40%
iter 530: loss 7.4681, lr 0.00413, time 1170.75ms, mfu 26.65%
iter 540: loss 7.3388, lr 0.00425, time 1177.48ms, mfu 26.86%
iter 550: loss 7.6156, lr 0.00438, time 1161.92ms, mfu 27.09%
iter 560: loss 7.6610, lr 0.00450, time 1159.35ms, mfu 27.30%
iter 570: loss 7.4000, lr 0.00463, time 1158.75ms, mfu 27.49%
iter 580: loss 7.6265, lr 0.00475, time 1162.78ms, mfu 27.66%
iter 590: loss 7.5249, lr 0.00488, time 1170.74ms, mfu 27.79%
step 600: train loss 7.3253, val loss 7.3672
saving checkpoint to out
iter 600: loss 7.4051, lr 0.00500, time 4014.64ms, mfu 25.85%
iter 610: loss 6.9269, lr 0.00500, time 1167.84ms, mfu 26.17%
iter 620: loss 7.4637, lr 0.00500, time 1173.69ms, mfu 26.43%
iter 630: loss 7.5753, lr 0.00500, time 1165.09ms, mfu 26.70%
iter 640: loss 7.4551, lr 0.00500, time 1169.49ms, mfu 26.92%
iter 650: loss 7.2577, lr 0.00500, time 1170.17ms, mfu 27.13%
iter 660: loss 7.3099, lr 0.00500, time 1158.56ms, mfu 27.34%
iter 670: loss 7.6328, lr 0.00500, time 1156.95ms, mfu 27.53%
iter 680: loss 7.3122, lr 0.00500, time 1160.03ms, mfu 27.70%
iter 690: loss 7.3144, lr 0.00500, time 1159.14ms, mfu 27.85%
step 700: train loss 7.1157, val loss 7.1597
saving checkpoint to out
iter 700: loss 6.9294, lr 0.00500, time 4179.61ms, mfu 25.87%
iter 710: loss 7.1171, lr 0.00500, time 1169.25ms, mfu 26.18%
Traceback (most recent call last):
  File "/teamspace/studios/this_studio/nanoGPT/train.py", line 635, in <module>
    scaler.scale(loss).backward()
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/_tensor.py", line 648, in backward
    torch.autograd.backward(
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/autograd/__init__.py", line 353, in backward
    _engine_run_backward(
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/autograd/graph.py", line 824, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
KeyboardInterrupt
Traceback (most recent call last):
  File "/teamspace/studios/this_studio/nanoGPT/train.py", line 635, in <module>
    scaler.scale(loss).backward()
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/_tensor.py", line 648, in backward
    torch.autograd.backward(
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/autograd/__init__.py", line 353, in backward
    _engine_run_backward(
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/autograd/graph.py", line 824, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
KeyboardInterrupt
Exception ignored in atexit callback: <function _start_and_connect_service.<locals>.teardown_atexit at 0x7a0f1ff8c820>
Traceback (most recent call last):
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/wandb/sdk/lib/service/service_connection.py", line 54, in teardown_atexit
    conn.teardown(hooks.exit_code)
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/wandb/sdk/lib/service/service_connection.py", line 182, in teardown
    self._router.join()
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/wandb/sdk/interface/router.py", line 75, in join
    self._thread.join()
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/threading.py", line 1096, in join
    self._wait_for_tstate_lock()
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/threading.py", line 1116, in _wait_for_tstate_lock
    if lock.acquire(block, timeout):
KeyboardInterrupt:
Exception ignored in atexit callback: <function shutdown_compile_workers at 0x7a0f40d8c550>
Traceback (most recent call last):
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/_inductor/async_compile.py", line 113, in shutdown_compile_workers
    pool.shutdown()
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/_inductor/compile_worker/subproc_pool.py", line 239, in shutdown
    self.process.wait(300)
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/subprocess.py", line 1209, in wait
    return self._wait(timeout=timeout)
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/subprocess.py", line 1937, in _wait
    time.sleep(delay)
KeyboardInterrupt:
