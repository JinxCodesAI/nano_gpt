Overriding config with .\config\shkspr_char_diff\optimal7.py:
out_dir = 'out'
init_from = 'scratch' # 'scratch' or 'resume'
wandb_log = True # disabled by default
wandb_project = 'experiments_diffusion'
wandb_run_name = 'shkspr_char_diff_moderate_first' # 'run' + str(time.time())
batch_size = 16
gradient_accumulation_steps = 12
# data
dataset = 'shakespeare_char'
use_paragraph_boundaries = False # if True, start samples at paragraph boundaries (double newlines)
# diffusion training config
training_type = 'unmasking'  # 'unmasking', 'remasking', or 'remasking_binary' - type of training
use_all_stages_for_training = True
weight_loss_by_mask_ratio = True
enable_entropy_penalty = False
uncertainty_factor = 0.03

# For unmasking: stage-based training with direct probability control

unmasking_stages = [
    {'type':'sticky','target_masked_ratio': 0.4, 'p1_probability': 0.15, 'p2_probability': 0.3, 'val_loss_stale_count': 6},
    {'type':'sticky','target_masked_ratio': 0.6, 'p1_probability': 0.1, 'p2_probability': 0.5, 'val_loss_stale_count': 8},
    {'type':'random','max_masked_ratio': 0.5, 'val_loss_stale_count': 10},
    {'type':'sticky','target_masked_ratio': 0.6, 'p1_probability': 0.3, 'p2_probability': 0.1, 'val_loss_stale_count': 8},
    {'type':'sticky','target_masked_ratio': 0.6, 'p1_probability': 0.1, 'p2_probability': 0.5, 'val_loss_stale_count': 8},
    {'type':'random','max_masked_ratio': 0.2, 'val_loss_stale_count': 10},
    {'type':'sticky','target_masked_ratio': 0.55, 'p1_probability': 0.1, 'p2_probability': 0.6, 'val_loss_stale_count': 10},
    {'type':'sticky','target_masked_ratio': 0.9, 'p1_probability': 0.1, 'p2_probability': 0.9, 'val_loss_stale_count': 20},
]


validation_stages = [
    {'type':'sticky','target_masked_ratio': 0.4, 'p1_probability': 0.15, 'p2_probability': 0.3, 'val_loss_stale_count': 6},
    {'type':'sticky','target_masked_ratio': 0.6, 'p1_probability': 0.1, 'p2_probability': 0.5, 'val_loss_stale_count': 8},
    {'type':'random','max_masked_ratio': 0.5, 'val_loss_stale_count': 10},
    {'type':'sticky','target_masked_ratio': 0.6, 'p1_probability': 0.3, 'p2_probability': 0.1, 'val_loss_stale_count': 8},
    {'type':'sticky','target_masked_ratio': 0.6, 'p1_probability': 0.1, 'p2_probability': 0.5, 'val_loss_stale_count': 8},
    {'type':'random','max_masked_ratio': 0.2, 'val_loss_stale_count': 10},
    {'type':'sticky','target_masked_ratio': 0.2, 'p1_probability': 0.3, 'p2_probability': 0.0, 'val_loss_stale_count': 2},
    {'type':'sticky','target_masked_ratio': 0.4, 'p1_probability': 0.3, 'p2_probability': 0.0, 'val_loss_stale_count': 4},
    {'type':'sticky','target_masked_ratio': 0.4, 'p1_probability': 0.15, 'p2_probability': 0.3, 'val_loss_stale_count': 6},
    {'type':'sticky','target_masked_ratio': 0.55, 'p1_probability': 0.1, 'p2_probability': 0.6, 'val_loss_stale_count': 10},
    {'type':'sticky','target_masked_ratio': 0.7, 'p1_probability': 0.2, 'p2_probability': 0.4, 'val_loss_stale_count': 15},
    {'type':'sticky','target_masked_ratio': 0.8, 'p1_probability': 0.2, 'p2_probability': 0.4, 'val_loss_stale_count': 20},
    {'type':'sticky','target_masked_ratio': 0.8, 'p1_probability': 0.1, 'p2_probability': 0.9, 'val_loss_stale_count': 20},
    {'type':'sticky','target_masked_ratio': 0.9, 'p1_probability': 0.1, 'p2_probability': 0.9, 'val_loss_stale_count': 20},
]

# adamw optimizer
learning_rate = 1e-3 # with baby networks can afford to go a bit higher
max_iters = 10000
warmup_iters = 2000 # how many steps to warm up for
lr_decay_iters = 8000 # make equal to max_iters usually
min_lr = 3e-5 # learning_rate / 10 usually
weight_decay=2e-2
dropout = 0.2 # for pretraining 0 is good, for finetuning try 0.1+

grad_clip = 0.0  # clip gradients at this value, or disable if == 0.0
# learning rate decay settings
decay_lr = True # it's just experiment, no need to decay

max_entropy_penalty = 3 # loss = loss * (1 + current_entropy_penalty * wrong_answers_entropy)

entropy_penalty_start_iter = 2500 # start increasing entropy penalty after this many iterations

================================================================================
SOURCE CODE:
================================================================================

--- model.py ---
"""
Full definition of a GPT Language Model, all of it in this single file.
References:
1) the official GPT-2 TensorFlow implementation released by OpenAI:
https://github.com/openai/gpt-2/blob/master/src/model.py
2) huggingface/transformers PyTorch implementation:
https://github.com/huggingface/transformers/blob/main/src/transformers/models/gpt2/modeling_gpt2.py
"""

import math
import inspect
from dataclasses import dataclass

import torch
import torch.nn as nn
from torch.nn import functional as F

class RotaryPositionalEmbedding(nn.Module):
    """
    Rotary Positional Embedding (RoPE) as described in:
    "RoFormer: Enhanced Transformer with Rotary Position Embedding"
    https://arxiv.org/abs/2104.09864
    """
    
    def __init__(self, dim, max_position_embeddings=2048, base=10000, device=None):
        super().__init__()
        self.dim = dim
        self.max_position_embeddings = max_position_embeddings
        self.base = base
        
        # Create frequency bands
        inv_freq = 1.0 / (self.base ** (torch.arange(0, self.dim, 2).float().to(device) / self.dim))
        self.register_buffer("inv_freq", inv_freq, persistent=False)
        
        # Build here to make `torch.jit.trace` work.
        self._set_cos_sin_cache(
            seq_len=max_position_embeddings, device=self.inv_freq.device, dtype=torch.get_default_dtype()
        )

    def _set_cos_sin_cache(self, seq_len, device, dtype):
        self.max_seq_len_cached = seq_len
        t = torch.arange(self.max_seq_len_cached, device=device, dtype=self.inv_freq.dtype)
        
        freqs = torch.einsum("i,j->ij", t, self.inv_freq)
        # Different from paper, but it uses a different permutation in order to obtain the same calculation
        emb = torch.cat((freqs, freqs), dim=-1)
        self.register_buffer("cos_cached", emb.cos().to(dtype), persistent=False)
        self.register_buffer("sin_cached", emb.sin().to(dtype), persistent=False)

    def forward(self, x, seq_len=None):
        # x: [bs, num_attention_heads, seq_len, head_size]
        if seq_len > self.max_seq_len_cached:
            self._set_cos_sin_cache(seq_len=seq_len, device=x.device, dtype=x.dtype)

        return (
            self.cos_cached[:seq_len].to(dtype=x.dtype),
            self.sin_cached[:seq_len].to(dtype=x.dtype),
        )

def rotate_half(x):
    """Rotates half the hidden dims of the input."""
    x1 = x[..., : x.shape[-1] // 2]
    x2 = x[..., x.shape[-1] // 2 :]
    return torch.cat((-x2, x1), dim=-1)

def apply_rotary_pos_emb(q, k, cos, sin, position_ids=None):
    """Applies Rotary Position Embedding to the query and key tensors."""
    cos = cos[position_ids].unsqueeze(1)  # [seq_len, 1, dim]
    sin = sin[position_ids].unsqueeze(1)  # [seq_len, 1, dim]
    q_embed = (q * cos) + (rotate_half(q) * sin)
    k_embed = (k * cos) + (rotate_half(k) * sin)
    return q_embed, k_embed

class LayerNorm(nn.Module):
    """ LayerNorm but with an optional bias. PyTorch doesn't support simply bias=False """

    def __init__(self, ndim, bias):
        super().__init__()
        self.weight = nn.Parameter(torch.ones(ndim))
        self.bias = nn.Parameter(torch.zeros(ndim)) if bias else None

    def forward(self, input):
        return F.layer_norm(input, self.weight.shape, self.weight, self.bias, 1e-5)

class CausalSelfAttention(nn.Module):
    """Causal self-attention with optional flash attention and RoPE"""

    def __init__(self, config):
        super().__init__()
        assert config.n_embd % config.n_head == 0
        # key, query, value projections for all heads, but in a batch
        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd, bias=config.bias)
        # output projection
        self.c_proj = nn.Linear(config.n_embd, config.n_embd, bias=config.bias)
        # regularization
        self.attn_dropout = nn.Dropout(config.dropout)
        self.resid_dropout = nn.Dropout(config.dropout)
        self.n_head = config.n_head
        self.n_embd = config.n_embd
        self.dropout = config.dropout
        
        # Rotary positional embeddings
        self.head_dim = config.n_embd // config.n_head
        use_rope = getattr(config, 'use_rope', True)
        if use_rope:
            self.rotary_emb = RotaryPositionalEmbedding(
                self.head_dim, 
                max_position_embeddings=config.block_size,
                device=None  # Will be set when model is moved to device
            )
        else:
            self.rotary_emb = None

        # flash attention make GPU go brrrrr but support is only in PyTorch >= 2.0
        self.flash = hasattr(torch.nn.functional, 'scaled_dot_product_attention')
        if not self.flash:
            print("WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0")
            # causal mask to ensure that attention is only applied to the left in the input sequence
            self.register_buffer("bias", torch.tril(torch.ones(config.block_size, config.block_size))
                                        .view(1, 1, config.block_size, config.block_size))

    def forward(self, x):
        B, T, C = x.size() # batch size, sequence length, embedding dimensionality (n_embd)

        # calculate query, key, values for all heads in batch and move head forward to be the batch dim
        q, k, v  = self.c_attn(x).split(self.n_embd, dim=2)
        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)
        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)
        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)

        # Apply rotary positional embeddings if available
        if self.rotary_emb is not None:
            cos, sin = self.rotary_emb(v, seq_len=T)
            position_ids = torch.arange(T, device=x.device).unsqueeze(0)
            q, k = apply_rotary_pos_emb(q, k, cos, sin, position_ids)

        # causal self-attention; Self-attend: (B, nh, T, hs) x (B, nh, hs, T) -> (B, nh, T, T)
        if self.flash:
            # efficient attention using Flash Attention CUDA kernels
            y = torch.nn.functional.scaled_dot_product_attention(q, k, v, attn_mask=None, dropout_p=self.dropout if self.training else 0, is_causal=True)
        else:
            # manual implementation of attention
            att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))
            att = att.masked_fill(self.bias[:,:,:T,:T] == 0, float('-inf'))
            att = F.softmax(att, dim=-1)
            att = self.attn_dropout(att)
            y = att @ v # (B, nh, T, T) x (B, nh, T, hs) -> (B, nh, T, hs)
        y = y.transpose(1, 2).contiguous().view(B, T, C) # re-assemble all head outputs side by side

        # output projection
        y = self.resid_dropout(self.c_proj(y))
        return y

class BidirectionalSelfAttention(nn.Module):
    """Bidirectional self-attention with optional flash attention and RoPE"""

    def __init__(self, config):
        super().__init__()
        assert config.n_embd % config.n_head == 0
        # key, query, value projections for all heads, but in a batch
        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd, bias=config.bias)
        # output projection
        self.c_proj = nn.Linear(config.n_embd, config.n_embd, bias=config.bias)
        # regularization
        self.attn_dropout = nn.Dropout(config.dropout)
        self.resid_dropout = nn.Dropout(config.dropout)
        self.n_head = config.n_head
        self.n_embd = config.n_embd
        self.dropout = config.dropout
        
        # Rotary positional embeddings
        self.head_dim = config.n_embd // config.n_head
        use_rope = getattr(config, 'use_rope', True)
        if use_rope:
            self.rotary_emb = RotaryPositionalEmbedding(
                self.head_dim, 
                max_position_embeddings=config.block_size,
                device=None  # Will be set when model is moved to device
            )
        else:
            self.rotary_emb = None

        # flash attention make GPU go brrrrr but support is only in PyTorch >= 2.0
        self.flash = hasattr(torch.nn.functional, 'scaled_dot_product_attention')
        if not self.flash:
            print("WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0")

    def forward(self, x):
        B, T, C = x.size() # batch size, sequence length, embedding dimensionality (n_embd)

        # calculate query, key, values for all heads in batch and move head forward to be the batch dim
        q, k, v  = self.c_attn(x).split(self.n_embd, dim=2)
        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)
        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)
        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)

        # Apply rotary positional embeddings if available
        if self.rotary_emb is not None:
            cos, sin = self.rotary_emb(v, seq_len=T)
            position_ids = torch.arange(T, device=x.device).unsqueeze(0)
            q, k = apply_rotary_pos_emb(q, k, cos, sin, position_ids)

        # bidirectional self-attention; Self-attend: (B, nh, T, hs) x (B, nh, hs, T) -> (B, nh, T, T)
        if self.flash:
            # efficient attention using Flash Attention CUDA kernels
            y = torch.nn.functional.scaled_dot_product_attention(q, k, v, attn_mask=None, dropout_p=self.dropout if self.training else 0, is_causal=False)
        else:
            # manual implementation of attention
            att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))
            # No masking for bidirectional attention
            att = F.softmax(att, dim=-1)
            att = self.attn_dropout(att)
            y = att @ v # (B, nh, T, T) x (B, nh, T, hs) -> (B, nh, T, hs)
        y = y.transpose(1, 2).contiguous().view(B, T, C) # re-assemble all head outputs side by side

        # output projection
        y = self.resid_dropout(self.c_proj(y))
        return y

class MLP(nn.Module):

    def __init__(self, config):
        super().__init__()
        self.c_fc    = nn.Linear(config.n_embd, 4 * config.n_embd, bias=config.bias)
        self.gelu    = nn.GELU()
        self.c_proj  = nn.Linear(4 * config.n_embd, config.n_embd, bias=config.bias)
        self.dropout = nn.Dropout(config.dropout)

    def forward(self, x):
        x = self.c_fc(x)
        x = self.gelu(x)
        x = self.c_proj(x)
        x = self.dropout(x)
        return x

class Block(nn.Module):

    def __init__(self, config):
        super().__init__()
        self.ln_1 = LayerNorm(config.n_embd, bias=config.bias)

        # Choose attention type based on config
        attention_type = getattr(config, 'attention_type', 'causal')
        if attention_type == 'bidirectional':
            self.attn = BidirectionalSelfAttention(config)
            print("Using bidirectional attention")
        else:
            self.attn = CausalSelfAttention(config)
            print("Using causal attention")

        self.ln_2 = LayerNorm(config.n_embd, bias=config.bias)
        self.mlp = MLP(config)

    def forward(self, x):
        x = x + self.attn(self.ln_1(x))
        x = x + self.mlp(self.ln_2(x))
        return x

@dataclass
class GPTConfig:
    block_size: int = 1024
    vocab_size: int = 50304 # GPT-2 vocab_size of 50257, padded up to nearest multiple of 64 for efficiency
    n_layer: int = 12
    n_head: int = 12
    n_embd: int = 768
    dropout: float = 0.0
    bias: bool = True # True: bias in Linears and LayerNorms, like GPT-2. False: a bit better and faster
    attention_type: str = 'causal' # 'causal' or 'bidirectional' - type of attention to use
    use_rope: bool = True # Use Rotary Position Embeddings instead of absolute position embeddings
    binary_classification: bool = False # True: use binary classification head (2 outputs), False: use language model head (vocab_size outputs)

class GPT(nn.Module):

    def __init__(self, config):
        super().__init__()
        assert config.vocab_size is not None
        assert config.block_size is not None
        self.config = config

        # Create transformer components - conditionally include position embeddings
        transformer_components = dict(
            wte = nn.Embedding(config.vocab_size, config.n_embd),
            drop = nn.Dropout(config.dropout),
            h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]),
            ln_f = LayerNorm(config.n_embd, bias=config.bias),
        )
        
        # Only add absolute position embeddings if not using RoPE
        if not getattr(config, 'use_rope', True):
            transformer_components['wpe'] = nn.Embedding(config.block_size, config.n_embd)
        
        self.transformer = nn.ModuleDict(transformer_components)
        
        # Choose between language model head and binary classification head
        if config.binary_classification:
            self.lm_head = nn.Linear(config.n_embd, 2, bias=False)  # Binary classifier: 2 outputs
        else:
            self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)
            # with weight tying when using torch.compile() some warnings get generated:
            # "UserWarning: functional_call was passed multiple values for tied weights.
            # This behavior is deprecated and will be an error in future versions"
            # not 100% sure what this is, so far seems to be harmless. TODO investigate
            self.transformer.wte.weight = self.lm_head.weight # https://paperswithcode.com/method/weight-tying

        # init all weights
        self.apply(self._init_weights)
        # apply special scaled init to the residual projections, per GPT-2 paper
        for pn, p in self.named_parameters():
            if pn.endswith('c_proj.weight'):
                torch.nn.init.normal_(p, mean=0.0, std=0.02/math.sqrt(2 * config.n_layer))
        
        # Special initialization for binary classification head (after general init)
        if config.binary_classification:
            # Initialize with much smaller weights to prevent gradient explosion
            torch.nn.init.normal_(self.lm_head.weight, mean=0.0, std=0.002)  # Very small std

        # report number of parameters
        print("number of parameters: %.2fM" % (self.get_num_params()/1e6,))

    def get_num_params(self, non_embedding=True):
        """
        Return the number of parameters in the model.
        For non-embedding count (default), the position embeddings get subtracted.
        The token embeddings would too, except due to the parameter sharing these
        params are actually used as weights in the final layer, so we include them.
        """
        n_params = sum(p.numel() for p in self.parameters())
        if non_embedding and hasattr(self.transformer, 'wpe'):
            n_params -= self.transformer.wpe.weight.numel()
        return n_params

    def _init_weights(self, module):
        if isinstance(module, nn.Linear):
            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)
            if module.bias is not None:
                torch.nn.init.zeros_(module.bias)
        elif isinstance(module, nn.Embedding):
            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)

    def forward(self, idx, targets=None):
        device = idx.device
        b, t = idx.size()
        assert t <= self.config.block_size, f"Cannot forward sequence of length {t}, block size is only {self.config.block_size}"

        # forward the GPT model itself
        tok_emb = self.transformer.wte(idx) # token embeddings of shape (b, t, n_embd)
        
        # Add positional embeddings only if not using RoPE
        if hasattr(self.transformer, 'wpe'):
            pos = torch.arange(0, t, dtype=torch.long, device=device) # shape (t)
            pos_emb = self.transformer.wpe(pos) # position embeddings of shape (t, n_embd)
            x = self.transformer.drop(tok_emb + pos_emb)
        else:
            # When using RoPE, no absolute position embeddings are needed
            x = self.transformer.drop(tok_emb)
            
        for block in self.transformer.h:
            x = block(x)
        x = self.transformer.ln_f(x)

        if targets is not None:
            # Training: always compute logits for all positions
            logits = self.lm_head(x)
            
            if self.config.binary_classification:
                # For binary classification, targets can be class indices or probability distributions
                if targets.dim() == 3:
                    # Probability distributions (batch_size, seq_len, num_classes)
                    # Cross-entropy can handle soft targets directly
                    loss = F.cross_entropy(logits.view(-1, 2), targets.view(-1, 2))
                else:
                    # Hard targets: 0 or 1 class indices
                    # Calculate dynamic class weights to handle class imbalance
                    flattened_targets = targets.view(-1)
                    valid_targets = flattened_targets[flattened_targets != -1]  # exclude ignore_index
                    
                    if len(valid_targets) > 0:
                        unique, counts = torch.unique(valid_targets, return_counts=True)
                        n_samples = len(valid_targets)
                        n_classes = 2
                        
                        # Create balanced class weights: n_samples / (n_classes * class_count)
                        class_weights = torch.zeros(2, device=targets.device, dtype=logits.dtype)
                        for cls, count in zip(unique, counts):
                            class_weights[cls] = n_samples / (n_classes * count)
                        
                        loss = F.cross_entropy(logits.view(-1, 2), flattened_targets, 
                                             weight=class_weights, ignore_index=-1)
                    else:
                        # Fallback if no valid targets (shouldn't happen in practice)
                        print(f"WARNING: No valid targets found for class weighting, using unweighted loss")
                        loss = F.cross_entropy(logits.view(-1, 2), flattened_targets, ignore_index=-1)
            else:
                # For language modeling, targets can be token IDs or probability distributions
                if targets.dim() == 3:
                    # Probability distributions (batch_size, seq_len, vocab_size)
                    # Cross-entropy can handle soft targets directly
                    loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1, logits.size(-1)))
                else:
                    # Token IDs (batch_size, seq_len)
                    loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-1)
        else:
            # Inference: behavior depends on attention type and classification mode
            if self.config.binary_classification:
                # For binary classification, always compute all positions
                logits = self.lm_head(x)
            else:
                attention_type = getattr(self.config, 'attention_type', 'causal')
                if attention_type == 'causal':
                    # For causal attention (autoregressive), optimize by only computing last position
                    logits = self.lm_head(x[:, [-1], :]) # note: using list [-1] to preserve the time dim
                else:
                    # For bidirectional attention (diffusion), we need logits for all positions
                    logits = self.lm_head(x)  # Compute logits for all positions
            loss = None

        return logits, loss

    def crop_block_size(self, block_size):
        # model surgery to decrease the block size if necessary
        # e.g. we may load the GPT2 pretrained model checkpoint (block size 1024)
        # but want to use a smaller block size for some smaller, simpler model
        assert block_size <= self.config.block_size
        self.config.block_size = block_size
        
        # Only crop position embeddings if they exist (not using RoPE)
        if hasattr(self.transformer, 'wpe'):
            self.transformer.wpe.weight = nn.Parameter(self.transformer.wpe.weight[:block_size])
            
        for block in self.transformer.h:
            # Only causal attention has bias buffer
            if hasattr(block.attn, 'bias'):
                block.attn.bias = block.attn.bias[:,:,:block_size,:block_size]
            # Update RoPE max position embeddings if using RoPE
            if hasattr(block.attn, 'rotary_emb') and block.attn.rotary_emb is not None:
                block.attn.rotary_emb.max_position_embeddings = block_size
                block.attn.rotary_emb._set_cos_sin_cache(
                    seq_len=block_size, 
                    device=block.attn.rotary_emb.inv_freq.device,
                    dtype=torch.get_default_dtype()
                )

    @classmethod
    def from_pretrained(cls, model_type, override_args=None):
        assert model_type in {'gpt2', 'gpt2-medium', 'gpt2-large', 'gpt2-xl'}
        override_args = override_args or {} # default to empty dict
        # only dropout can be overridden see more notes below
        assert all(k == 'dropout' for k in override_args)
        from transformers import GPT2LMHeadModel
        print("loading weights from pretrained gpt: %s" % model_type)

        # n_layer, n_head and n_embd are determined from model_type
        config_args = {
            'gpt2':         dict(n_layer=12, n_head=12, n_embd=768),  # 124M params
            'gpt2-medium':  dict(n_layer=24, n_head=16, n_embd=1024), # 350M params
            'gpt2-large':   dict(n_layer=36, n_head=20, n_embd=1280), # 774M params
            'gpt2-xl':      dict(n_layer=48, n_head=25, n_embd=1600), # 1558M params
        }[model_type]
        print("forcing vocab_size=50257, block_size=1024, bias=True")
        config_args['vocab_size'] = 50257 # always 50257 for GPT model checkpoints
        config_args['block_size'] = 1024 # always 1024 for GPT model checkpoints
        config_args['bias'] = True # always True for GPT model checkpoints
        # we can override the dropout rate, if desired
        if 'dropout' in override_args:
            print(f"overriding dropout rate to {override_args['dropout']}")
            config_args['dropout'] = override_args['dropout']
        # create a from-scratch initialized minGPT model
        config = GPTConfig(**config_args)
        model = GPT(config)
        sd = model.state_dict()
        sd_keys = sd.keys()
        sd_keys = [k for k in sd_keys if not k.endswith('.attn.bias')] # discard this mask / buffer, not a param

        # init a huggingface/transformers model
        model_hf = GPT2LMHeadModel.from_pretrained(model_type)
        sd_hf = model_hf.state_dict()

        # copy while ensuring all of the parameters are aligned and match in names and shapes
        sd_keys_hf = sd_hf.keys()
        sd_keys_hf = [k for k in sd_keys_hf if not k.endswith('.attn.masked_bias')] # ignore these, just a buffer
        sd_keys_hf = [k for k in sd_keys_hf if not k.endswith('.attn.bias')] # same, just the mask (buffer)
        transposed = ['attn.c_attn.weight', 'attn.c_proj.weight', 'mlp.c_fc.weight', 'mlp.c_proj.weight']
        # basically the openai checkpoints use a "Conv1D" module, but we only want to use a vanilla Linear
        # this means that we have to transpose these weights when we import them
        assert len(sd_keys_hf) == len(sd_keys), f"mismatched keys: {len(sd_keys_hf)} != {len(sd_keys)}"
        for k in sd_keys_hf:
            if any(k.endswith(w) for w in transposed):
                # special treatment for the Conv1D weights we need to transpose
                assert sd_hf[k].shape[::-1] == sd[k].shape
                with torch.no_grad():
                    sd[k].copy_(sd_hf[k].t())
            else:
                # vanilla copy over the other parameters
                assert sd_hf[k].shape == sd[k].shape
                with torch.no_grad():
                    sd[k].copy_(sd_hf[k])

        return model

    def configure_optimizers(self, weight_decay, learning_rate, betas, device_type):
        # start with all of the candidate parameters
        param_dict = {pn: p for pn, p in self.named_parameters()}
        # filter out those that do not require grad
        param_dict = {pn: p for pn, p in param_dict.items() if p.requires_grad}
        # create optim groups. Any parameters that is 2D will be weight decayed, otherwise no.
        # i.e. all weight tensors in matmuls + embeddings decay, all biases and layernorms don't.
        decay_params = [p for n, p in param_dict.items() if p.dim() >= 2]
        nodecay_params = [p for n, p in param_dict.items() if p.dim() < 2]
        optim_groups = [
            {'params': decay_params, 'weight_decay': weight_decay},
            {'params': nodecay_params, 'weight_decay': 0.0}
        ]
        num_decay_params = sum(p.numel() for p in decay_params)
        num_nodecay_params = sum(p.numel() for p in nodecay_params)
        print(f"num decayed parameter tensors: {len(decay_params)}, with {num_decay_params:,} parameters")
        print(f"num non-decayed parameter tensors: {len(nodecay_params)}, with {num_nodecay_params:,} parameters")
        # Create AdamW optimizer and use the fused version if it is available
        fused_available = 'fused' in inspect.signature(torch.optim.AdamW).parameters
        use_fused = fused_available and device_type == 'cuda'
        extra_args = dict(fused=True) if use_fused else dict()
        optimizer = torch.optim.AdamW(optim_groups, lr=learning_rate, betas=betas, **extra_args)
        print(f"using fused AdamW: {use_fused}")

        return optimizer

    def estimate_mfu(self, fwdbwd_per_iter, dt):
        """ estimate model flops utilization (MFU) in units of A100 bfloat16 peak FLOPS """
        # first estimate the number of flops we do per iteration.
        # see PaLM paper Appendix B as ref: https://arxiv.org/abs/2204.02311
        N = self.get_num_params()
        cfg = self.config
        L, H, Q, T = cfg.n_layer, cfg.n_head, cfg.n_embd//cfg.n_head, cfg.block_size
        flops_per_token = 6*N + 12*L*H*Q*T
        flops_per_fwdbwd = flops_per_token * T
        flops_per_iter = flops_per_fwdbwd * fwdbwd_per_iter
        # express our flops throughput as ratio of A100 bfloat16 peak flops
        flops_achieved = flops_per_iter * (1.0/dt) # per second
        flops_promised = 312e12 # A100 GPU bfloat16 peak flops is 312 TFLOPS
        mfu = flops_achieved / flops_promised
        return mfu

    @torch.no_grad()
    def generate(self, idx, max_new_tokens, temperature=1.0, top_k=None):
        """
        Take a conditioning sequence of indices idx (LongTensor of shape (b,t)) and complete
        the sequence max_new_tokens times, feeding the predictions back into the model each time.
        Most likely you'll want to make sure to be in model.eval() mode of operation for this.
        """
        for _ in range(max_new_tokens):
            # if the sequence context is growing too long we must crop it at block_size
            idx_cond = idx if idx.size(1) <= self.config.block_size else idx[:, -self.config.block_size:]
            # forward the model to get the logits for the index in the sequence
            logits, _ = self(idx_cond)
            # pluck the logits at the final step and scale by desired temperature
            logits = logits[:, -1, :] / temperature
            # optionally crop the logits to only the top k options
            if top_k is not None:
                v, _ = torch.topk(logits, min(top_k, logits.size(-1)))
                logits[logits < v[:, [-1]]] = -float('Inf')
            # apply softmax to convert logits to (normalized) probabilities
            probs = F.softmax(logits, dim=-1)
            # sample from the distribution
            idx_next = torch.multinomial(probs, num_samples=1)
            # append sampled index to the running sequence and continue
            idx = torch.cat((idx, idx_next), dim=1)

        return idx


--- train_run.py ---
"""
Main training script runner for diffusion training.
Uses train_utils.py for all function definitions.
"""

import os
import sys
import time
import math
import pickle
from contextlib import nullcontext
import threading
from queue import Queue

import numpy as np
import torch
from torch.nn.parallel import DistributedDataParallel as DDP
from torch.distributed import init_process_group, destroy_process_group

from model import GPTConfig, GPT
from utils import Timer, log_masking_stats
from train_utils import (
    get_batch, estimate_loss, get_lr, load_synthetic_model, 
    start_prefetch, stop_prefetch, TrainingContext, UnmaskingStage, update_stage_progress,
    create_unmasking_validation_set, UnmaskingStageType, StickyStageConfig, RandomStageConfig,
    calculate_wrong_answer_entropy, get_current_entropy_penalty, update_entropy_multiplier_ema,
    apply_label_smoothing
)

torch._dynamo.config.suppress_errors = True

# Global timer instance
timer = Timer()

def print_and_flush(msg):
    """Print message and immediately flush stdout for real-time logging"""
    print(msg)
    sys.stdout.flush()

# -----------------------------------------------------------------------------
# default config values 
# I/O
out_dir = 'out'
training_type = 'unmasking'  
eval_interval = 200
log_interval = 20
eval_iters = 20
eval_only = False # if True, script exits right after the first eval
always_save_checkpoint = True # if True, always save a checkpoint after each eval
init_from = 'resume' # 'scratch' or 'resume'
ckpt_filename = '34.5_58.4_UM.pt' # Specific checkpoint to load (if not latest)
# model
n_layer = 6
n_head = 6
n_embd = 384
dropout = 0.01 # for pretraining 0 is good, for finetuning try 0.1+
bias = False # do we use bias inside LayerNorm and Linear layers?
attention_type = 'bidirectional' # 'causal' or 'bidirectional' - type of attention to use (bidirectional recommended for diffusion)
use_rope = True # use Rotary Position Embeddings instead of absolute position embeddings
# wandb logging
wandb_log = True # disabled by default
wandb_project = 'diffusion'
wandb_run_name = '13k_UN_noise_0.2' # 'run' + str(time.time())
# data
dataset = 'shakespeare_char'
gradient_accumulation_steps = 1 # used to simulate larger batch sizes
batch_size = 16 # if gradient_accumulation_steps > 1, this is the micro-batch size
block_size = 1024
use_paragraph_boundaries = False # if True, start samples at paragraph boundaries (double newlines)
# unmasking training config
unmasking_stages = [] # override in config file
validation_stages = [] # override in config file
use_all_stages_for_training = False # if True, generate training batches from all stages like validation
weight_loss_by_mask_ratio = False # if True, weight loss by sqrt(1.0 / mask_ratio) to balance gradient magnitude across masking ratios
enable_entropy_penalty = False # if True, apply entropy penalty to incentivize uniform wrong answer distributions
max_entropy_penalty = 0.5 # maximum entropy penalty multiplier (penalizes concentrated wrong answers)
entropy_penalty_start_iter = 6000 # iteration to start applying entropy penalty
# label smoothing config
uncertainty_factor = 0.0 # if > 0, apply label smoothing: correct answer gets (1-u), wrong answers get u/(vocab_size-1)

# adamw optimizer
learning_rate = 1e-3 # with baby networks can afford to go a bit higher
max_iters = 50000
warmup_iters = 2000 # how many steps to warm up for
lr_decay_iters = 41000 # make equal to max_iters usually
min_lr = 1e-4 # learning_rate / 10 usually
beta1 = 0.9
beta2 = 0.99 # make a bit bigger because number of tokens per iter is small
weight_decay=1e-3

grad_clip = 1.0 # clip gradients at this value, or disable if == 0.0
# learning rate decay settings
decay_lr = True # whether to decay the learning rat
# DDP settings
backend = 'nccl' # 'nccl', 'gloo', etc.
# system
device = 'cuda' # examples: 'cpu', 'cuda', 'cuda:0', 'cuda:1' etc., or try 'mps' on macbooks
dtype = 'float16'
compile = True # use PyTorch 2.0 to compile the model to be faster
start_iter_num = 0

# -----------------------------------------------------------------------------
config_keys = [k for k,v in globals().items() if not k.startswith('_') and isinstance(v, (int, float, bool, str))]
exec(open('configurator.py').read()) # overrides from command line or config file

if len(unmasking_stages) == 0 or unmasking_stages is None:
    print_and_flush("No unmasking stages defined, exiting...")
    exit()

# Update wandb run name after configuration is loaded
if training_type == 'unmasking':
    wandb_run_name = f'{wandb_run_name}_unmasking'

config = {k: globals()[k] for k in config_keys} # will be useful for logging

# Print source code and global variables on startup
print("=" * 80)
print("SOURCE CODE:")
print("=" * 80)

import sys
import os

# Get all local Python files that are imported
local_files = set()
for module_name, module in sys.modules.items():
    if hasattr(module, '__file__') and module.__file__:
        file_path = module.__file__
        # Only include .py files in current directory (not packages/libraries)
        if file_path.endswith('.py') and os.path.dirname(file_path) == os.getcwd():
            local_files.add(os.path.basename(file_path))

# Always include the main script
local_files.add('train_run.py')

# Convert to sorted list for consistent output
local_files = sorted(local_files)

for filename in local_files:
    print(f"\n--- {filename} ---")
    try:
        with open(filename, 'r') as f:
            print(f.read())
    except FileNotFoundError:
        print(f"File {filename} not found")

print("\n" + "=" * 80)
print("GLOBAL VARIABLES:")
print("=" * 80)
for name, value in sorted(globals().items()):
    if not name.startswith('_') and not callable(value):
        print(f"{name} = {value}")

print("\n" + "=" * 80)
# -----------------------------------------------------------------------------

# various inits, derived attributes, I/O setup
ddp = int(os.environ.get('RANK', -1)) != -1 # is this a ddp run?
if ddp:
    init_process_group(backend=backend)
    ddp_rank = int(os.environ['RANK'])
    ddp_local_rank = int(os.environ['LOCAL_RANK'])
    ddp_world_size = int(os.environ['WORLD_SIZE'])
    device = f'cuda:{ddp_local_rank}'
    torch.cuda.set_device(device)
    master_process = ddp_rank == 0 # this process will do logging, checkpointing etc.
    seed_offset = ddp_rank # each process gets a different seed
    # world_size number of processes will be training simultaneously, so we can scale
    # down the desired gradient accumulation iterations per process proportionally
    assert gradient_accumulation_steps % ddp_world_size == 0
    gradient_accumulation_steps //= ddp_world_size
else:
    # if not ddp, we are running on a single gpu, and one process
    master_process = True
    seed_offset = 0
    ddp_world_size = 1
tokens_per_iter = gradient_accumulation_steps * ddp_world_size * batch_size * block_size
print_and_flush(f"tokens per iteration will be: {tokens_per_iter:,}")

if master_process:
    os.makedirs(out_dir, exist_ok=True)
torch.manual_seed(1337 + seed_offset)
torch.backends.cuda.matmul.allow_tf32 = True # allow tf32 on matmul
torch.backends.cudnn.allow_tf32 = True # allow tf32 on cudnn
device_type = 'cuda' if 'cuda' in device else 'cpu' # for later use in torch.autocast
# note: float16 data type will automatically use a GradScaler
ptdtype = {'float32': torch.float32, 'bfloat16': torch.bfloat16, 'float16': torch.float16}[dtype]
ctx = nullcontext() if device_type == 'cpu' else torch.amp.autocast(device_type=device_type, dtype=ptdtype)

# poor man's data loader
data_dir = os.path.join('data', dataset)

# init these up here, can override if init_from='resume' (i.e. from a checkpoint)
iter_num = 0
best_val_loss = 1e9
checkpoint_training_context = None  # For restoring training context state

# attempt to derive vocab_size from the dataset
meta_path = os.path.join(data_dir, 'meta.pkl')
meta_vocab_size = None
if os.path.exists(meta_path):
    with open(meta_path, 'rb') as f:
        meta = pickle.load(f)
    meta_vocab_size = meta['vocab_size']
    print_and_flush(f"found vocab_size = {meta_vocab_size} (inside {meta_path})")
    
    # Set special token ID for unmasking training
    mask_token_id = meta_vocab_size
    extended_vocab_size = meta_vocab_size + 15  # Reserve 15 special tokens for future finetuning
    print_and_flush(f"mask_token_id = {mask_token_id}, extended_vocab_size = {extended_vocab_size}")
else:
    print_and_flush("No meta.pkl found, using default GPT-2 vocab")
    mask_token_id = 50304
    extended_vocab_size = 50304 + 15  # Reserve 15 special tokens

# Create training context with all parameters
# Convert unmasking_stages dict to UnmaskingStage objects
unmasking_stage_objects = None
if training_type == 'unmasking':
    unmasking_stage_objects = []
    for stage in unmasking_stages:
        stage_type = stage['type']
        if stage_type == 'sticky':
            config = StickyStageConfig(
                target_masked_ratio=stage['target_masked_ratio'],
                p1_probability=stage['p1_probability'],
                p2_probability=stage['p2_probability'],
                val_loss_stale_count=stage['val_loss_stale_count']
            )
        elif stage_type == 'random':
            config = RandomStageConfig(
                max_masked_ratio=stage['max_masked_ratio'],
                val_loss_stale_count=stage['val_loss_stale_count']
            )
        else:
            raise ValueError(f"Unknown stage type: {stage_type}")
        
        unmasking_stage_objects.append(UnmaskingStage(config))

# Convert validation_stages dict to UnmaskingStage objects (if different from training stages)
validation_stage_objects = None
if training_type == 'unmasking' and len(validation_stages) > 0:
    validation_stage_objects = []
    for stage in validation_stages:
        stage_type = stage['type']
        if stage_type == 'sticky':
            config = StickyStageConfig(
                target_masked_ratio=stage['target_masked_ratio'],
                p1_probability=stage['p1_probability'],
                p2_probability=stage['p2_probability'],
                val_loss_stale_count=stage['val_loss_stale_count']
            )
        elif stage_type == 'random':
            config = RandomStageConfig(
                max_masked_ratio=stage['max_masked_ratio'],
                val_loss_stale_count=stage['val_loss_stale_count']
            )
        else:
            raise ValueError(f"Unknown stage type: {stage_type}")
        
        validation_stage_objects.append(UnmaskingStage(config))

training_ctx = TrainingContext(
    training_type=training_type,
    batch_size=batch_size,
    block_size=block_size,
    max_iters=max_iters,
    device=device,
    device_type=device_type,
    seed_offset=seed_offset,
    data_dir=data_dir,
    meta_vocab_size=meta_vocab_size,
    mask_token_id=mask_token_id,
    extended_vocab_size=extended_vocab_size,
    iter_num=iter_num,
    unmasking_stages=unmasking_stage_objects,
    validation_stages=validation_stage_objects,
    eval_iters=eval_iters,
    warmup_iters=warmup_iters,
    lr_decay_iters=lr_decay_iters,
    learning_rate=learning_rate,
    min_lr=min_lr,
    use_paragraph_boundaries=use_paragraph_boundaries,
    use_all_stages_for_training=use_all_stages_for_training,
    weight_loss_by_mask_ratio=weight_loss_by_mask_ratio,
    enable_entropy_penalty=enable_entropy_penalty,
    max_entropy_penalty=max_entropy_penalty,
    entropy_penalty_start_iter=entropy_penalty_start_iter,
    uncertainty_factor=uncertainty_factor
)

# Apply restored training context state if resuming from checkpoint
print(f"DEBUG: init_from='{init_from}', checkpoint_training_context={checkpoint_training_context}")
if init_from == 'resume' and checkpoint_training_context is not None:
    print("Applying restored training context state...")
    training_ctx.current_stage = checkpoint_training_context.get('current_stage', 0)
    training_ctx.val_loss_stale_count = checkpoint_training_context.get('val_loss_stale_count', 0)
    training_ctx.best_val_loss_this_stage = checkpoint_training_context.get('best_val_loss_for_stage', float('inf'))
    training_ctx.entropy_multiplier_ema = checkpoint_training_context.get('entropy_multiplier_ema', 1.0)
    print(f"Training context restored: stage={training_ctx.current_stage}, stale_count={training_ctx.val_loss_stale_count}, entropy_ema={training_ctx.entropy_multiplier_ema:.4f}")
else:
    print(f"DEBUG: NOT applying training context. init_from='{init_from}', checkpoint_training_context={checkpoint_training_context is not None}")

# model init
model_args = dict(n_layer=n_layer, n_head=n_head, n_embd=n_embd, block_size=block_size,
                  bias=bias, vocab_size=None, dropout=dropout, attention_type=attention_type, use_rope=use_rope) # start with model_args from command line
if init_from == 'scratch':
    # init a new model from scratch
    print_and_flush("Initializing a new model from scratch")
    # determine the vocab size we'll use for from-scratch training
    if meta_vocab_size is None:
        print_and_flush("defaulting to vocab_size of GPT-2 to 50304 (50257 rounded up for efficiency)")
    model_args['vocab_size'] = extended_vocab_size if meta_vocab_size is not None else 50304 + 15
    gptconf = GPTConfig(**model_args)
    model = GPT(gptconf)
elif init_from == 'resume':
    print_and_flush(f"Resuming unmasking training from {out_dir}")
    # resume training from a checkpoint.
    # Find the latest unmasking checkpoint file
    if ckpt_filename is None:
        import glob
        ckpt_pattern = os.path.join(out_dir, 'ckpt_*unmasking*.pt')
        ckpt_files = glob.glob(ckpt_pattern)
        if not ckpt_files:
            # Fallback to old naming convention
            ckpt_path = os.path.join(out_dir, 'ckpt.pt')
            if not os.path.exists(ckpt_path):
                raise FileNotFoundError(f"No unmasking checkpoint files found in {out_dir}")
        else:
            # Extract iteration numbers and find the latest
            def extract_iter_num(filename):
                basename = os.path.basename(filename)
                # Extract number from ckpt_unmasking_XXX.pt
                parts = basename.split('_')
                for part in parts:
                    if part.replace('.pt', '').isdigit():
                        return int(part.replace('.pt', ''))
                return 0

            latest_ckpt = max(ckpt_files, key=extract_iter_num)
            ckpt_path = latest_ckpt
        print_and_flush(f"Loading latest checkpoint: {os.path.basename(ckpt_path)}")
    else:
        ckpt_path = os.path.join(out_dir, ckpt_filename)
        if not os.path.exists(ckpt_path):
            raise FileNotFoundError(f"Checkpoint file {ckpt_path} not found")

    checkpoint = torch.load(ckpt_path, map_location=device, weights_only=False)
    checkpoint_model_args = checkpoint['model_args']
    training_ctx.extended_vocab_size = checkpoint_model_args['vocab_size']
    print_and_flush(f"Checkpoint vocab size: {training_ctx.extended_vocab_size}")
    # force these config attributes to be equal otherwise we can't even resume training
    # the rest of the attributes (e.g. dropout) can stay as desired from command line
    for k in ['n_layer', 'n_head', 'n_embd', 'block_size', 'bias', 'vocab_size']:
        model_args[k] = checkpoint_model_args[k]
    # Also restore use_rope setting if it exists in checkpoint
    if 'use_rope' in checkpoint_model_args:
        model_args['use_rope'] = checkpoint_model_args['use_rope']
    # create the model
    gptconf = GPTConfig(**model_args)
    model = GPT(gptconf)
    state_dict = checkpoint['model']
    # fix the keys of the state dictionary :(
    # honestly no idea how checkpoints sometimes get this prefix, have to debug more
    unwanted_prefix = '_orig_mod.'
    for k,v in list(state_dict.items()):
        if k.startswith(unwanted_prefix):
            state_dict[k[len(unwanted_prefix):]] = state_dict.pop(k)
    model.load_state_dict(state_dict)
    iter_num = checkpoint['iter_num']
    start_iter_num = iter_num
    best_val_loss = checkpoint['best_val_loss']
    
    # Restore training context state if available
    if 'training_context' in checkpoint:
        ctx_state = checkpoint['training_context']
        print_and_flush(f"Restoring training context state:")
        print_and_flush(f"  Stage: {ctx_state.get('current_stage', 0)}")
        print_and_flush(f"  Val loss stale count: {ctx_state.get('val_loss_stale_count', 0)}")
        print_and_flush(f"  Best val loss for stage: {ctx_state.get('best_val_loss_for_stage', float('inf'))}")
        
        # These will be set on the training_ctx after it's created
        checkpoint_training_context = ctx_state
    else:
        checkpoint_training_context = None

# crop down the model block size if desired, using model surgery
if block_size < model.config.block_size:
    model.crop_block_size(block_size)
    model_args['block_size'] = block_size # so that the checkpoint will have the right value
model.to(device)

# No synthetic model loading needed for unmasking training

# initialize a GradScaler. If enabled=False scaler is a no-op
scaler = torch.cuda.amp.GradScaler(enabled=(dtype == 'float16'))

# optimizer
optimizer = model.configure_optimizers(weight_decay, learning_rate, (beta1, beta2), device_type)
if init_from == 'resume':
    optimizer.load_state_dict(checkpoint['optimizer'])
checkpoint = None # free up memory

# compile the model
if compile:
    print_and_flush("compiling the model... (takes a ~minute)")
    unoptimized_model = model
    model = torch.compile(model) # requires PyTorch 2.0

# wrap model into DDP container
if ddp:
    model = DDP(model, device_ids=[ddp_local_rank])

# logging
if wandb_log and master_process and not eval_only:
    import wandb
    wandb.init(project=wandb_project, name=wandb_run_name, config=config)

# Function to reload model and optimizer from checkpoint during training
def reload_from_checkpoint():
    """Reload model and optimizer from the latest checkpoint"""
    global model, optimizer, iter_num, best_val_loss, training_ctx, raw_model
    
    print(f"\n*** RELOADING FROM CHECKPOINT ***")
    
    # Find the latest unmasking checkpoint file
    import glob
    ckpt_pattern = os.path.join(out_dir, 'ckpt_*unmasking*.pt')
    ckpt_files = glob.glob(ckpt_pattern)
    
    if not ckpt_files:
        print("No unmasking checkpoint files found for recovery - cannot continue")
        return False
    
    # Extract iteration numbers and find the latest
    def extract_iter_num(filename):
        basename = os.path.basename(filename)
        # Extract number from ckpt_unmasking_XXX.pt
        parts = basename.split('_')
        for part in parts:
            if part.replace('.pt', '').isdigit():
                return int(part.replace('.pt', ''))
        return 0
    
    latest_ckpt = max(ckpt_files, key=extract_iter_num)
    ckpt_path = latest_ckpt
    print(f"Reloading from checkpoint: {os.path.basename(ckpt_path)}")
    
    # Load checkpoint
    checkpoint = torch.load(ckpt_path, map_location=device, weights_only=False)
    
    # Reload model state - handle compiled vs non-compiled model mismatches
    model_state = checkpoint['model']
    
    # Check if current model expects _orig_mod prefix but checkpoint doesn't have it
    current_keys = set(raw_model.state_dict().keys())
    checkpoint_keys = set(model_state.keys())
    
    # Determine if we need to add or remove _orig_mod prefix
    if any(k.startswith('_orig_mod.') for k in current_keys) and not any(k.startswith('_orig_mod.') for k in checkpoint_keys):
        # Current model is compiled (has _orig_mod prefix), but checkpoint doesn't - add prefix
        print("Adding _orig_mod prefix to checkpoint keys for compiled model")
        new_state = {}
        for k, v in model_state.items():
            new_state[f'_orig_mod.{k}'] = v
        model_state = new_state
    elif not any(k.startswith('_orig_mod.') for k in current_keys) and any(k.startswith('_orig_mod.') for k in checkpoint_keys):
        # Current model is not compiled, but checkpoint has _orig_mod prefix - remove prefix
        print("Removing _orig_mod prefix from checkpoint keys for non-compiled model")
        unwanted_prefix = '_orig_mod.'
        for k, v in list(model_state.items()):
            if k.startswith(unwanted_prefix):
                model_state[k[len(unwanted_prefix):]] = model_state.pop(k)
    
    raw_model.load_state_dict(model_state)
    
    # Reload optimizer state
    optimizer.load_state_dict(checkpoint['optimizer'])
    
    # Update iteration and loss tracking
    # Step back iteration to avoid immediately hitting the same problematic iteration
    iter_num = checkpoint['iter_num'] - 1
    best_val_loss = checkpoint['best_val_loss']
    
    # Restore training context state if available
    if 'training_context' in checkpoint:
        ctx_state = checkpoint['training_context']
        training_ctx.current_stage = ctx_state.get('current_stage', 0)
        training_ctx.val_loss_stale_count = ctx_state.get('val_loss_stale_count', 0)
        training_ctx.best_val_loss_this_stage = ctx_state.get('best_val_loss_for_stage', float('inf'))
        training_ctx.entropy_multiplier_ema = ctx_state.get('entropy_multiplier_ema', 1.0)
        print(f"Training context restored: stage={training_ctx.current_stage}, entropy_ema={training_ctx.entropy_multiplier_ema:.4f}")
    
    
    print(f"Model and optimizer reloaded from iteration {iter_num}")
    print("*** CHECKPOINT RELOAD COMPLETE ***\n")
    return True

# training loop
X, Y, mask = get_batch('train', training_ctx) # fetch the very first batch
t0 = time.time()
local_iter_num = 0 # number of iterations in the lifetime of this process
raw_model = model.module if ddp else model # unwrap DDP container if needed
running_mfu = -1.0

# Show initial stage configuration for unmasking training
if training_ctx.training_type == 'unmasking':
    stage_config = training_ctx.get_current_stage_config()
    print(f"\n*** STAGE-BASED UNMASKING TRAINING INITIALIZED ***")
    print(f"Starting at Stage {training_ctx.current_stage}:")
    stage_type = stage_config.get_stage_type()
    print(f"  Stage type: {stage_type.value}")
    if stage_type == UnmaskingStageType.STICKY:
        config = stage_config.config
        print(f"  Target masked ratio: {config.target_masked_ratio}")
        print(f"  P1 probability: {config.p1_probability}")
        print(f"  P2 probability: {config.p2_probability}")
    elif stage_type == UnmaskingStageType.RANDOM:
        config = stage_config.config
        print(f"  Max masked ratio: {config.max_masked_ratio}")
    print(f"  Val loss stale count limit: {stage_config.get_val_loss_stale_count()}")
    print(f"Total stages configured: {len(training_ctx.unmasking_stages)}")
    print("*** STAGE INITIALIZATION COMPLETE ***\n")
    
    # Pre-create validation set with equal representation from all stages
    print("Pre-creating validation set...")
    create_unmasking_validation_set(training_ctx)
    
    # Training batches will be generated fresh each time from all stages when flag is enabled
    if training_ctx.use_all_stages_for_training:
        print("Training will generate fresh batches from all stages each iteration")

print_and_flush("Starting training loop...")
just_recovered = False
while True:

    # determine and set the learning rate for this iteration
    lr = get_lr(iter_num, training_ctx) if decay_lr else learning_rate
    for param_group in optimizer.param_groups:
        param_group['lr'] = lr

    # evaluate the loss on train/val sets and write checkpoints
    if (iter_num % eval_interval == 0 and master_process and not just_recovered) or eval_only:
        print_and_flush(f"\n--- Starting validation at iteration {iter_num} ---")
        with timer.time_function('validation'):
            # Update training context with current iteration
            training_ctx.iter_num = iter_num
            losses = estimate_loss(model, ctx, timer, training_ctx)

        # VALIDATION INSTABILITY DETECTION
        train_loss_finite = math.isfinite(losses['train'])
        val_loss_finite = math.isfinite(losses['val'])
        if not train_loss_finite or not val_loss_finite:
            print_and_flush(f"\n*** VALIDATION INSTABILITY at iter {iter_num} ***")
            print_and_flush(f"Train loss: {losses['train']} ({'finite' if train_loss_finite else 'NaN/Inf'})")
            print_and_flush(f"Val loss: {losses['val']} ({'finite' if val_loss_finite else 'NaN/Inf'})")
            print_and_flush("NaN detected in validation - model has become unstable")
            print_and_flush("*** TERMINATING TRAINING ***")
            break
        
        # Print basic losses
        print_and_flush(f"--- Validation complete ---")
        print_and_flush(f"step {iter_num}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}, lr {lr:.6f}")
        
        # Print entropy penalty information if enabled
        if training_ctx.enable_entropy_penalty:
            current_entropy_penalty = get_current_entropy_penalty(iter_num, training_ctx)
            print_and_flush(f"entropy penalty: {current_entropy_penalty:.4f}, multiplier EMA: {training_ctx.entropy_multiplier_ema:.4f}")
        
        # Print stage information for unmasking training
        if 'current_stage' in losses:
            stage_config = training_ctx.get_current_stage_config()
            stage_type = stage_config.get_stage_type()
            stage_info = f"Stage {losses['current_stage']} ({stage_type.value}): "
            if stage_type == UnmaskingStageType.STICKY:
                config = stage_config.config
                stage_info += f"target_ratio={config.target_masked_ratio:.1f}, p1={config.p1_probability:.1f}, p2={config.p2_probability:.1f}"
            elif stage_type == UnmaskingStageType.RANDOM:
                config = stage_config.config
                stage_info += f"max_ratio={config.max_masked_ratio:.1f}"
            stage_info += f", stale_count={losses.get('val_loss_stale_count', 0)}"
            print_and_flush(stage_info)

        # Print model vs random statistics if available
        if 'val_model_vs_random' in losses:
            print(f"  val model vs random: {losses['val_model_vs_random']:.2f}x better")
            print(f"  val avg correct prob: {losses['val_avg_correct_prob']:.4f} (random: {1.0/training_ctx.extended_vocab_size:.4f})")
            if 'val_signal_to_noise' in losses:
                print(f"  val signal to noise: {losses['val_signal_to_noise']:.2f} (median: {losses.get('val_signal_to_noise_median', 0.0):.2f})")
            if 'val_most_likely_accuracy' in losses:
                print(f"  Most likely guess correct P %: {losses['val_most_likely_accuracy']:.1f}%")
        
        # Update stage progress for unmasking training
        stage_advanced = update_stage_progress(training_ctx, losses['val'])
        if stage_advanced:
            print(f"Advanced to stage {training_ctx.current_stage} - validation set remains consistent across all stages")
        
        print()  # Add blank line for readability

        if wandb_log and master_process and not eval_only:
            log_dict = {
                "iter": iter_num,
                "train/loss": losses['train'],
                "val/loss": losses['val'],
                "lr": lr,
                "model vs random": losses.get('val_model_vs_random', 0.0),
                "signal to noise": losses.get('val_signal_to_noise', 0.0),
                "signal to noise median": losses.get('val_signal_to_noise_median', 0.0),
                "mfu": running_mfu*100, # convert to percentage
                "masked_token_ratio": losses.get('train_masked_token_ratio', 0.0),
                "min_masked_token_ratio": losses.get('train_min_masked_token_ratio', 0.0),
                "max_masked_token_ratio": losses.get('train_max_masked_token_ratio', 0.0),
            }
            
            # Add entropy penalty to validation wandb logging if enabled
            if training_ctx.enable_entropy_penalty:
                current_entropy_penalty = get_current_entropy_penalty(iter_num, training_ctx)
                log_dict["entropy_penalty"] = current_entropy_penalty
                log_dict["entropy_multiplier_ema"] = training_ctx.entropy_multiplier_ema
            
            # Add per-stage validation losses for unmasking training
            for stage_idx in range(len(training_ctx.validation_stages or [])):
                stage_loss_key = f'val_stage_{stage_idx}_loss'
                stage_samples_key = f'val_stage_{stage_idx}_samples'
                if stage_loss_key in losses:
                    log_dict[f'val/stage_{stage_idx}_loss'] = losses[stage_loss_key]
                    log_dict[f'val/stage_{stage_idx}_samples'] = losses[stage_samples_key]

            wandb.log(log_dict)
        if losses['val'] < best_val_loss or always_save_checkpoint:
            best_val_loss = losses['val']
            if iter_num > 0:
                checkpoint = {
                    'model': raw_model.state_dict(),
                    'optimizer': optimizer.state_dict(),
                    'model_args': model_args,
                    'iter_num': iter_num,
                    'best_val_loss': best_val_loss,
                    'config': config,
                }
                
                # Save training context state for proper resumption
                checkpoint['training_context'] = {
                    'current_stage': training_ctx.current_stage,
                    'val_loss_stale_count': training_ctx.val_loss_stale_count,
                    'best_val_loss_for_stage': training_ctx.best_val_loss_this_stage,
                    'entropy_multiplier_ema': training_ctx.entropy_multiplier_ema
                }
                ckpt_filename = f'ckpt_unmasking_{iter_num}.pt'
                    
                if start_iter_num != iter_num:
                    print(f"saving checkpoint to {out_dir}/{ckpt_filename}")
                    torch.save(checkpoint, os.path.join(out_dir, ckpt_filename))
    if eval_only:
        break

    # forward backward update, with optional gradient accumulation to simulate larger batch size
    # and using the GradScaler if data type is float16
    with timer.time_function('gradient_accumulation_loop'):
        for micro_step in range(gradient_accumulation_steps):
            if ddp:
                # in DDP training we only need to sync gradients at the last micro step.
                # the official way to do this is with model.no_sync() context manager, but
                # I really dislike that this bloats the code and forces us to repeat code
                # looking at the source of that context manager, it just toggles this variable
                model.require_backward_grad_sync = (micro_step == gradient_accumulation_steps - 1)
            with ctx:
                with timer.time_function('forward_pass'):
                    # Combined forward pass and loss computation for efficiency
                    logits, loss = model(X, Y)
                
                with timer.time_function('instability_detection'):
                    # TRAINING INSTABILITY DETECTION
                    if not torch.isfinite(logits).all():
                        print(f"\n*** INSTABILITY DETECTED at iter {iter_num} ***")
                        print(f"Logits contain NaN/Inf: {torch.isnan(logits).sum().item()} NaN, {torch.isinf(logits).sum().item()} Inf")
                        print(f"Logits stats: min={logits.min().item():.6f}, max={logits.max().item():.6f}, mean={logits.mean().item():.6f}")
                        print("*** ATTEMPTING RECOVERY FROM CHECKPOINT ***")
                        if reload_from_checkpoint():
                            # Reset local state and restart iteration completely
                            local_iter_num = 0
                            running_mfu = -1.0
                            training_ctx.iter_num = iter_num
                            # Generate new batch to avoid same problematic data
                            X, Y, mask = get_batch('train', training_ctx)
                            # Reset scaler state and start fresh iteration
                            scaler = torch.cuda.amp.GradScaler(enabled=(dtype == 'float16'))
                            optimizer.zero_grad(set_to_none=True)
                            just_recovered = True
                            t0 = time.time()
                            continue
                        else:
                            print("*** RECOVERY FAILED - TERMINATING TRAINING ***")
                            break
                    
                    if not torch.isfinite(loss):
                        print(f"\n*** LOSS INSTABILITY at iter {iter_num} ***")
                        print(f"Loss is {loss.item()}: {'NaN' if torch.isnan(loss) else 'Inf'}")
                        print("*** ATTEMPTING RECOVERY FROM CHECKPOINT ***")
                        if reload_from_checkpoint():
                            # Reset local state and restart iteration completely
                            local_iter_num = 0
                            running_mfu = -1.0
                            training_ctx.iter_num = iter_num
                            # Generate new batch to avoid same problematic data
                            X, Y, mask = get_batch('train', training_ctx)
                            # Reset scaler state and start fresh iteration
                            scaler = torch.cuda.amp.GradScaler(enabled=(dtype == 'float16'))
                            optimizer.zero_grad(set_to_none=True)
                            just_recovered = True
                            t0 = time.time()
                            continue
                        else:
                            print("*** RECOVERY FAILED - TERMINATING TRAINING ***")
                            break
                
                # Apply masking for unmasking training only (most efficient path)
                if training_ctx.training_type == 'unmasking' and mask.any():
                    # Fast path: reshape once and use boolean indexing
                    # Cross-entropy handles both hard targets (indices) and soft targets (probabilities)
                    logits_reshaped = logits.view(-1, logits.size(-1))
                    mask_reshaped = mask.view(-1)
                    
                    if Y.dim() == 3:
                        # Soft targets (probability distributions)
                        targets_reshaped = Y.view(-1, Y.size(-1))
                        loss = torch.nn.functional.cross_entropy(
                            logits_reshaped[mask_reshaped], 
                            targets_reshaped[mask_reshaped], 
                            reduction='mean'
                        )
                    else:
                        # Hard targets (token indices)
                        targets_reshaped = Y.view(-1)
                        loss = torch.nn.functional.cross_entropy(
                            logits_reshaped[mask_reshaped], 
                            targets_reshaped[mask_reshaped], 
                            reduction='mean'
                        )
                    
                    # Apply mask ratio weighting if enabled
                    if training_ctx.weight_loss_by_mask_ratio:
                        mask_ratio = mask.float().mean().item()
                        if mask_ratio > 0:
                            weight = (1.0 / mask_ratio) ** 0.5  # sqrt(1.0 / mask_ratio)
                            loss = loss * weight
                else:
                    if training_ctx.training_type == 'unmasking':
                        loss = torch.tensor(0.0, device=loss.device, requires_grad=True)
                
                with timer.time_function('loss_processing'):
                    # Apply entropy penalty if enabled (works for all training types)
                    if training_ctx.enable_entropy_penalty:
                        current_entropy_penalty = get_current_entropy_penalty(iter_num, training_ctx)
                        if current_entropy_penalty > 0:
                            # Calculate entropy of wrong answer distributions
                            wrong_answer_entropy = calculate_wrong_answer_entropy(logits, Y, training_ctx.extended_vocab_size)
                            
                            # Calculate max possible entropy for wrong answers: log(vocab_size - 1)
                            max_wrong_entropy = math.log(training_ctx.extended_vocab_size - 1)
                            
                            # Penalty for LOW entropy (concentrated wrong answers)
                            # When entropy is low (bad) -> high penalty
                            # When entropy is high (good) -> low penalty  
                            entropy_penalty_factor = (max_wrong_entropy - wrong_answer_entropy) / max_wrong_entropy
                            entropy_multiplier = 1.0 + current_entropy_penalty * entropy_penalty_factor
                            loss = loss * entropy_multiplier
                            
                            # Update EMA of entropy multiplier
                            update_entropy_multiplier_ema(training_ctx, entropy_multiplier)
                        else:
                            # No penalty applied, multiplier is 1.0
                            update_entropy_multiplier_ema(training_ctx, 1.0)

                # For remasking variants, model's internal loss is already correct
                
                # UNIVERSAL: Check final loss after any training-type-specific processing
                if not torch.isfinite(loss):
                    print(f"\n*** FINAL LOSS INSTABILITY at iter {iter_num} ***")
                    print(f"Final loss is {loss.item()}: {'NaN' if torch.isnan(loss) else 'Inf'}")
                    print(f"Training type: {training_ctx.training_type}")
                    if hasattr(mask, 'float'):  # Check if mask exists
                        print(f"Mask ratio: {mask.float().mean().item():.4f}")
                    print("*** ATTEMPTING RECOVERY FROM CHECKPOINT ***")
                    if reload_from_checkpoint():
                        # Reset local state and restart iteration completely
                        local_iter_num = 0
                        running_mfu = -1.0
                        training_ctx.iter_num = iter_num
                        # Generate new batch to avoid same problematic data
                        X, Y, mask = get_batch('train', training_ctx)
                        # Reset scaler state and start fresh iteration
                        scaler = torch.cuda.amp.GradScaler(enabled=(dtype == 'float16'))
                        optimizer.zero_grad(set_to_none=True)
                        just_recovered = True
                        t0 = time.time()
                        continue
                    else:
                        print("*** RECOVERY FAILED - TERMINATING TRAINING ***")
                        break
                        
                loss = loss / gradient_accumulation_steps
        # immediately async prefetch next batch while model is doing the forward pass on the GPU
        with timer.time_function('data_generation'):
            # Update training context with current iteration for the next batch
            training_ctx.iter_num = iter_num
            X, Y, mask = get_batch('train', training_ctx)
            # backward pass, with gradient scaling if training in fp16
            with timer.time_function('backward_pass'):
                scaler.scale(loss).backward()
    
    # GRADIENT PROCESSING AND CLIPPING
    with timer.time_function('gradient_processing'):
        if grad_clip != 0.0:
            scaler.unscale_(optimizer)
            # Monitor gradient norms before clipping
            grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)
        
            # Check for true instability (NaN/Inf gradients)
            if not torch.isfinite(grad_norm):
                # At iteration 0 with lr=0, infinite gradients indicate model/loss issues
                if iter_num == 0:
                    print(f"\n*** INITIALIZATION PROBLEM at iter {iter_num} ***")
                    print(f"Gradient norm is {grad_norm.item()}: {'NaN' if torch.isnan(grad_norm) else 'Inf'}")
                    print(f"Learning rate: {lr:.6f}")
                    print("This suggests model initialization or loss computation issues")
                    
                    # Check a few key statistics
                    print("\nModel parameter stats:")
                    for name, param in list(model.named_parameters())[:3]:  # First 3 params
                        print(f"  {name}: mean={param.data.mean().item():.6f}, std={param.data.std().item():.6f}")
                        if param.grad is not None:
                            print(f"    grad: mean={param.grad.data.mean().item():.6f}, std={param.grad.data.std().item():.6f}")
                else:
                    print(f"\n*** GRADIENT INSTABILITY at iter {iter_num} ***")
                    print(f"Gradient norm is {grad_norm.item()}: {'NaN' if torch.isnan(grad_norm) else 'Inf'}")
                
                # Check individual parameter gradients
                nan_params = 0
                inf_params = 0
                for name, param in model.named_parameters():
                    if param.grad is not None:
                        if torch.isnan(param.grad).any():
                            nan_params += 1
                        if torch.isinf(param.grad).any():
                            inf_params += 1
                print(f"Parameters with NaN gradients: {nan_params}, with Inf gradients: {inf_params}")
                print("*** ATTEMPTING RECOVERY FROM CHECKPOINT ***")
                if reload_from_checkpoint():
                    # Reset local state and restart iteration completely
                    local_iter_num = 0
                    running_mfu = -1.0
                    training_ctx.iter_num = iter_num
                    # Generate new batch to avoid same problematic data
                    X, Y, mask = get_batch('train', training_ctx)
                    # Reset scaler state and start fresh iteration
                    scaler = torch.cuda.amp.GradScaler(enabled=(dtype == 'float16'))
                    optimizer.zero_grad(set_to_none=True)
                    just_recovered = True
                    t0 = time.time()
                    continue
                else:
                    print("*** RECOVERY FAILED - TERMINATING TRAINING ***")
                    break
            
            # Only warn about large gradients after initial iterations (when lr > 0)
            if iter_num > 10 and grad_norm > grad_clip * 10:
                print(f"WARNING: Large gradient norm at iter {iter_num}: {grad_norm.item():.4f} (clip threshold: {grad_clip})")
        else:
            # Still check gradient norms even without clipping
            total_norm = 0.0
            nan_grads = False
            inf_grads = False
            
            for param in model.parameters():
                if param.grad is not None:
                    param_norm = param.grad.data.norm(2)
                    if torch.isnan(param_norm):
                        nan_grads = True
                    if torch.isinf(param_norm):
                        inf_grads = True
                    total_norm += param_norm.item() ** 2
            
            total_norm = total_norm ** (1. / 2)
            
            if nan_grads or inf_grads:
                print(f"\n*** GRADIENT INSTABILITY at iter {iter_num} (no clipping) ***")
                print(f"NaN gradients: {nan_grads}, Inf gradients: {inf_grads}")
                print(f"Total gradient norm: {total_norm:.6f}")
                print("*** ATTEMPTING RECOVERY FROM CHECKPOINT ***")
                if reload_from_checkpoint():
                    # Reset local state and restart iteration completely
                    local_iter_num = 0
                    running_mfu = -1.0
                    training_ctx.iter_num = iter_num
                    # Generate new batch to avoid same problematic data
                    X, Y, mask = get_batch('train', training_ctx)
                    # Reset scaler state and start fresh iteration
                    scaler = torch.cuda.amp.GradScaler(enabled=(dtype == 'float16'))
                    optimizer.zero_grad(set_to_none=True)
                    just_recovered = True
                    t0 = time.time()
                    continue
                else:
                    print("*** RECOVERY FAILED - TERMINATING TRAINING ***")
                    break
    
    # OPTIMIZER OPERATIONS  
    with timer.time_function('optimizer_operations'):
        # step the optimizer and scaler if training in fp16
        scaler.step(optimizer)
        scaler.update()
    
    # PARAMETER STABILITY DETECTION
    with timer.time_function('parameter_stability_check'):
        nan_params = 0
        inf_params = 0
        param_names_with_issues = []
        
        for name, param in model.named_parameters():
            if param.data is not None:
                if torch.isnan(param.data).any():
                    nan_params += 1
                    param_names_with_issues.append(f"{name}(NaN)")
                if torch.isinf(param.data).any():
                    inf_params += 1
                    param_names_with_issues.append(f"{name}(Inf)")
        
        if nan_params > 0 or inf_params > 0:
            print(f"\n*** PARAMETER INSTABILITY at iter {iter_num} ***")
            print(f"Parameters with NaN values: {nan_params}, with Inf values: {inf_params}")
            print(f"Affected parameters: {param_names_with_issues[:10]}")  # Show first 10
            if len(param_names_with_issues) > 10:
                print(f"... and {len(param_names_with_issues) - 10} more")
            print("*** ATTEMPTING RECOVERY FROM CHECKPOINT ***")
            if reload_from_checkpoint():
                # Reset local state and restart iteration completely
                local_iter_num = 0
                running_mfu = -1.0
                training_ctx.iter_num = iter_num
                # Generate new batch to avoid same problematic data
                X, Y, mask = get_batch('train', training_ctx)
                # Reset scaler state and start fresh iteration
                scaler = torch.cuda.amp.GradScaler(enabled=(dtype == 'float16'))
                optimizer.zero_grad(set_to_none=True)
                just_recovered = True
                t0 = time.time()
                continue
            else:
                print("*** RECOVERY FAILED - TERMINATING TRAINING ***")
                break
    
    # CLEANUP OPERATIONS
    with timer.time_function('cleanup_operations'):
        # flush the gradients as soon as we can, no need for this memory anymore
        optimizer.zero_grad(set_to_none=True)

    # timing and logging
    t1 = time.time()
    dt = t1 - t0
    t0 = t1
    if iter_num % log_interval == 0 and master_process:
        # GPU SYNCHRONIZATION OPERATIONS
        with timer.time_function('gpu_synchronization'):
            # get loss as float. note: this is a CPU-GPU sync point
            # scale up to undo the division above, approximating the true total loss (exact would have been a sum)
            lossf = loss.item() * gradient_accumulation_steps
            if local_iter_num >= 5: # let the training loop settle a bit
                mfu = raw_model.estimate_mfu(batch_size * gradient_accumulation_steps, dt)
                running_mfu = mfu if running_mfu == -1.0 else 0.9*running_mfu + 0.1*mfu

        # Enhanced logging with detailed timing - use recent measurements only
        data_time = timer.get_recent_average('data_generation') * 1000
        forward_time = timer.get_recent_average('forward_pass') * 1000
        loss_time = timer.get_recent_average('loss_computation') * 1000
        backward_time = timer.get_recent_average('backward_pass') * 1000
        grad_accum_time = timer.get_recent_average('gradient_accumulation_loop') * 1000
        grad_proc_time = timer.get_recent_average('gradient_processing') * 1000
        optimizer_time = timer.get_recent_average('optimizer_operations') * 1000
        param_check_time = timer.get_recent_average('parameter_stability_check') * 1000
        cleanup_time = timer.get_recent_average('cleanup_operations') * 1000
        gpu_sync_time = timer.get_recent_average('gpu_synchronization') * 1000
        loss_proc_time = timer.get_recent_average('loss_processing') * 1000
        instability_time = timer.get_recent_average('instability_detection') * 1000

        # Calculate total of measured components (avoid double-counting nested timers)
        # grad_accum_time already contains ALL nested operations: data, forward, backward, loss_proc, instability
        measured_total = grad_accum_time + grad_proc_time + optimizer_time + param_check_time + cleanup_time + gpu_sync_time
        total_time = dt * 1000
        unaccounted_time = total_time - measured_total

        print(f"iter {iter_num}: loss {lossf:.4f}, time {dt*1000:.2f}ms, mfu {running_mfu*100:.2f}%")
        print(f"  data: {data_time:.1f}ms, grad_accum: {grad_accum_time:.1f}ms (fw: {forward_time:.1f}ms, bw: {backward_time:.1f}ms)")
        print(f"  grad_proc: {grad_proc_time:.1f}ms, optimizer: {optimizer_time:.1f}ms, param_check: {param_check_time:.1f}ms")
        print(f"  loss_proc: {loss_proc_time:.1f}ms, instability: {instability_time:.1f}ms")
        print(f"  cleanup: {cleanup_time:.1f}ms, gpu_sync: {gpu_sync_time:.1f}ms")
        print(f"  measured: {measured_total:.1f}ms, unaccounted: {unaccounted_time:.1f}ms ({unaccounted_time/total_time*100:.1f}%)")
        
        # Add entropy penalty logging if enabled
        if training_ctx.enable_entropy_penalty:
            current_entropy_penalty = get_current_entropy_penalty(iter_num, training_ctx)
            if current_entropy_penalty > 0:
                print(f"  entropy_penalty: {current_entropy_penalty:.4f}, multiplier_ema: {training_ctx.entropy_multiplier_ema:.4f} (max: {training_ctx.max_entropy_penalty})")

        # Validation timing (when applicable)
        if iter_num % eval_interval == 0:
            val_time = timer.get_average('validation') * 1000
            val_data_time = timer.get_average('validation_data_generation') * 1000
            val_forward_time = timer.get_average('validation_forward_pass') * 1000
            val_loss_time = timer.get_average('validation_loss_computation') * 1000
            print(f"  validation: {val_time:.2f}ms (data: {val_data_time:.2f}ms, forward: {val_forward_time:.2f}ms, loss: {val_loss_time:.2f}ms)")

        # Add masking statistics logging for unmasking
        stage_config = training_ctx.get_current_stage_config()
        if stage_config and iter_num % (log_interval * 10) == 0:
            mask_ratio = mask.float().mean().item()
            stage_type = stage_config.get_stage_type()
            stage_info = f"Masking: stage={training_ctx.current_stage} ({stage_type.value}), actual_ratio={mask_ratio:.3f}"
            if stage_type == UnmaskingStageType.STICKY:
                config = stage_config.config
                stage_info += f", target={config.target_masked_ratio:.1f}, p1={config.p1_probability:.1f}, p2={config.p2_probability:.1f}"
            elif stage_type == UnmaskingStageType.RANDOM:
                config = stage_config.config
                stage_info += f", max={config.max_masked_ratio:.1f}"
            print(stage_info)
        
        if wandb_log and master_process and not eval_only:
            log_dict = {
                "iter": iter_num,
                "train/loss": lossf,
                "lr": lr,
                "mfu": running_mfu*100 # convert to percentage
            }
            
            # Add entropy penalty to wandb logging if enabled
            if training_ctx.enable_entropy_penalty:
                current_entropy_penalty = get_current_entropy_penalty(iter_num, training_ctx)
                log_dict["entropy_penalty"] = current_entropy_penalty
                log_dict["entropy_multiplier_ema"] = training_ctx.entropy_multiplier_ema

            wandb.log(log_dict)
    iter_num += 1
    local_iter_num += 1
    just_recovered = False  # Reset recovery flag after successful iteration

    # termination conditions
    if iter_num > max_iters:
        break

if ddp:
    destroy_process_group()

# Cleanup prefetch thread
stop_prefetch()

--- train_utils.py ---
"""
Training utilities for diffusion training script.
Contains all reusable functions for data loading, corruption strategies, and model utilities.
"""

import os
import time
import math
import pickle
import threading
from queue import Queue
from contextlib import nullcontext
from enum import Enum
from abc import ABC, abstractmethod
from typing import Union

import numpy as np
import torch
from torch.nn.parallel import DistributedDataParallel as DDP
from torch.distributed import init_process_group, destroy_process_group

from model import GPTConfig, GPT
from utils import Timer, log_masking_stats
from dataclasses import dataclass

# Global variables for data caching and prefetching
_val_batch_cache = None
_progressive_val_cache = {}  # Cache for progressive validation batches
_progressive_val_full_cache = None  # Cache for full progressive validation set (all 320 samples)
_unmasking_val_set = None  # Complete validation set for unmasking training (eval_iters * batch_size samples)
_remasking_val_set = None  # Complete validation set for remasking training (eval_iters * batch_size samples)
_data_cache = {'train': None, 'val': None}
_valid_indices_cache = {'train': None, 'val': None}
_prefetch_enabled = True
_prefetch_queue = Queue(maxsize=2)
_prefetch_thread = None
_prefetch_active = False

# Global synthetic model for remasking
synthetic_model = None

class UnmaskingStageType(Enum):
    """Enumeration of available unmasking stage types"""
    STICKY = "sticky"
    RANDOM = "random"

@dataclass
class BaseStageConfig(ABC):
    """Base class for stage-specific configuration"""
    val_loss_stale_count: int
    
    @abstractmethod
    def get_stage_type(self) -> UnmaskingStageType:
        """Return the stage type"""
        pass

@dataclass
class StickyStageConfig(BaseStageConfig):
    """Configuration for sticky masking stages"""
    target_masked_ratio: float
    p1_probability: float
    p2_probability: float
    
    def get_stage_type(self) -> UnmaskingStageType:
        return UnmaskingStageType.STICKY

@dataclass
class RandomStageConfig(BaseStageConfig):
    """Configuration for random masking stages"""
    max_masked_ratio: float
    
    def get_stage_type(self) -> UnmaskingStageType:
        return UnmaskingStageType.RANDOM

@dataclass
class UnmaskingStage:
    """Configuration for a single stage of unmasking training"""
    config: Union[StickyStageConfig, RandomStageConfig]
    
    def get_stage_type(self) -> UnmaskingStageType:
        return self.config.get_stage_type()
    
    def get_val_loss_stale_count(self) -> int:
        return self.config.val_loss_stale_count

@dataclass
class TrainingContext:
    """Configuration class for training parameters to avoid long parameter lists"""
    # Training configuration
    training_type: str = 'remasking'
    batch_size: int = 16
    block_size: int = 1024
    max_iters: int = 12000  # Maximum training iterations
    
    # Device configuration
    device: str = 'cuda'
    device_type: str = 'cuda'
    seed_offset: int = 0
    
    # Data configuration
    data_dir: str = 'data/shakespeare_char'
    meta_vocab_size: int = None
    vocab_size: int = None  # extended_vocab_size (meta_vocab_size + 15 reserved special tokens)
    use_paragraph_boundaries: bool = True  # If True, start samples at paragraph boundaries (double newlines)
    use_all_stages_for_training: bool = False  # If True, generate training batches from all stages like validation
    weight_loss_by_mask_ratio: bool = False  # If True, weight loss by sqrt(1.0 / mask_ratio) to balance gradient magnitude
    
    # Token IDs
    mask_token_id: int = None
    wrong_token_id: int = None
    remask_good_id: int = None
    remask_wrong_id: int = None
    extended_vocab_size: int = None
    
    # Training iteration and stage tracking
    iter_num: int = 0
    current_stage: int = 0
    val_loss_stale_count: int = 0
    best_val_loss_this_stage: float = float('inf')
    
    # Unmasking stages (only for unmasking training)
    unmasking_stages: list = None
    validation_stages: list = None  # Separate stages for validation set creation
    
    # Remasking strategy - only random supported
    
    # Remasking corruption parameters
    # Random corruption parameters
    guaranteed_unmasked_max: float = 0.95
    guaranteed_unmasked_min: float = 0.1
    sticky_transition_start: int = 1000
    sticky_transition_end: int = 6000
    random_mask_warmup: int = 1000
    
    # Sticky corruption parameters
    sticky_rounds: int = 4
    sticky_p1_p2_multiplier: float = 1.2
    sticky_p1_divisor: float = 2.0
    p1_p2_ratio: float = 1.0  # For remasking sticky masking: if 1.0 use random, else use sticky
    
    # Evaluation parameters
    eval_iters: int = 20
    
    # Learning rate parameters
    warmup_iters: int = 2000
    lr_decay_iters: int = 8000
    learning_rate: float = 1e-4
    min_lr: float = 1e-5
    
    # Entropy penalty parameters (incentivize uniform wrong answer distributions)
    enable_entropy_penalty: bool = False
    max_entropy_penalty: float = 0.5  # Penalty for concentrated wrong answers
    entropy_penalty_start_iter: int = 6000
    
    # Entropy multiplier tracking
    entropy_multiplier_ema: float = 1.0  # Exponential moving average of entropy multiplier
    entropy_multiplier_ema_factor: float = 0.99  # EMA decay factor
    
    # Label smoothing parameters
    uncertainty_factor: float = 0.0  # Label smoothing factor: 0 = no smoothing, >0 = apply smoothing
    
    def __post_init__(self):
        # Default unmasking stages if not provided
        if self.training_type == 'unmasking' and self.unmasking_stages is None:
            self.unmasking_stages = [
                UnmaskingStage(StickyStageConfig(target_masked_ratio=0.2, p1_probability=0.3, p2_probability=0.0, val_loss_stale_count=5)),
                UnmaskingStage(StickyStageConfig(target_masked_ratio=0.4, p1_probability=0.2, p2_probability=0.8, val_loss_stale_count=5)),
                UnmaskingStage(StickyStageConfig(target_masked_ratio=0.6, p1_probability=0.1, p2_probability=0.9, val_loss_stale_count=10)),
            ]
        
        # Default validation stages if not provided (same as training stages by default)
        if self.training_type == 'unmasking' and self.validation_stages is None:
            self.validation_stages = self.unmasking_stages
        
        # Keep both meta_vocab_size (for random token generation) and extended_vocab_size (for model)
        # vocab_size is kept for backward compatibility but should equal extended_vocab_size
        self.vocab_size = self.extended_vocab_size
    
    def get_current_stage_config(self) -> UnmaskingStage:
        """Get configuration for current unmasking stage"""
        if self.training_type != 'unmasking' or not self.unmasking_stages:
            return None
        
        if self.current_stage >= len(self.unmasking_stages):
            # Return last stage if we've exceeded all stages
            return self.unmasking_stages[-1]
        
        return self.unmasking_stages[self.current_stage]
    
    def advance_stage(self):
        """Advance to next unmasking stage and reset stale count"""
        if self.current_stage < len(self.unmasking_stages) - 1:
            self.current_stage += 1
            self.val_loss_stale_count = 0
            self.best_val_loss_this_stage = float('inf')
            return True
        return False

def find_double_newline_indices(data, meta_vocab_size, block_size):
    """Find all valid starting indices that begin with double newlines (\\n\\n)"""
    # Get the token IDs for newlines
    if meta_vocab_size is not None:
        # For Shakespeare character-level data, newline is token 0
        newline_id = 0
    else:
        # For GPT-2 style tokenization, this would be different
        newline_id = 198  # GPT-2 newline token
    
    # Find positions where we have \\n\\n (two consecutive newlines)
    valid_indices = []
    for i in range(len(data) - block_size - 1):  # -block_size-1 to ensure we can extract full block
        if i >= 1 and data[i] == newline_id and data[i+1] == newline_id:
            valid_indices.append(i)
    
    return np.array(valid_indices)

def _prepare_batch_data_only(split, ctx: TrainingContext):
    """Background function to prepare raw batch data (CPU only)"""
    global _data_cache, _valid_indices_cache
    
    # Ensure data is cached
    if _data_cache[split] is None:
        return None
        
    data = _data_cache[split]
    valid_indices = _valid_indices_cache[split]
    
    # Fast index sampling - all on CPU
    if len(valid_indices) == 0:
        ix_np = torch.randint(len(data) - ctx.block_size, (ctx.batch_size,)).numpy()
    else:
        ix_indices = torch.randint(len(valid_indices), (ctx.batch_size,)).numpy()
        ix_np = valid_indices[ix_indices]

    # VECTORIZED DATA LOADING - Use advanced indexing for parallel loading
    ix_expanded = ix_np[:, None] + np.arange(ctx.block_size)[None, :]  # (batch_size, block_size)
    x_np = data[ix_expanded].astype(np.int64)
    
    return x_np

def _prefetch_worker(ctx: TrainingContext):
    """Background thread worker for data prefetching"""
    global _prefetch_active
    while _prefetch_active:
        try:
            # Prepare next batch in background
            x_np = _prepare_batch_data_only('train', ctx)
            if x_np is not None:
                _prefetch_queue.put(x_np, timeout=1.0)
        except:
            # Queue full or other error, just continue
            time.sleep(0.001)

def start_prefetch(ctx: TrainingContext):
    """Start background data prefetching"""
    global _prefetch_thread, _prefetch_active
    if _prefetch_thread is None and _prefetch_enabled:
        _prefetch_active = True
        _prefetch_thread = threading.Thread(target=lambda: _prefetch_worker(ctx), daemon=True)
        _prefetch_thread.start()

def stop_prefetch():
    """Stop background data prefetching"""
    global _prefetch_thread, _prefetch_active
    _prefetch_active = False
    if _prefetch_thread is not None:
        _prefetch_thread.join(timeout=1.0)
        _prefetch_thread = None

def clear_validation_cache():
    """Clear progressive validation cache - useful when training parameters change"""
    global _progressive_val_cache, _val_batch_cache, _progressive_val_full_cache, _unmasking_val_set, _remasking_val_set
    _progressive_val_cache.clear()
    _val_batch_cache = None
    _progressive_val_full_cache = None
    _unmasking_val_set = None
    _remasking_val_set = None

def create_unmasking_validation_set(ctx: TrainingContext):
    """Create complete validation set with samples evenly distributed across all stages"""
    global _unmasking_val_set, _data_cache, _valid_indices_cache
    
    if _unmasking_val_set is not None:
        print("Using existing validation set from cache")
        return  # Already created
    
    print("Creating validation set with samples from all stages...")
    
    # Cache validation data if not already cached
    if _data_cache['val'] is None:
        _data_cache['val'] = np.memmap(os.path.join(ctx.data_dir, 'val.bin'), dtype=np.uint16, mode='r')
        if ctx.use_paragraph_boundaries:
            _valid_indices_cache['val'] = find_double_newline_indices(_data_cache['val'], ctx.meta_vocab_size, ctx.block_size)
        else:
            _valid_indices_cache['val'] = np.array([])
    
    data = _data_cache['val']
    valid_indices = _valid_indices_cache['val']
    
    total_samples = ctx.eval_iters * ctx.batch_size
    num_stages = len(ctx.validation_stages)
    samples_per_stage = total_samples // num_stages
    
    validation_batches = []
    
    # Generate samples for each stage
    torch.manual_seed(42)  # Fixed seed for reproducible validation set
    
    for stage_idx, stage_config in enumerate(ctx.validation_stages):
        stage_samples = samples_per_stage
        # Handle remainder samples
        if stage_idx < (total_samples % num_stages):
            stage_samples += 1
            
        stage_type = stage_config.get_stage_type()
        stage_info = f"  Stage {stage_idx} ({stage_type.value}): {stage_samples} samples"
        if stage_type == UnmaskingStageType.STICKY:
            config = stage_config.config
            stage_info += f" (target_ratio={config.target_masked_ratio:.1f}, p1={config.p1_probability:.1f}, p2={config.p2_probability:.1f})"
        elif stage_type == UnmaskingStageType.RANDOM:
            config = stage_config.config
            stage_info += f" (max_ratio={config.max_masked_ratio:.1f})"
        print(stage_info)
        
        # Generate batches for this stage
        stage_batches = []
        samples_generated = 0
        
        while samples_generated < stage_samples:
            batch_size = min(ctx.batch_size, stage_samples - samples_generated)
            
            # Sample data indices
            if len(valid_indices) == 0:
                ix_np = torch.randint(len(data) - ctx.block_size, (batch_size,)).numpy()
            else:
                ix_indices = torch.randint(len(valid_indices), (batch_size,)).numpy()
                ix_np = valid_indices[ix_indices]
            
            # Load data with vectorized indexing
            ix_expanded = ix_np[:, None] + np.arange(ctx.block_size)[None, :]  # (batch_size, block_size)
            x_np = data[ix_expanded].astype(np.int64)
            
            # Convert to tensor
            x = torch.from_numpy(x_np)
            if ctx.device_type == 'cuda':
                x = x.pin_memory().to(ctx.device, non_blocking=True)
            else:
                x = x.to(ctx.device)
            
            # Apply stage-specific masking
            masked_x, mask = apply_stage_masking(x, stage_config, ctx.mask_token_id, ctx.meta_vocab_size)
            
            # Apply label smoothing to targets if enabled
            y = x.clone()
            if ctx.uncertainty_factor > 0.0:
                # Determine special token IDs to exclude from smoothing
                special_token_ids = []
                if ctx.mask_token_id is not None and ctx.mask_token_id < ctx.extended_vocab_size:
                    special_token_ids.append(ctx.mask_token_id)
                if ctx.wrong_token_id is not None and ctx.wrong_token_id < ctx.extended_vocab_size:
                    special_token_ids.append(ctx.wrong_token_id)
                if ctx.remask_good_id is not None and ctx.remask_good_id < ctx.extended_vocab_size:
                    special_token_ids.append(ctx.remask_good_id)
                if ctx.remask_wrong_id is not None and ctx.remask_wrong_id < ctx.extended_vocab_size:
                    special_token_ids.append(ctx.remask_wrong_id)
                
                y = apply_label_smoothing(y, ctx.uncertainty_factor, ctx.extended_vocab_size, 
                                          special_token_ids=special_token_ids, device=ctx.device)
            
            stage_batches.append((masked_x.clone(), y.clone(), mask.clone()))
            samples_generated += batch_size
        
        validation_batches.extend(stage_batches)
    
    torch.manual_seed(1337 + ctx.seed_offset)  # Reset seed
    _unmasking_val_set = validation_batches
    print(f"Validation set created: {len(validation_batches)} batches, {sum(b[0].size(0) for b in validation_batches)} total samples")

def get_unmasking_validation_batch(ctx: TrainingContext, batch_idx=None):
    """Get a specific batch from the pre-created validation set"""
    global _unmasking_val_set
    
    if _unmasking_val_set is None:
        create_unmasking_validation_set(ctx)
    
    if batch_idx is None:
        batch_idx = 0
    
    # Handle batch index wrapping
    batch_idx = batch_idx % len(_unmasking_val_set)
    return _unmasking_val_set[batch_idx]

def get_unmasking_training_batch_all_stages(ctx: TrainingContext):
    """Generate fresh training batch with samples distributed across all stages"""
    global _data_cache, _valid_indices_cache
    
    # Cache training data if not already cached
    if _data_cache['train'] is None:
        _data_cache['train'] = np.memmap(os.path.join(ctx.data_dir, 'train.bin'), dtype=np.uint16, mode='r')
        if ctx.use_paragraph_boundaries:
            _valid_indices_cache['train'] = find_double_newline_indices(_data_cache['train'], ctx.meta_vocab_size, ctx.block_size)
        else:
            _valid_indices_cache['train'] = np.array([])
    
    data = _data_cache['train']
    valid_indices = _valid_indices_cache['train']
    
    num_stages = len(ctx.unmasking_stages)
    samples_per_stage = ctx.batch_size // num_stages
    remainder = ctx.batch_size % num_stages
    
    # Create mixed batch with samples from all stages
    all_masked_x = []
    all_x = []
    all_masks = []
    
    for stage_idx, stage_config in enumerate(ctx.unmasking_stages):
        # Determine number of samples for this stage
        stage_samples = samples_per_stage + (1 if stage_idx < remainder else 0)
        
        if stage_samples > 0:
            # Sample data indices for this stage
            if len(valid_indices) == 0:
                ix_np = torch.randint(len(data) - ctx.block_size, (stage_samples,)).numpy()
            else:
                ix_indices = torch.randint(len(valid_indices), (stage_samples,)).numpy()
                ix_np = valid_indices[ix_indices]
            
            # Load data with vectorized indexing
            ix_expanded = ix_np[:, None] + np.arange(ctx.block_size)[None, :]  # (stage_samples, block_size)
            x_np = data[ix_expanded].astype(np.int64)
            
            # Convert to tensor
            x = torch.from_numpy(x_np)
            if ctx.device_type == 'cuda':
                x = x.pin_memory().to(ctx.device, non_blocking=True)
            else:
                x = x.to(ctx.device)
            
            # Apply masking for this stage
            masked_x, mask = apply_stage_masking(x, stage_config, ctx.mask_token_id, ctx.meta_vocab_size)
            
            # Apply label smoothing to targets if enabled
            y = x.clone()
            if ctx.uncertainty_factor > 0.0:
                # Determine special token IDs to exclude from smoothing
                special_token_ids = []
                if ctx.mask_token_id is not None and ctx.mask_token_id < ctx.extended_vocab_size:
                    special_token_ids.append(ctx.mask_token_id)
                if ctx.wrong_token_id is not None and ctx.wrong_token_id < ctx.extended_vocab_size:
                    special_token_ids.append(ctx.wrong_token_id)
                if ctx.remask_good_id is not None and ctx.remask_good_id < ctx.extended_vocab_size:
                    special_token_ids.append(ctx.remask_good_id)
                if ctx.remask_wrong_id is not None and ctx.remask_wrong_id < ctx.extended_vocab_size:
                    special_token_ids.append(ctx.remask_wrong_id)
                
                y = apply_label_smoothing(y, ctx.uncertainty_factor, ctx.extended_vocab_size, 
                                          special_token_ids=special_token_ids, device=ctx.device)
            
            all_masked_x.append(masked_x)
            all_x.append(y)  # Use smoothed targets
            all_masks.append(mask)
    
    # Concatenate all stages back into batch
    final_masked_x = torch.cat(all_masked_x, dim=0)
    final_x = torch.cat(all_x, dim=0)
    final_mask = torch.cat(all_masks, dim=0)
    
    # Shuffle the batch to mix stages randomly
    perm = torch.randperm(final_masked_x.size(0))
    final_masked_x = final_masked_x[perm]
    final_x = final_x[perm]
    final_mask = final_mask[perm]
    
    return final_masked_x, final_x, final_mask

def create_remasking_validation_set(ctx: TrainingContext, force_recreate=False):
    """Create complete validation set for remasking training with progressive corruption intensities"""
    global _remasking_val_set, _data_cache, _valid_indices_cache
    
    if _remasking_val_set is not None and not force_recreate:
        print("Using existing remasking validation set from cache")
        return  # Already created
    
    if force_recreate:
        print("Force recreating remasking validation set...")
        _remasking_val_set = None
    
    print("Creating remasking validation set with progressive corruption intensities...")
    
    # Cache validation data if not already cached
    if _data_cache['val'] is None:
        _data_cache['val'] = np.memmap(os.path.join(ctx.data_dir, 'val.bin'), dtype=np.uint16, mode='r')
        if ctx.use_paragraph_boundaries:
            _valid_indices_cache['val'] = find_double_newline_indices(_data_cache['val'], ctx.meta_vocab_size, ctx.block_size)
        else:
            _valid_indices_cache['val'] = np.array([])
    
    data = _data_cache['val']
    valid_indices = _valid_indices_cache['val']
    
    total_samples = ctx.eval_iters * ctx.batch_size
    validation_batches = []
    
    # Use fixed seed for reproducible validation set
    torch.manual_seed(42)  
    
    # Use a fixed mid-training corruption level for consistent validation
    # This represents a reasonable difficulty level for evaluation
    fixed_validation_iter = ctx.max_iters // 2  # Mid-training corruption level
    
    # Calculate what the corruption rate will be for validation
    corruption_min = 1.0 - ctx.guaranteed_unmasked_max
    corruption_max = 1.0 - ctx.guaranteed_unmasked_min
    if fixed_validation_iter < ctx.random_mask_warmup:
        progress = fixed_validation_iter / ctx.random_mask_warmup
        val_corruption_rate = corruption_min + progress * (corruption_max - corruption_min)
    else:
        val_corruption_rate = corruption_max
    
    print(f"  Using fixed validation corruption level: iter {fixed_validation_iter} = {val_corruption_rate:.1%} corruption")
    
    for k in range(ctx.eval_iters):
        # Sample data indices
        if len(valid_indices) == 0:
            ix_np = torch.randint(len(data) - ctx.block_size, (ctx.batch_size,)).numpy()
        else:
            ix_indices = torch.randint(len(valid_indices), (ctx.batch_size,)).numpy()
            ix_np = valid_indices[ix_indices]
        
        # Load data with vectorized indexing
        ix_expanded = ix_np[:, None] + np.arange(ctx.block_size)[None, :]
        x_np = data[ix_expanded].astype(np.int64)
        
        # Convert to tensor
        x = torch.from_numpy(x_np)
        if ctx.device_type == 'cuda':
            x = x.pin_memory().to(ctx.device, non_blocking=True)
        else:
            x = x.to(ctx.device)
        
        # Apply corruption using the fixed validation iteration (disable debug to avoid spam)
        corrupted_x, mask = apply_corruption_gpu(x, fixed_validation_iter, ctx.guaranteed_unmasked_max, ctx.guaranteed_unmasked_min,
                                                ctx.sticky_transition_start, ctx.sticky_transition_end, ctx.meta_vocab_size, 
                                                ctx.random_mask_warmup, ctx.p1_p2_ratio, debug=False)
        
        if ctx.training_type == 'remasking_binary':
            # Binary targets: 0=keep, 1=remask
            y = torch.full_like(x, ctx.remask_good_id)
            y[mask] = ctx.remask_wrong_id
        else:  # remasking
            # Target: original tokens at correct positions, wrong_token_id at corrupted positions
            y = x.clone()
            y[mask] = ctx.wrong_token_id
        
        # Apply label smoothing if enabled
        if ctx.uncertainty_factor > 0.0:
            if ctx.training_type == 'remasking_binary':
                # For binary classification, smooth over the 2 classes
                y = apply_label_smoothing(y, ctx.uncertainty_factor, ctx.extended_vocab_size, 
                                          device=ctx.device)
            else:  # remasking
                # For remasking, exclude special tokens from smoothing
                special_token_ids = []
                if ctx.wrong_token_id is not None and ctx.wrong_token_id < ctx.extended_vocab_size:
                    special_token_ids.append(ctx.wrong_token_id)
                if ctx.mask_token_id is not None and ctx.mask_token_id < ctx.extended_vocab_size:
                    special_token_ids.append(ctx.mask_token_id)
                if ctx.remask_good_id is not None and ctx.remask_good_id < ctx.extended_vocab_size:
                    special_token_ids.append(ctx.remask_good_id)
                if ctx.remask_wrong_id is not None and ctx.remask_wrong_id < ctx.extended_vocab_size:
                    special_token_ids.append(ctx.remask_wrong_id)
                
                y = apply_label_smoothing(y, ctx.uncertainty_factor, ctx.extended_vocab_size, 
                                          special_token_ids=special_token_ids, device=ctx.device)
        
        validation_batches.append((corrupted_x.clone(), y.clone(), mask.clone()))
    
    torch.manual_seed(1337 + ctx.seed_offset)  # Reset seed
    _remasking_val_set = validation_batches
    print(f"Remasking validation set created: {len(validation_batches)} batches, {sum(b[0].size(0) for b in validation_batches)} total samples")

def get_remasking_validation_batch(ctx: TrainingContext, batch_idx=None):
    """Get a specific batch from the pre-created remasking validation set"""
    global _remasking_val_set
    
    if _remasking_val_set is None:
        create_remasking_validation_set(ctx, force_recreate=False)
    
    if batch_idx is None:
        batch_idx = 0
    
    # Handle batch index wrapping
    batch_idx = batch_idx % len(_remasking_val_set)
    return _remasking_val_set[batch_idx]


# In train_utils.py, modify this function
def apply_random_masking_gpu(x, max_masked_ratio, mask_token_id, meta_vocab_size):
    """
    GPU-optimized random masking for unmasking training.
    Each sample in the batch gets a different random masking probability.
    """
    batch_size, seq_len = x.shape
    device = x.device
    
    mask_probs = torch.rand(batch_size, device=device) * max_masked_ratio
    rand_vals = torch.rand_like(x, dtype=torch.float, device=device)
    mask_probs_expanded = mask_probs.unsqueeze(1).expand(-1, seq_len)
    
    # Step 1: Generate the boolean mask of positions to predict (this logic is unchanged)
    mask = rand_vals < mask_probs_expanded
    
    # Step 2: Apply the 80/10/10 corruption using the new function
    # NOTE: We use meta_vocab_size to avoid generating special tokens randomly
    corrupted_x = apply_bert_style_corruption_gpu(x, mask, mask_token_id, meta_vocab_size)
    
    return corrupted_x, mask

def apply_stage_masking(x, stage_config: UnmaskingStage, mask_token_id, meta_vocab_size):
    """
    Apply masking based on stage configuration type.
    
    Args:
        x: Input tokens (batch_size, seq_len)
        stage_config: UnmaskingStage configuration
        mask_token_id: Token ID to use for masking
        meta_vocab_size: Size of original vocabulary (for random token generation, excluding special tokens)
        
    Returns:
        masked_x: Input with masked tokens replaced by mask_token_id
        mask: Boolean mask indicating which positions were masked
    """
    stage_type = stage_config.get_stage_type()
    
    if stage_type == UnmaskingStageType.RANDOM:
        config = stage_config.config
        return apply_random_masking_gpu(x, config.max_masked_ratio, mask_token_id, meta_vocab_size)
    elif stage_type == UnmaskingStageType.STICKY:
        config = stage_config.config
        return apply_target_driven_sticky_masking_gpu(
            x, config.target_masked_ratio, config.p1_probability, 
            config.p2_probability, mask_token_id
        )
    else:
        raise ValueError(f"Unknown stage type: {stage_type}")

def apply_target_driven_sticky_masking_gpu(x, target_masked_ratio, p1_probability, p2_probability, mask_token_id):
    """
    GPU-optimized target-driven sticky masking for unmasking training.
    
    Args:
        x: Input tokens (batch_size, seq_len)
        target_masked_ratio: Target fraction of tokens to mask (0.0 to 1.0)
        p1_probability: Probability of masking when no neighbors are masked
        p2_probability: Probability of masking when neighbors are masked
        mask_token_id: Token ID to use for masking
        
    Returns:
        masked_x: Input with masked tokens replaced by mask_token_id
        mask: Boolean mask indicating which positions were masked
    """
    batch_size, seq_len = x.shape
    device = x.device
    
    # Calculate target number of masked tokens per sequence
    target_masked_count = int(target_masked_ratio * seq_len)
    
    if target_masked_count == 0:
        # No masking needed - return early
        return x.clone(), torch.zeros_like(x, dtype=torch.bool, device=device)
    
    # Start with no masks
    masked_x = x.clone()
    
    # Pre-allocate tensors to avoid repeated allocations
    current_mask = torch.zeros_like(x, dtype=torch.bool, device=device)
    neighbor_masked = torch.zeros_like(x, dtype=torch.bool, device=device)
    
    # Continue masking until we reach the target for each sequence
    max_rounds = min(1000, target_masked_count * 10)  # Adaptive safety limit
    target_tensor = torch.tensor(target_masked_count, device=device, dtype=torch.long)
    
    for round_idx in range(max_rounds):
        # Update current mask state
        current_mask = (masked_x == mask_token_id)
        
        # Check if we've reached target for all sequences (GPU-only operation)
        current_counts = current_mask.sum(dim=1)  # (batch_size,)
        sequences_need_more = current_counts < target_tensor
        
        if not sequences_need_more.any():
            break  # All sequences reached target
        
        # Find neighbor positions for sticky masking (reuse buffer)
        neighbor_masked.zero_()
        
        # Check left and right neighbors (vectorized)
        neighbor_masked[:, 1:] |= current_mask[:, :-1]  # Left neighbor
        neighbor_masked[:, :-1] |= current_mask[:, 1:]  # Right neighbor
        
        # Generate random values for masking decision (single GPU call)
        rand_vals = torch.rand_like(x, dtype=torch.float, device=device)
        
        # Apply different probabilities based on neighbor status (vectorized)
        mask_probs = torch.where(neighbor_masked, p2_probability, p1_probability)
        new_masks = (rand_vals < mask_probs) & ~current_mask
        
        # Only mask sequences that haven't reached target yet (vectorized)
        sequences_need_more_expanded = sequences_need_more.unsqueeze(1).expand(-1, seq_len)
        new_masks &= sequences_need_more_expanded
        
        # Apply new masks (vectorized)
        masked_x[new_masks] = mask_token_id
    
    # Final adjustment: remove excess masks with fully vectorized approach
    final_mask = (masked_x == mask_token_id)
    final_counts = final_mask.sum(dim=1)  # (batch_size,)
    
    # Only process sequences that exceeded target (minimize CPU-GPU sync)
    exceeded_sequences = torch.where(final_counts > target_tensor)[0]
    
    if exceeded_sequences.numel() > 0:
        # Process exceeded sequences with minimal loops
        for batch_idx in exceeded_sequences:
            excess = (final_counts[batch_idx] - target_tensor).item()
            if excess > 0:
                # Find masked positions (keep on GPU)
                seq_mask = final_mask[batch_idx]
                masked_positions = torch.where(seq_mask)[0]
                
                # Randomly select positions to unmask (single GPU operation)
                perm_indices = torch.randperm(masked_positions.size(0), device=device)[:excess]
                positions_to_unmask = masked_positions[perm_indices]
                
                # Restore original tokens (vectorized)
                masked_x[batch_idx, positions_to_unmask] = x[batch_idx, positions_to_unmask]
    
    # Return final mask state
    final_mask = (masked_x == mask_token_id)
    return masked_x, final_mask





def load_synthetic_model(checkpoint_path, device, extended_vocab_size):
    """Load the synthetic model for generating fake data in remasking training"""
    global synthetic_model
    
    if not checkpoint_path or synthetic_model is not None:
        return
    
    try:
        print(f"Loading synthetic model from {checkpoint_path}")
        checkpoint = torch.load(checkpoint_path, map_location=device)
        
        # Extract model arguments from checkpoint
        checkpoint_model_args = checkpoint['model_args']
        
        # Create synthetic model with same architecture as checkpoint
        synthetic_gptconf = GPTConfig(**checkpoint_model_args)
        synthetic_model = GPT(synthetic_gptconf)
        
        # Load state dict
        state_dict = checkpoint['model']
        # Fix keys if needed (same as main model loading)
        unwanted_prefix = '_orig_mod.'
        for k,v in list(state_dict.items()):
            if k.startswith(unwanted_prefix):
                state_dict[k[len(unwanted_prefix):]] = state_dict.pop(k)
        
        synthetic_model.load_state_dict(state_dict)
        synthetic_model.to(device)
        synthetic_model.eval()  # Always in eval mode
        
        print(f"Synthetic model loaded successfully (vocab_size: {synthetic_model.config.vocab_size})")
        
    except Exception as e:
        print(f"Warning: Could not load synthetic model from {checkpoint_path}: {e}")
        synthetic_model = None

# Deprecated synthetic corruption removed - only random corruption supported

def apply_sticky_corruption_gpu(x, target_masked_ratio, p1_probability, p2_probability, meta_vocab_size, debug=True):
    """Sticky corruption for remasking training with target-driven masking"""
    batch_size, seq_len = x.shape
    device = x.device
    
    # Calculate target number of masked tokens per sequence
    target_masked_count = int(target_masked_ratio * seq_len)
    
    if target_masked_count == 0:
        # No masking needed - return original unchanged
        return x.clone(), torch.zeros_like(x, dtype=torch.bool, device=device)
    
    # Start with original text
    corrupted_x = x.clone()
    
    # Pre-allocate tensors to avoid repeated allocations
    current_mask = torch.zeros_like(x, dtype=torch.bool, device=device)
    neighbor_masked = torch.zeros_like(x, dtype=torch.bool, device=device)
    
    # Continue masking until we reach the target for each sequence
    max_rounds = min(1000, target_masked_count * 10)  # Adaptive safety limit
    target_tensor = torch.tensor(target_masked_count, device=device, dtype=torch.long)
    
    for round_idx in range(max_rounds):
        # Update current mask state (positions that are corrupted)
        current_mask = (corrupted_x != x)  # True where tokens have been corrupted
        
        # Check if we've reached target for all sequences (GPU-only operation)
        current_counts = current_mask.sum(dim=1)  # (batch_size,)
        sequences_need_more = current_counts < target_tensor
        
        if not sequences_need_more.any():
            break  # All sequences reached target
        
        # Find neighbor positions for sticky masking (reuse buffer)
        neighbor_masked.zero_()
        
        # Check left and right neighbors (vectorized)
        neighbor_masked[:, 1:] |= current_mask[:, :-1]  # Left neighbor
        neighbor_masked[:, :-1] |= current_mask[:, 1:]  # Right neighbor
        
        # Generate random values for masking decision (single GPU call)
        rand_vals = torch.rand_like(x, dtype=torch.float, device=device)
        
        # Apply different probabilities based on neighbor status (vectorized)
        mask_probs = torch.where(neighbor_masked, p2_probability, p1_probability)
        new_masks = (rand_vals < mask_probs) & ~current_mask
        
        # Only mask sequences that haven't reached target yet (vectorized)
        sequences_need_more_expanded = sequences_need_more.unsqueeze(1).expand(-1, seq_len)
        new_masks &= sequences_need_more_expanded
        
        # Apply corruption to newly masked positions (vectorized)
        if new_masks.any():
            # Replace with random tokens from vocabulary
            random_tokens = torch.randint(0, meta_vocab_size, new_masks.sum().shape, device=device)
            corrupted_x[new_masks] = random_tokens
    
    # Final adjustment: remove excess corruptions with fully vectorized approach
    final_mask = (corrupted_x != x)
    final_counts = final_mask.sum(dim=1)  # (batch_size,)
    
    # Only process sequences that exceeded target (minimize CPU-GPU sync)
    exceeded_sequences = torch.where(final_counts > target_tensor)[0]
    
    if exceeded_sequences.numel() > 0:
        # Process exceeded sequences with minimal loops
        for batch_idx in exceeded_sequences:
            excess = (final_counts[batch_idx] - target_tensor).item()
            if excess > 0:
                # Find corrupted positions (keep on GPU)
                seq_mask = final_mask[batch_idx]
                corrupted_positions = torch.where(seq_mask)[0]
                
                # Randomly select positions to restore (single GPU operation)
                perm_indices = torch.randperm(corrupted_positions.size(0), device=device)[:excess]
                positions_to_restore = corrupted_positions[perm_indices]
                
                # Restore original tokens (vectorized)
                corrupted_x[batch_idx, positions_to_restore] = x[batch_idx, positions_to_restore]
    
    # Return final mask state
    final_mask = (corrupted_x != x)
    return corrupted_x, final_mask

def apply_random_corruption_gpu(x, iter_num, guaranteed_unmasked_max, guaranteed_unmasked_min, sticky_transition_start, sticky_transition_end, meta_vocab_size, random_mask_warmup, debug=True):
    """Random corruption for remasking training with iteration-based masking probability"""
    batch_size, seq_len = x.shape
    device = x.device
    
    # FIXED: Convert "unmasked" parameters to actual corruption probabilities
    # guaranteed_unmasked_max=0.9 means 90% unmasked -> 10% corrupted
    # guaranteed_unmasked_min=0.6 means 60% unmasked -> 40% corrupted
    corruption_min = 1.0 - guaranteed_unmasked_max  # Start: 10% corruption
    corruption_max = 1.0 - guaranteed_unmasked_min  # End: 40% corruption
    
    # Calculate masking probability based on iteration
    if iter_num < random_mask_warmup:
        # During warmup, gradually increase corruption from min to max
        progress = iter_num / random_mask_warmup
        mask_prob = corruption_min + progress * (corruption_max - corruption_min)
    elif iter_num < sticky_transition_start:
        mask_prob = corruption_max  # Maximum corruption (40%)
    elif iter_num < sticky_transition_end:
        # Keep maximum corruption during transition
        mask_prob = corruption_max
    else:
        mask_prob = corruption_max  # Stay at maximum corruption
    
    # Apply random masking
    rand_vals = torch.rand_like(x, dtype=torch.float, device=device)
    mask = rand_vals < mask_prob
    
    # Debug: Print corruption rate occasionally during training only (not validation set creation)
    if debug and iter_num % 1000 == 0 and iter_num > 0:
        actual_mask_ratio = mask.float().mean().item()
        print(f"DEBUG: iter {iter_num}, target_corruption={mask_prob:.3f} ({mask_prob*100:.1f}%), actual={actual_mask_ratio:.3f} ({actual_mask_ratio*100:.1f}%)")
        print(f"  corruption_min={corruption_min:.3f}, corruption_max={corruption_max:.3f}, warmup={random_mask_warmup}")
    
    # Create corrupted version by randomly replacing masked tokens
    corrupted_x = x.clone()
    if mask.any():
        # Replace with random tokens from vocabulary
        random_tokens = torch.randint(0, meta_vocab_size, mask.sum().shape, device=device)
        corrupted_x[mask] = random_tokens
    
    return corrupted_x, mask

def apply_corruption_gpu(x, iter_num, guaranteed_unmasked_max, guaranteed_unmasked_min, sticky_transition_start, sticky_transition_end, meta_vocab_size, random_mask_warmup, p1_p2_ratio=1.0, debug=True):
    """Unified corruption function that chooses between random and sticky masking based on p1_p2_ratio"""
    batch_size, seq_len = x.shape
    device = x.device
    
    # Calculate current corruption probability based on iteration
    corruption_min = 1.0 - guaranteed_unmasked_max  # Start: 10% corruption
    corruption_max = 1.0 - guaranteed_unmasked_min  # End: 40% corruption
    
    if iter_num < random_mask_warmup:
        # During warmup, gradually increase corruption from min to max
        progress = iter_num / random_mask_warmup
        target_corruption_rate = corruption_min + progress * (corruption_max - corruption_min)
    elif iter_num < sticky_transition_start:
        target_corruption_rate = corruption_max  # Maximum corruption
    elif iter_num < sticky_transition_end:
        target_corruption_rate = corruption_max  # Keep maximum corruption during transition
    else:
        target_corruption_rate = corruption_max  # Stay at maximum corruption
    
    # Choose masking strategy based on p1_p2_ratio
    if p1_p2_ratio == 1.0:
        # Use random corruption
        return apply_random_corruption_gpu(x, iter_num, guaranteed_unmasked_max, guaranteed_unmasked_min, 
                                         sticky_transition_start, sticky_transition_end, meta_vocab_size, 
                                         random_mask_warmup, debug=debug)
    else:
        # Use sticky corruption
        # Calculate p1 and p2 based on ratio, with max(p1, p2) = target_corruption_rate / 4
        max_prob = target_corruption_rate / 4.0
        
        if p1_p2_ratio > 1.0:
            # p1 is larger
            p1_probability = max_prob
            p2_probability = max_prob / p1_p2_ratio
        else:
            # p2 is larger
            p2_probability = max_prob
            p1_probability = max_prob * p1_p2_ratio
        
        if debug and iter_num % 1000 == 0 and iter_num > 0:
            print(f"DEBUG: iter {iter_num}, sticky masking: target_corruption={target_corruption_rate:.3f} ({target_corruption_rate*100:.1f}%)")
            print(f"  p1_p2_ratio={p1_p2_ratio:.3f}, p1={p1_probability:.3f}, p2={p2_probability:.3f}")
        
        return apply_sticky_corruption_gpu(x, target_corruption_rate, p1_probability, p2_probability, meta_vocab_size, debug=debug)

# Deprecated sticky corruption removed - random and new sticky corruption now supported

# Deprecated fragment corruption removed - only random corruption supported

def get_progressive_validation_iterations(eval_iters, max_iters):
    """Generate validation iterations for progressive validation"""
    # Create a range of iterations from early to late training
    iterations = []
    for i in range(eval_iters):
        progress = i / (eval_iters - 1) if eval_iters > 1 else 0
        iter_val = int(progress * max_iters)
        iterations.append(iter_val)
    return iterations

def get_batch(split, ctx: TrainingContext, validation_sample_idx=None):
    """Main batch generation function that delegates to specific training type functions"""
    if ctx.training_type == 'unmasking':
        return get_batch_unmasking(split, ctx, validation_sample_idx)
    elif ctx.training_type == 'remasking_binary':
        return get_batch_remasking_binary(split, ctx, validation_sample_idx)
    else:
        raise ValueError(f"Unsupported training type: {ctx.training_type}")

def get_batch_unmasking(split, ctx: TrainingContext, validation_sample_idx=None):
    """Stage-based unmasking with target-driven sticky masking"""
    global _val_batch_cache, _progressive_val_cache, _data_cache, _valid_indices_cache, _unmasking_val_set

    # For validation, use the pre-created validation set distributed across all stages
    if split == 'val':
        return get_unmasking_validation_batch(ctx, validation_sample_idx)
    
    # For training, check if we should generate batch from all stages
    if split == 'train' and ctx.use_all_stages_for_training:
        return get_unmasking_training_batch_all_stages(ctx)

    # Cache memory-mapped data and valid indices - MAJOR SPEEDUP
    if _data_cache[split] is None:
        if split == 'train':
            _data_cache[split] = np.memmap(os.path.join(ctx.data_dir, 'train.bin'), dtype=np.uint16, mode='r')
        else:
            _data_cache[split] = np.memmap(os.path.join(ctx.data_dir, 'val.bin'), dtype=np.uint16, mode='r')
        
        # Cache the expensive valid indices computation
        if ctx.use_paragraph_boundaries:
            print(f"Computing valid indices for {split} (paragraph boundaries)... (one-time cost)")
            _valid_indices_cache[split] = find_double_newline_indices(_data_cache[split], ctx.meta_vocab_size, ctx.block_size)
        else:
            print(f"Using random sampling for {split} (no paragraph boundaries)")
            _valid_indices_cache[split] = np.array([])  # Empty array indicates random sampling
        print(f"Found {len(_valid_indices_cache[split])} valid indices for {split}")
        
        # Start prefetching for training data
        if split == 'train':
            start_prefetch(ctx)

    # Try to get prefetched data for training
    x_np = None
    if split == 'train' and _prefetch_enabled:
        try:
            x_np = _prefetch_queue.get_nowait()
        except:
            pass  # Queue empty, generate normally
    
    # Generate data if not prefetched
    if x_np is None:
        data = _data_cache[split]
        valid_indices = _valid_indices_cache[split]
        
        # Fast index sampling - all on CPU to avoid GPU-CPU sync
        if len(valid_indices) == 0:
            if split == 'val':
                torch.manual_seed(42)
                ix_np = torch.randint(len(data) - ctx.block_size, (ctx.batch_size,)).numpy()
                torch.manual_seed(1337 + ctx.seed_offset)
            else:
                ix_np = torch.randint(len(data) - ctx.block_size, (ctx.batch_size,)).numpy()
        else:
            if split == 'val':
                torch.manual_seed(42)
                ix_indices = torch.randint(len(valid_indices), (ctx.batch_size,)).numpy()
                ix_np = valid_indices[ix_indices]
                torch.manual_seed(1337 + ctx.seed_offset)
            else:
                ix_indices = torch.randint(len(valid_indices), (ctx.batch_size,)).numpy()
                ix_np = valid_indices[ix_indices]

        # VECTORIZED DATA LOADING - Use advanced indexing for parallel loading
        ix_expanded = ix_np[:, None] + np.arange(ctx.block_size)[None, :]  # (batch_size, block_size)
        x_np = data[ix_expanded].astype(np.int64)
    
    # Single GPU transfer with pinned memory
    x = torch.from_numpy(x_np)
    if ctx.device_type == 'cuda':
        x = x.pin_memory().to(ctx.device, non_blocking=True)
    else:
        x = x.to(ctx.device)

    # For validation: create samples from all stages for comprehensive evaluation
    if split == 'val':
        torch.manual_seed(42 + (validation_sample_idx or 0))
        
        # Distribute validation samples across all validation stages
        if ctx.validation_stages and len(ctx.validation_stages) > 1:
            stage_idx = (validation_sample_idx or 0) % len(ctx.validation_stages)
            stage_config = ctx.validation_stages[stage_idx]
            # Apply stage-specific masking
            masked_x, mask = apply_stage_masking(x, stage_config, ctx.mask_token_id, ctx.meta_vocab_size)
        else:
            # Fallback to current stage if no stages defined
            stage_config = ctx.get_current_stage_config()
            if stage_config is None:
                raise ValueError(f"No stage configuration available for {ctx.training_type} training")
            # Apply stage-specific masking
            masked_x, mask = apply_stage_masking(x, stage_config, ctx.mask_token_id, ctx.meta_vocab_size)
    else:
        # For training: distribute batch samples across all stages up to current stage (inclusive)
        if ctx.unmasking_stages and ctx.current_stage >= 0:
            # Get all available stages up to current stage
            available_stages = ctx.unmasking_stages[:ctx.current_stage + 1]
            num_stages = len(available_stages)
            
            # Distribute batch_size samples evenly across all available stages
            samples_per_stage = ctx.batch_size // num_stages
            remainder = ctx.batch_size % num_stages
            
            # Create mixed batch with samples from all stages
            all_masked_x = []
            all_masks = []
            
            start_idx = 0
            for stage_idx, stage_config in enumerate(available_stages):
                # Determine number of samples for this stage
                stage_samples = samples_per_stage + (1 if stage_idx < remainder else 0)
                
                if stage_samples > 0:
                    # Get DIFFERENT subset of data for this stage (FIX: was x[:stage_samples])
                    stage_x = x[start_idx:start_idx + stage_samples]
                    start_idx += stage_samples
                    
                    # Apply masking for this stage
                    stage_masked_x, stage_mask = apply_stage_masking(stage_x, stage_config, ctx.mask_token_id, ctx.meta_vocab_size)
                    
                    all_masked_x.append(stage_masked_x)
                    all_masks.append(stage_mask)
            
            # Concatenate all stages back into batch
            masked_x = torch.cat(all_masked_x, dim=0)
            mask = torch.cat(all_masks, dim=0)
            
            # Shuffle the batch to mix stages randomly
            perm = torch.randperm(masked_x.size(0))
            masked_x = masked_x[perm]
            mask = mask[perm]
            x = x[perm]  # Also permute original x to match
            
            # Log stage distribution occasionally for training only
            if split == 'train' and ctx.iter_num % 1500 == 0 and ctx.iter_num > 0:
                stage_counts = [samples_per_stage + (1 if i < remainder else 0) for i in range(num_stages)]
                print(f"Training iter {ctx.iter_num}: Mixed batch from {num_stages} stages: {stage_counts}")
        else:
            # Fallback to current stage configuration
            stage_config = ctx.get_current_stage_config()
            if stage_config is None:
                raise ValueError(f"No stage configuration available for {ctx.training_type} training")
            
            # Apply single stage masking (fallback case)
            masked_x, mask = apply_stage_masking(x, stage_config, ctx.mask_token_id, ctx.meta_vocab_size)
    
    if split == 'val':
        torch.manual_seed(1337 + ctx.seed_offset)

    # Target is original x
    y = x.clone()
    
    # Apply label smoothing if enabled
    if ctx.uncertainty_factor > 0.0:
        # Determine special token IDs to exclude from smoothing
        special_token_ids = []
        if ctx.mask_token_id is not None and ctx.mask_token_id < ctx.extended_vocab_size:
            special_token_ids.append(ctx.mask_token_id)
        if ctx.wrong_token_id is not None and ctx.wrong_token_id < ctx.extended_vocab_size:
            special_token_ids.append(ctx.wrong_token_id)
        if ctx.remask_good_id is not None and ctx.remask_good_id < ctx.extended_vocab_size:
            special_token_ids.append(ctx.remask_good_id)
        if ctx.remask_wrong_id is not None and ctx.remask_wrong_id < ctx.extended_vocab_size:
            special_token_ids.append(ctx.remask_wrong_id)
        
        y = apply_label_smoothing(y, ctx.uncertainty_factor, ctx.extended_vocab_size, 
                                  special_token_ids=special_token_ids, device=ctx.device)

    # Cache validation batch for consistency
    if split == 'val':
        if validation_sample_idx is not None:
            cache_key = f"unmasking_{validation_sample_idx}"
            _progressive_val_cache[cache_key] = (masked_x, y, mask)
        else:
            _val_batch_cache = (masked_x, y, mask)

    return masked_x, y, mask

def get_batch_remasking_binary(split, ctx: TrainingContext, validation_sample_idx=None):
    """GPU-optimized remasking binary training: symmetric task with remask_good_id and remask_wrong_id targets"""
    # Use pre-created validation set for validation
    if split == 'val':
        return get_remasking_validation_batch(ctx, validation_sample_idx)
    
    # Training data generation - fast implementation 
    global _data_cache, _valid_indices_cache

    # Use same data caching and prefetching as remasking
    if _data_cache[split] is None:
        if split == 'train':
            _data_cache[split] = np.memmap(os.path.join(ctx.data_dir, 'train.bin'), dtype=np.uint16, mode='r')
        else:
            _data_cache[split] = np.memmap(os.path.join(ctx.data_dir, 'val.bin'), dtype=np.uint16, mode='r')
        
        if ctx.use_paragraph_boundaries:
            print(f"Computing valid indices for {split} (paragraph boundaries)... (one-time cost)")
            _valid_indices_cache[split] = find_double_newline_indices(_data_cache[split], ctx.meta_vocab_size, ctx.block_size)
        else:
            print(f"Using random sampling for {split} (no paragraph boundaries)")
            _valid_indices_cache[split] = np.array([])  # Empty array indicates random sampling
        print(f"Found {len(_valid_indices_cache[split])} valid indices for {split}")
        
        if split == 'train':
            start_prefetch(ctx)

    # Try to get prefetched data for training (reuse existing prefetch system)
    x_np = None
    if split == 'train' and _prefetch_enabled:
        try:
            x_np = _prefetch_queue.get_nowait()
        except:
            pass

    # Generate data if not prefetched (same as remasking)
    if x_np is None:
        data = _data_cache[split]
        valid_indices = _valid_indices_cache[split]
        
        if len(valid_indices) == 0:
            if split == 'val':
                torch.manual_seed(42)
                ix_np = torch.randint(len(data) - ctx.block_size, (ctx.batch_size,)).numpy()
                torch.manual_seed(1337 + ctx.seed_offset)
            else:
                ix_np = torch.randint(len(data) - ctx.block_size, (ctx.batch_size,)).numpy()
        else:
            if split == 'val':
                torch.manual_seed(42)
                ix_indices = torch.randint(len(valid_indices), (ctx.batch_size,)).numpy()
                ix_np = valid_indices[ix_indices]
                torch.manual_seed(1337 + ctx.seed_offset)
            else:
                ix_indices = torch.randint(len(valid_indices), (ctx.batch_size,)).numpy()
                ix_np = valid_indices[ix_indices]

        ix_expanded = ix_np[:, None] + np.arange(ctx.block_size)[None, :]  # (batch_size, block_size)
        x_np = data[ix_expanded].astype(np.int64)
    
    # Single GPU transfer
    x = torch.from_numpy(x_np)
    if ctx.device_type == 'cuda':
        x = x.pin_memory().to(ctx.device, non_blocking=True)
    else:
        x = x.to(ctx.device)

    # Determine which iteration to use for corruption strategy
    if split == 'val' and validation_sample_idx is not None:
        # For progressive validation, use the specific validation iteration
        progressive_iterations = get_progressive_validation_iterations(ctx.eval_iters, ctx.max_iters)
        corruption_iter = progressive_iterations[validation_sample_idx % len(progressive_iterations)]
    else:
        corruption_iter = ctx.iter_num

    # Use unified corruption function (random or sticky based on p1_p2_ratio)
    corrupted_x, mask = apply_corruption_gpu(x, corruption_iter, ctx.guaranteed_unmasked_max, ctx.guaranteed_unmasked_min,
                                           ctx.sticky_transition_start, ctx.sticky_transition_end, ctx.meta_vocab_size, 
                                           ctx.random_mask_warmup, ctx.p1_p2_ratio, debug=True)

    # Binary targets: remask_good_id for uncorrupted, remask_wrong_id for corrupted (already on GPU)
    y = torch.full_like(x, ctx.remask_good_id)
    y[mask] = ctx.remask_wrong_id
    
    # Apply label smoothing if enabled
    if ctx.uncertainty_factor > 0.0:
        # For binary classification, only smooth over the 2 classes (remask_good_id, remask_wrong_id)
        # Don't include other special tokens as they shouldn't appear in binary classification
        y = apply_label_smoothing(y, ctx.uncertainty_factor, ctx.extended_vocab_size, 
                                  device=ctx.device)

    # No caching needed for training - validation uses pre-created set
    return corrupted_x, y, mask

def estimate_loss(model, torch_ctx, timer, training_ctx: TrainingContext):
    """Estimate loss over either split using many batches"""
    out = {}
    model.eval()
    
    # Add current stage information for unmasking training
    if training_ctx.training_type == 'unmasking':
        stage_config = training_ctx.get_current_stage_config()
        if stage_config:
            out['current_stage'] = training_ctx.current_stage
            stage_type = stage_config.get_stage_type()
            out['stage_type'] = stage_type.value
            if stage_type == UnmaskingStageType.STICKY:
                config = stage_config.config
                out['target_masked_ratio'] = config.target_masked_ratio
                out['p1_probability'] = config.p1_probability
                out['p2_probability'] = config.p2_probability
            elif stage_type == UnmaskingStageType.RANDOM:
                config = stage_config.config
                out['max_masked_ratio'] = config.max_masked_ratio
            out['val_loss_stale_count'] = training_ctx.val_loss_stale_count
    
    for split in ['train', 'val']:
        losses = torch.zeros(training_ctx.eval_iters)
        # Track masked token ratios for all splits
        masked_token_ratios = []

        if split == 'val':
            # For validation, also track model vs random performance
            model_probs = []
            # Track signal to noise ratio (correct prob vs most probable incorrect prob)
            signal_to_noise_ratios = []
            # Track detailed probability breakdown for binary classification by class
            right_probs_p0 = []  # Probabilities for correct predictions where target=0
            right_probs_p1 = []  # Probabilities for correct predictions where target=1
            # Track most likely predictions for accuracy calculation
            most_likely_correct = []
            # For binary classification and remasking, track corruption statistics
            if training_ctx.training_type in ['remasking_binary', 'remasking']:
                total_positions = 0
                corrupted_positions = 0
            else:
                random_prob = 1.0 / training_ctx.extended_vocab_size  # Random chance probability

        # For unmasking, use pre-created validation set with samples from all stages
        # Track per-stage losses for detailed analysis
        stage_losses = {}
        stage_sample_counts = {}
        if split == 'val' and training_ctx.training_type == 'unmasking':
            print(f"Using validation set with samples from all {len(training_ctx.validation_stages)} stages")
            # Initialize per-stage tracking
            for stage_idx in range(len(training_ctx.validation_stages)):
                stage_losses[stage_idx] = []
                stage_sample_counts[stage_idx] = 0
        
        for k in range(training_ctx.eval_iters):
            with timer.time_function('validation_data_generation'):
                if split == 'val' and training_ctx.training_type == 'unmasking':
                    # Use pre-created validation set with batch index
                    X, Y, mask = get_batch(split, training_ctx, validation_sample_idx=k)
                    # Determine which stage this batch belongs to based on validation set structure
                    total_samples = training_ctx.eval_iters * training_ctx.batch_size
                    num_stages = len(training_ctx.validation_stages)
                    samples_per_stage = total_samples // num_stages
                    current_sample_idx = k * training_ctx.batch_size
                    current_stage_idx = min(current_sample_idx // samples_per_stage, num_stages - 1)
                elif split == 'val' and training_ctx.training_type in ['remasking_binary', 'remasking']:
                    # Fix: pass validation_sample_idx to get different validation batches
                    X, Y, mask = get_batch(split, training_ctx, validation_sample_idx=k)
                    current_stage_idx = None
                else:
                    X, Y, mask = get_batch(split, training_ctx)
                    current_stage_idx = None

            # Calculate masked token ratio for this batch
            # Get per-sample ratios to capture the full range of masking rates
            sample_ratios = mask.float().mean(dim=1)  # Shape: (batch_size,) - ratio per sample
            masked_token_ratios.extend(sample_ratios.cpu().tolist())  # Add all individual sample ratios

            with torch_ctx:
                with timer.time_function('validation_forward_pass'):
                    # This is handled in validation_loss_computation section
                    pass
                with timer.time_function('validation_loss_computation'):
                    # Optimized single forward pass for validation
                    logits, loss = model(X, Y)

                    # Apply masking for unmasking training only
                    if training_ctx.training_type == 'unmasking' and mask.any():
                        # Fast validation path - single reshape and boolean indexing
                        # Cross-entropy handles both hard targets (indices) and soft targets (probabilities)
                        logits_reshaped = logits.view(-1, logits.size(-1))
                        mask_reshaped = mask.view(-1)
                        
                        if Y.dim() == 3:
                            # Soft targets (probability distributions)
                            targets_reshaped = Y.view(-1, Y.size(-1))
                            loss = torch.nn.functional.cross_entropy(
                                logits_reshaped[mask_reshaped],
                                targets_reshaped[mask_reshaped],
                                reduction='mean'
                            )
                        else:
                            # Hard targets (token indices)
                            targets_reshaped = Y.view(-1)
                            loss = torch.nn.functional.cross_entropy(
                                logits_reshaped[mask_reshaped],
                                targets_reshaped[mask_reshaped],
                                reduction='mean'
                            )
                        
                        # Apply mask ratio weighting if enabled (same as training)
                        if training_ctx.weight_loss_by_mask_ratio:
                            mask_ratio = mask.float().mean().item()
                            if mask_ratio > 0:
                                weight = (1.0 / mask_ratio) ** 0.5  # sqrt(1.0 / mask_ratio)
                                loss = loss * weight
                    # For remasking variants, model's internal loss is correct

                # For validation, compute model vs random statistics
                if split == 'val':
                    # Get probabilities from logits and flatten for statistics
                    probs = torch.nn.functional.softmax(logits, dim=-1)  # (batch_size, seq_len, vocab_size)
                    probs_flat = probs.view(-1, probs.size(-1))  # (batch_size * seq_len, vocab_size)
                    
                    # Handle both hard and soft targets
                    if Y.dim() == 3:
                        # Soft targets - get the most likely class from probability distribution
                        targets_flat = torch.argmax(Y.view(-1, Y.size(-1)), dim=-1)  # (batch_size * seq_len,)
                    else:
                        # Hard targets
                        targets_flat = Y.view(-1)  # (batch_size * seq_len,)
                    
                    # Calculate most likely predictions (argmax)
                    predictions = torch.argmax(probs, dim=-1)  # (batch_size, seq_len)
                    predictions_flat = predictions.view(-1)  # (batch_size * seq_len,)

                    if training_ctx.training_type == 'remasking_binary':
                        # For binary classification, compute accuracy on all positions
                        # Track corruption statistics for proper baseline
                        total_positions += targets_flat.numel()
                        corrupted_positions += (targets_flat == training_ctx.remask_wrong_id).sum().item()
                        
                        # Track validation statistics for summary
                        if split == 'val':
                            # Initialize counters on first batch
                            if k == 0:
                                val_total_class_0, val_total_class_1 = 0, 0
                                val_pred_class_0, val_pred_class_1 = 0, 0
                                val_correct_pred_0, val_correct_pred_1 = 0, 0
                            
                            # Count actual class distribution
                            class_0_count = (targets_flat == 0).sum().item()
                            class_1_count = (targets_flat == 1).sum().item()
                            val_total_class_0 += class_0_count
                            val_total_class_1 += class_1_count
                            
                            # Count predictions
                            pred_0_count = (predictions_flat == 0).sum().item()
                            pred_1_count = (predictions_flat == 1).sum().item()
                            val_pred_class_0 += pred_0_count
                            val_pred_class_1 += pred_1_count
                            
                            # Count correct predictions by class
                            correct_0 = ((predictions_flat == 0) & (targets_flat == 0)).sum().item()
                            correct_1 = ((predictions_flat == 1) & (targets_flat == 1)).sum().item()
                            val_correct_pred_0 += correct_0
                            val_correct_pred_1 += correct_1

                        # Get probabilities for correct binary classification
                        correct_token_probs = probs_flat[range(len(targets_flat)), targets_flat]
                        model_probs.extend(correct_token_probs.cpu().tolist())
                        
                        # Calculate signal to noise ratio for binary classification
                        # For each position, find the most probable incorrect class
                        incorrect_probs = torch.where(targets_flat == 0, probs_flat[:, 1], probs_flat[:, 0])
                        # Calculate signal to noise ratio, capped at 100
                        sn_ratios = torch.clamp(correct_token_probs / (incorrect_probs + 1e-10), max=100.0)
                        signal_to_noise_ratios.extend(sn_ratios.cpu().tolist())
                        
                        # Track detailed probability breakdown by class
                        class_0_mask = (targets_flat == 0)
                        class_1_mask = (targets_flat == 1)
                        
                        # Get probabilities for correct predictions by class
                        if class_0_mask.sum() > 0:
                            class_0_correct_probs = probs_flat[class_0_mask, 0]  # P(class=0) where target=0
                            right_probs_p0.extend(class_0_correct_probs.cpu().tolist())
                        
                        if class_1_mask.sum() > 0:
                            class_1_correct_probs = probs_flat[class_1_mask, 1]  # P(class=1) where target=1
                            right_probs_p1.extend(class_1_correct_probs.cpu().tolist())
                        
                        # Track most likely prediction accuracy for all positions
                        correct_predictions = (predictions_flat == targets_flat).cpu().tolist()
                        most_likely_correct.extend(correct_predictions)
                    elif training_ctx.training_type == 'remasking':
                        # For remasking, compute accuracy on ALL positions (corrupted + uncorrupted)
                        # Track corruption statistics for proper baseline
                        mask_flat = mask.view(-1)  # (batch_size * seq_len,)
                        total_positions += targets_flat.numel()
                        corrupted_positions += mask_flat.sum().item()  # mask indicates corrupted positions

                        # Get probabilities for correct predictions at ALL positions
                        correct_token_probs = probs_flat[range(len(targets_flat)), targets_flat]
                        model_probs.extend(correct_token_probs.cpu().tolist())
                        
                        # Calculate signal to noise ratio for remasking
                        # Create a copy of probabilities and zero out the correct class to find max incorrect
                        probs_masked = probs_flat.clone()
                        probs_masked[range(len(targets_flat)), targets_flat] = 0.0
                        max_incorrect_probs = torch.max(probs_masked, dim=1)[0]
                        # Calculate signal to noise ratio, capped at 100
                        sn_ratios = torch.clamp(correct_token_probs / (max_incorrect_probs + 1e-10), max=100.0)
                        signal_to_noise_ratios.extend(sn_ratios.cpu().tolist())
                        
                        # Track most likely prediction accuracy for all positions
                        correct_predictions = (predictions_flat == targets_flat).cpu().tolist()
                        most_likely_correct.extend(correct_predictions)
                    else:
                        # For unmasking, compute on masked positions only
                        mask_flat = mask.view(-1)  # (batch_size * seq_len,)
                        masked_positions = mask_flat.bool()
                        if masked_positions.sum() > 0:  # Only if there are masked tokens
                            correct_token_probs = probs_flat[masked_positions, targets_flat[masked_positions]]
                            model_probs.extend(correct_token_probs.cpu().tolist())
                            
                            # Calculate signal to noise ratio for unmasking (masked positions only)
                            masked_probs = probs_flat[masked_positions]
                            masked_targets = targets_flat[masked_positions]
                            # Create a copy and zero out correct probabilities to find max incorrect
                            probs_masked = masked_probs.clone()
                            probs_masked[range(len(masked_targets)), masked_targets] = 0.0
                            max_incorrect_probs = torch.max(probs_masked, dim=1)[0]
                            # Calculate signal to noise ratio, capped at 100
                            sn_ratios = torch.clamp(correct_token_probs / (max_incorrect_probs + 1e-10), max=100.0)
                            signal_to_noise_ratios.extend(sn_ratios.cpu().tolist())
                            
                            # Track most likely prediction accuracy for masked positions only
                            correct_predictions = (predictions_flat[masked_positions] == targets_flat[masked_positions]).cpu().tolist()
                            most_likely_correct.extend(correct_predictions)

            losses[k] = loss.item()
            
            # Track per-stage losses for unmasking validation
            if split == 'val' and training_ctx.training_type == 'unmasking' and current_stage_idx is not None:
                stage_losses[current_stage_idx].append(loss.item())
                stage_sample_counts[current_stage_idx] += X.size(0)  # Add batch size

        out[split] = losses.mean()
        
        # Add per-stage validation losses for unmasking
        if split == 'val' and training_ctx.training_type == 'unmasking' and stage_losses:
            for stage_idx, stage_loss_list in stage_losses.items():
                if stage_loss_list:  # Only if we have samples for this stage
                    avg_stage_loss = sum(stage_loss_list) / len(stage_loss_list)
                    out[f'val_stage_{stage_idx}_loss'] = avg_stage_loss
                    out[f'val_stage_{stage_idx}_samples'] = stage_sample_counts[stage_idx]
        
        if split == 'val':
            total_samples = training_ctx.eval_iters * training_ctx.batch_size
            print(f"  Validation complete: {training_ctx.eval_iters} batches processed ({total_samples} samples), avg loss = {out[split]:.4f}")
            
            # Print class distribution summary for binary classification
            if training_ctx.training_type == 'remasking_binary' and 'val_total_class_0' in locals():
                total_targets = val_total_class_0 + val_total_class_1
                total_preds = val_pred_class_0 + val_pred_class_1
                if total_targets > 0 and total_preds > 0:
                    # Class distribution
                    class_0_pct = (val_total_class_0 / total_targets) * 100
                    class_1_pct = (val_total_class_1 / total_targets) * 100
                    
                    # Prediction distribution (for display only)
                    pred_0_pct = (val_pred_class_0 / total_preds) * 100
                    pred_1_pct = (val_pred_class_1 / total_preds) * 100
                    
                    # Accuracy by class
                    acc_0 = (val_correct_pred_0 / val_total_class_0 * 100) if val_total_class_0 > 0 else 0
                    acc_1 = (val_correct_pred_1 / val_total_class_1 * 100) if val_total_class_1 > 0 else 0
                    
                    print(f"  Class distribution: no-mask {val_total_class_0} ({class_0_pct:.1f}%), mask {val_total_class_1} ({class_1_pct:.1f}%)")
                    print(f"  Model predictions: no-mask {val_pred_class_0} ({pred_0_pct:.1f}%), mask {val_pred_class_1} ({pred_1_pct:.1f}%)")
                    print(f"  Accuracy by class: no-mask {acc_0:.1f}%, mask {acc_1:.1f}%")
                    
                    # Print detailed probability breakdown if available
                    if f'{split}_avg_prob_right_p0' in out and f'{split}_avg_prob_right_p1' in out:
                        avg_p_right_p0 = out[f'{split}_avg_prob_right_p0']
                        avg_p_right_p1 = out[f'{split}_avg_prob_right_p1']
                        print(f"  Validation probabilities: avg_p_right_p0={avg_p_right_p0:.3f}, avg_p_right_p1={avg_p_right_p1:.3f}")
                    
                    # Add per-class accuracies and distributions to output for wandb logging
                    out[f'{split}_accuracy_no_mask'] = acc_0
                    out[f'{split}_accuracy_mask'] = acc_1
                    out[f'{split}_class_dist_no_mask'] = class_0_pct
                    out[f'{split}_class_dist_mask'] = class_1_pct
            
            # Print per-stage validation losses for unmasking
            if training_ctx.training_type == 'unmasking' and stage_losses:
                print("  Per-stage validation losses:")
                for stage_idx in range(len(training_ctx.validation_stages)):
                    if stage_idx in stage_losses and stage_losses[stage_idx]:
                        stage_config = training_ctx.validation_stages[stage_idx]
                        stage_type = stage_config.get_stage_type()
                        avg_loss = sum(stage_losses[stage_idx]) / len(stage_losses[stage_idx])
                        sample_count = stage_sample_counts[stage_idx]
                        
                        stage_info = f"    Stage {stage_idx} ({stage_type.value}): {avg_loss:.4f} ({sample_count} samples)"
                        if stage_type == UnmaskingStageType.STICKY:
                            config = stage_config.config
                            stage_info += f" - ratio={config.target_masked_ratio:.1f}"
                        elif stage_type == UnmaskingStageType.RANDOM:
                            config = stage_config.config
                            stage_info += f" - max_ratio={config.max_masked_ratio:.1f}"
                        print(stage_info)

        # Add masked token ratio statistics
        if masked_token_ratios:
            avg_masked_ratio = sum(masked_token_ratios) / len(masked_token_ratios)
            out[f'{split}_masked_token_ratio'] = avg_masked_ratio
            out[f'{split}_min_masked_token_ratio'] = min(masked_token_ratios)
            out[f'{split}_max_masked_token_ratio'] = max(masked_token_ratios)

        # Add model vs random comparison for validation
        if split == 'val' and model_probs:
            # Add signal to noise ratio
            if signal_to_noise_ratios:
                finite_sn_ratios = [r for r in signal_to_noise_ratios if math.isfinite(r)]
                if finite_sn_ratios:
                    avg_signal_to_noise = sum(finite_sn_ratios) / len(finite_sn_ratios)
                    out[f'{split}_signal_to_noise'] = avg_signal_to_noise
                    # Add median signal to noise ratio
                    finite_sn_ratios_sorted = sorted(finite_sn_ratios)
                    n = len(finite_sn_ratios_sorted)
                    if n % 2 == 0:
                        median_sn = (finite_sn_ratios_sorted[n//2 - 1] + finite_sn_ratios_sorted[n//2]) / 2.0
                    else:
                        median_sn = finite_sn_ratios_sorted[n//2]
                    out[f'{split}_signal_to_noise_median'] = median_sn
                else:
                    out[f'{split}_signal_to_noise'] = float('nan')
                    out[f'{split}_signal_to_noise_median'] = float('nan')
            # Calculate most likely prediction accuracy percentage
            if most_likely_correct:
                most_likely_accuracy = (sum(most_likely_correct) / len(most_likely_correct)) * 100.0
                out[f'{split}_most_likely_accuracy'] = most_likely_accuracy
            
            # VALIDATION METRICS STABILITY CHECK
            finite_probs = [p for p in model_probs if math.isfinite(p)]
            if len(finite_probs) == 0:
                print(f"\n*** VALIDATION METRICS INSTABILITY ***")
                print(f"All model probabilities are NaN/Inf (total: {len(model_probs)})")
                print(f"Sample of problematic values: {model_probs[:5]}")
                out[f'{split}_model_vs_random'] = float('nan')
                out[f'{split}_avg_correct_prob'] = float('nan')
                if training_ctx.training_type in ['remasking_binary', 'remasking']:
                    out[f'{split}_corruption_ratio'] = corrupted_positions / total_positions if total_positions > 0 else 0.0
                    out[f'{split}_random_baseline'] = 0.5  # Fallback value
            elif len(finite_probs) < len(model_probs):
                print(f"WARNING: {len(model_probs) - len(finite_probs)}/{len(model_probs)} model probabilities are NaN/Inf")
                avg_model_prob = sum(finite_probs) / len(finite_probs)
            else:
                avg_model_prob = sum(model_probs) / len(model_probs)
            
            # Only proceed if we have valid probabilities
            if len(finite_probs) > 0:
                if training_ctx.training_type == 'remasking_binary':
                    # For binary classification, compare against distribution-aware random baseline
                    corruption_ratio = corrupted_positions / total_positions if total_positions > 0 else 0.0
                    # Random classifier matching the distribution would get:
                    # P(correct) = P(guess_good) * P(actual_good) + P(guess_wrong) * P(actual_wrong)
                    # With optimal random strategy: P(guess_good) = P(actual_good), P(guess_wrong) = P(actual_wrong)
                    random_accuracy = (1 - corruption_ratio) ** 2 + corruption_ratio ** 2
                    prob_ratio = avg_model_prob / random_accuracy if random_accuracy > 0 else float('inf')
                    out[f'{split}_model_vs_random'] = prob_ratio
                    out[f'{split}_avg_correct_prob'] = avg_model_prob
                    out[f'{split}_corruption_ratio'] = corruption_ratio
                    out[f'{split}_random_baseline'] = random_accuracy
                    
                    # Calculate detailed probability breakdown by class
                    if right_probs_p0 or right_probs_p1:
                        finite_right_p0 = [p for p in right_probs_p0 if math.isfinite(p)]
                        finite_right_p1 = [p for p in right_probs_p1 if math.isfinite(p)]
                        
                        avg_p_right_p0 = sum(finite_right_p0) / len(finite_right_p0) if finite_right_p0 else 0.0
                        avg_p_right_p1 = sum(finite_right_p1) / len(finite_right_p1) if finite_right_p1 else 0.0
                        
                        out[f'{split}_avg_prob_right_p0'] = avg_p_right_p0
                        out[f'{split}_avg_prob_right_p1'] = avg_p_right_p1
                elif training_ctx.training_type == 'unmasking':
                    # For unmasking, use uniform random baseline
                    prob_ratio = avg_model_prob / random_prob
                    out[f'{split}_model_vs_random'] = prob_ratio
                    out[f'{split}_avg_correct_prob'] = avg_model_prob
                else:
                    raise ValueError(f"Unsupported training type: {training_ctx.training_type}")

    model.train()
    return out

def update_stage_progress(training_ctx: TrainingContext, val_loss: float):
    """
    Update stage progress for unmasking training based on validation loss.
    Returns True if stage was advanced, False otherwise.
    """
    if training_ctx.training_type != 'unmasking':
        return False
    
    stage_config = training_ctx.get_current_stage_config()
    if stage_config is None:
        return False
    
    # Check if validation loss improved
    if val_loss < training_ctx.best_val_loss_this_stage:
        training_ctx.best_val_loss_this_stage = val_loss
        training_ctx.val_loss_stale_count = 0
        print(f"  Stage {training_ctx.current_stage}: New best val loss {val_loss:.4f}, reset stale count to 0")
        return False
    else:
        training_ctx.val_loss_stale_count += 1
        print(f"  Stage {training_ctx.current_stage}: Val loss stale count {training_ctx.val_loss_stale_count}/{stage_config.get_val_loss_stale_count()}")
        
        # Check if we should advance to next stage
        if training_ctx.val_loss_stale_count >= stage_config.get_val_loss_stale_count():
            advanced = training_ctx.advance_stage()
            if advanced:
                new_stage_config = training_ctx.get_current_stage_config()
                stage_type = new_stage_config.get_stage_type()
                print(f"\n*** ADVANCING TO STAGE {training_ctx.current_stage} ({stage_type.value}) ***")
                if stage_type == UnmaskingStageType.STICKY:
                    config = new_stage_config.config
                    print(f"  Target masked ratio: {config.target_masked_ratio}")
                    print(f"  P1 probability: {config.p1_probability}")
                    print(f"  P2 probability: {config.p2_probability}")
                elif stage_type == UnmaskingStageType.RANDOM:
                    config = new_stage_config.config
                    print(f"  Max masked ratio: {config.max_masked_ratio}")
                print(f"  Val loss stale count limit: {new_stage_config.get_val_loss_stale_count()}")
                print("*** STAGE ADVANCEMENT COMPLETE ***\n")
                return True
            else:
                print(f"  Stage {training_ctx.current_stage}: Reached final stage, continuing training")
                return False
        
        return False

def get_lr(it, ctx: TrainingContext):
    """Learning rate decay scheduler (cosine with warmup)"""
    # 1) linear warmup for warmup_iters steps
    if it < ctx.warmup_iters:
        return ctx.learning_rate * (it + 1) / (ctx.warmup_iters + 1)
    # 2) if it > lr_decay_iters, return min learning rate
    if it > ctx.lr_decay_iters:
        return ctx.min_lr
    # 3) in between, use cosine decay down to min learning rate
    decay_ratio = (it - ctx.warmup_iters) / (ctx.lr_decay_iters - ctx.warmup_iters)
    assert 0 <= decay_ratio <= 1
    coeff = 0.5 * (1.0 + math.cos(math.pi * decay_ratio)) # coeff ranges 0..1
    return ctx.min_lr + coeff * (ctx.learning_rate - ctx.min_lr)

# In train_utils.py

# Add this new function to train_utils.py
def apply_bert_style_corruption_gpu(x, mask, mask_token_id, meta_vocab_size):
    """
    Applies the 80/10/10 corruption strategy from BERT to the selected positions.
    
    Args:
        x: Original input tokens (batch_size, seq_len)
        mask: Boolean mask of positions selected for prediction (batch_size, seq_len)
        mask_token_id: The ID of the [MASK] token.
        meta_vocab_size: The size of the original vocabulary for generating random tokens (excluding special tokens).
        
    Returns:
        corrupted_x: The input tokens after applying the 80/10/10 rule.
    """
    corrupted_x = x.clone()
    
    # Generate random numbers to decide on the corruption type for each masked position
    rand = torch.rand(x.shape, device=x.device)
    
    # Determine the positions for each case based on the main mask
    # 80% of the time, we replace with [MASK]
    mask_token_positions = mask & (rand < 0.8)
    
    # 10% of the time, we replace with a random token (0.8 <= rand < 0.9)
    random_token_positions = mask & (rand >= 0.8) & (rand < 0.9)
    
    # 10% of the time, we keep the original token (rand >= 0.9) - no action needed for these
    
    # Apply the [MASK] tokens
    corrupted_x[mask_token_positions] = mask_token_id
    
    # Apply the random tokens
    num_random = random_token_positions.sum()
    if num_random > 0:
        random_tokens = torch.randint(0, meta_vocab_size, (num_random,), device=x.device)
        corrupted_x[random_token_positions] = random_tokens
        
    return corrupted_x

def calculate_wrong_answer_entropy(logits, targets, vocab_size):
    """
    Calculate entropy of wrong answer distributions for entropy penalty.
    
    HIGH entropy (uniform wrong answers) = GOOD (high signal-to-noise ratio)
    LOW entropy (concentrated wrong answers) = BAD (low signal-to-noise ratio)
    
    Args:
        logits: Model logits (batch_size, seq_len, vocab_size)
        targets: Target tokens (batch_size, seq_len)
        vocab_size: Size of vocabulary
        
    Returns:
        avg_entropy: Average entropy of wrong answer distributions across all positions
    """
    batch_size, seq_len, vocab_size = logits.shape
    device = logits.device
    epsilon = 1e-9 # Use a slightly larger epsilon for stability
    
    # Get probabilities from logits
    probs = torch.nn.functional.softmax(logits, dim=-1)
    
    # Flatten for easier processing
    probs_flat = probs.view(-1, vocab_size)
    targets_flat = targets.view(-1)
    
    # --- START FIX ---
    
    # Create a mask to zero out the correct answer probabilities
    wrong_probs = probs_flat.clone()
    wrong_probs[range(len(targets_flat)), targets_flat] = 0.0
    
    # Calculate the sum of the remaining "wrong" probabilities
    # This sum is (1.0 - p_correct)
    sum_wrong_probs = wrong_probs.sum(dim=1, keepdim=True)
    
    # Avoid division by zero for positions where p_correct was close to 1.0
    # If sum_wrong_probs is near zero, entropy is also zero, so we can ignore these.
    # We create a mask for safe normalization.
    safe_mask = sum_wrong_probs.squeeze() > epsilon
    
    if not safe_mask.any():
        # Handle the edge case where no positions have significant wrong probabilities
        return torch.tensor(0.0, device=device)
        
    # Re-normalize the wrong probabilities so they sum to 1
    # This creates a true probability distribution over the incorrect tokens
    normalized_wrong_probs = wrong_probs[safe_mask] / sum_wrong_probs[safe_mask]
    
    # Calculate entropy on the properly normalized distribution
    log_probs = torch.log(normalized_wrong_probs + epsilon)
    entropies = -(normalized_wrong_probs * log_probs).sum(dim=1)
    
    # --- END FIX ---
    
    # Return average entropy across all valid positions
    return entropies.mean()

def get_current_entropy_penalty(iter_num, ctx: TrainingContext):
    """
    Calculate current entropy penalty based on iteration number.
    
    Args:
        iter_num: Current iteration number
        ctx: Training context with penalty parameters
        
    Returns:
        current_penalty: Current entropy penalty multiplier (0 to max_entropy_penalty)
    """
    if not ctx.enable_entropy_penalty:
        return 0.0
    
    if iter_num < ctx.entropy_penalty_start_iter:
        return 0.0
    
    if iter_num >= ctx.max_iters:
        return ctx.max_entropy_penalty
    
    # Linear increase from start_iter to max_iters
    progress = (iter_num - ctx.entropy_penalty_start_iter) / (ctx.max_iters - ctx.entropy_penalty_start_iter)
    progress = max(0.0, min(1.0, progress))  # Clamp to [0, 1]
    
    return progress * ctx.max_entropy_penalty

def update_entropy_multiplier_ema(ctx: TrainingContext, current_multiplier: float):
    """
    Update the exponential moving average of entropy multiplier.
    
    Args:
        ctx: Training context
        current_multiplier: Current entropy multiplier value
    """
    if ctx.enable_entropy_penalty:
        # EMA update: ema = alpha * ema + (1-alpha) * current_value
        alpha = ctx.entropy_multiplier_ema_factor
        ctx.entropy_multiplier_ema = alpha * ctx.entropy_multiplier_ema + (1 - alpha) * current_multiplier

def apply_label_smoothing(targets, uncertainty_factor, vocab_size, special_token_ids=None, device=None):
    """
    Apply label smoothing to target tokens.
    
    Args:
        targets: Target token IDs (batch_size, seq_len)
        uncertainty_factor: Label smoothing factor (0.0 = no smoothing, >0 = apply smoothing)
        vocab_size: Size of vocabulary
        special_token_ids: List of special token IDs to exclude from smoothing (optional)
        device: Device to create tensors on
        
    Returns:
        smoothed_targets: Probability distribution targets (batch_size, seq_len, vocab_size)
    """
    if uncertainty_factor <= 0.0:
        # No smoothing, return one-hot encoded targets
        return torch.nn.functional.one_hot(targets, num_classes=vocab_size).float()
    
    if device is None:
        device = targets.device
    
    batch_size, seq_len = targets.shape
    
    # Create smoothed probability distribution
    smoothed_targets = torch.zeros(batch_size, seq_len, vocab_size, device=device)
    
    # Set correct answer probability to (1 - uncertainty_factor)
    correct_prob = 1.0 - uncertainty_factor
    smoothed_targets.scatter_(2, targets.unsqueeze(-1), correct_prob)
    
    # Calculate incorrect answer probability: u / (vocab_size - 1)
    # But we need to exclude special tokens from getting smoothed probability
    incorrect_prob = uncertainty_factor / (vocab_size - len(special_token_ids))
    
    # Add smoothing probability to all positions except the correct answer
    smoothed_targets += incorrect_prob
    
    # Remove the extra probability that was added to correct answers
    smoothed_targets.scatter_(2, targets.unsqueeze(-1), correct_prob)
    
    # Handle special tokens - set their probability to 0 (except when they are the correct answer)
    if special_token_ids is not None:
        for special_id in special_token_ids:
            if special_id < vocab_size:
                # Create mask for positions where special_id is NOT the correct answer
                not_correct_mask = (targets != special_id).unsqueeze(-1)
                # Zero out probability for this special token where it's not correct
                special_mask = torch.zeros(batch_size, seq_len, vocab_size, device=device)
                special_mask[:, :, special_id] = 1.0
                smoothed_targets = smoothed_targets * (1 - special_mask * not_correct_mask.float())
    
    sum_probs = smoothed_targets.sum(dim=-1, keepdim=True)
    # Renormalize to ensure probabilities sum to 1
    smoothed_targets = smoothed_targets / sum_probs
    
    return smoothed_targets

--- utils.py ---
"""
Utility functions for diffusion training including timing and logging
"""
import time
import torch
from collections import defaultdict


class Timer:
    """Timer class for performance monitoring with context manager support"""
    
    def __init__(self):
        self.times = defaultdict(list)
    
    def time_function(self, name):
        """Context manager for timing function calls"""
        class TimerContext:
            def __init__(self, timer, name):
                self.timer = timer
                self.name = name
                self.start_time = None
            
            def __enter__(self):
                self.start_time = time.time()
                return self
            
            def __exit__(self, *args):
                elapsed = time.time() - self.start_time
                self.timer.times[self.name].append(elapsed)
        
        return TimerContext(self, name)
    
    def get_average(self, name, last_n=100):
        """Get average time for last N calls"""
        if name not in self.times or not self.times[name]:
            return 0.0
        return sum(self.times[name][-last_n:]) / min(len(self.times[name]), last_n)
    
    def get_recent_average(self, name, last_n=10):
        """Get average time for very recent calls only"""
        if name not in self.times or not self.times[name]:
            return 0.0
        return sum(self.times[name][-last_n:]) / min(len(self.times[name]), last_n)
    
    def get_last_time(self, name):
        """Get the most recent time measurement"""
        if name not in self.times or not self.times[name]:
            return 0.0
        return self.times[name][-1]




def analyze_clustering(mask):
    """Analyze clustering properties of mask patterns"""
    batch_size, seq_len = mask.shape
    cluster_sizes = []

    for batch_idx in range(batch_size):
        mask_seq = mask[batch_idx].cpu().numpy()

        # Find connected components (clusters)
        in_cluster = False
        current_cluster_size = 0

        for pos in range(seq_len):
            if mask_seq[pos]:  # Masked position
                if not in_cluster:
                    in_cluster = True
                    current_cluster_size = 1
                else:
                    current_cluster_size += 1
            else:  # Unmasked position
                if in_cluster:
                    cluster_sizes.append(current_cluster_size)
                    in_cluster = False
                    current_cluster_size = 0

        # Handle cluster at end of sequence
        if in_cluster:
            cluster_sizes.append(current_cluster_size)

    if cluster_sizes:
        avg_cluster_size = sum(cluster_sizes) / len(cluster_sizes)
        max_cluster_size = max(cluster_sizes)
        num_clusters = len(cluster_sizes)
        return {
            'avg_cluster_size': avg_cluster_size,
            'max_cluster_size': max_cluster_size,
            'num_clusters_per_batch': num_clusters / batch_size
        }
    else:
        return {
            'avg_cluster_size': 0,
            'max_cluster_size': 0,
            'num_clusters_per_batch': 0
        }


def analyze_masking_patterns_with_transition(mask, iter_num, sticky_transition_start, sticky_transition_end):
    """Analyze masking patterns during independent->sticky transition"""
    mask_ratio = mask.float().mean().item()

    # Calculate current transition state
    if iter_num < sticky_transition_start:
        transition_state = "independent"
        sticky_ratio = 0.0
    elif iter_num >= sticky_transition_end:
        transition_state = "sticky"
        sticky_ratio = 1.0
    else:
        progress = (iter_num - sticky_transition_start) / (sticky_transition_end - sticky_transition_start)
        sticky_ratio = progress
        transition_state = f"transition ({sticky_ratio:.2f})"

    # Analyze clustering (more relevant during sticky phase)
    if sticky_ratio > 0.1:  # Only analyze clusters when some sticky masking present
        cluster_stats = analyze_clustering(mask)
        return {
            'mask_ratio': mask_ratio,
            'transition_state': transition_state,
            'sticky_ratio': sticky_ratio,
            **cluster_stats
        }
    else:
        return {
            'mask_ratio': mask_ratio,
            'transition_state': transition_state,
            'sticky_ratio': sticky_ratio
        }


def log_masking_stats(mask, iter_num, log_interval, sticky_transition_start=None, sticky_transition_end=None):
    """Log statistics about masking patterns"""
    if sticky_transition_start is not None and sticky_transition_end is not None:
        # Enhanced logging with transition tracking
        masking_stats = analyze_masking_patterns_with_transition(mask, iter_num, sticky_transition_start, sticky_transition_end)
        if iter_num % (log_interval * 10) == 0:  # Less frequent detailed stats
            print(f"Masking: {masking_stats}")
    else:
        # Original simple logging
        mask_ratio = mask.float().mean().item()
        batch_size, seq_len = mask.shape

        # Count consecutive masked regions
        mask_np = mask.cpu().numpy()
        consecutive_regions = []
        for batch_idx in range(batch_size):
            regions = []
            current_length = 0
            for pos in range(seq_len):
                if mask_np[batch_idx, pos]:
                    current_length += 1
                else:
                    if current_length > 0:
                        regions.append(current_length)
                        current_length = 0
            if current_length > 0:
                regions.append(current_length)
            consecutive_regions.extend(regions)

        avg_region_length = sum(consecutive_regions) / len(consecutive_regions) if consecutive_regions else 0

        if iter_num % (log_interval * 10) == 0:  # Less frequent detailed stats
            print(f"Masking stats: {mask_ratio:.3f} ratio, {avg_region_length:.1f} avg region length")


================================================================================
GLOBAL VARIABLES:
================================================================================
always_save_checkpoint = True
arg = .\config\shkspr_char_diff\optimal7.py
attention_type = bidirectional
backend = nccl
batch_size = 16
beta1 = 0.9
beta2 = 0.99
bias = False
block_size = 1024
ckpt_filename = 34.5_58.4_UM.pt
compile = True
config = {'out_dir': 'out', 'training_type': 'unmasking', 'eval_interval': 200, 'log_interval': 20, 'eval_iters': 20, 'eval_only': False, 'always_save_checkpoint': True, 'init_from': 'scratch', 'ckpt_filename': '34.5_58.4_UM.pt', 'n_layer': 6, 'n_head': 6, 'n_embd': 384, 'dropout': 0.2, 'bias': False, 'attention_type': 'bidirectional', 'use_rope': True, 'wandb_log': True, 'wandb_project': 'experiments_diffusion', 'wandb_run_name': 'shkspr_char_diff_moderate_first_unmasking', 'dataset': 'shakespeare_char', 'gradient_accumulation_steps': 12, 'batch_size': 16, 'block_size': 1024, 'use_paragraph_boundaries': False, 'use_all_stages_for_training': True, 'weight_loss_by_mask_ratio': True, 'enable_entropy_penalty': False, 'max_entropy_penalty': 3, 'entropy_penalty_start_iter': 2500, 'uncertainty_factor': 0.03, 'learning_rate': 0.001, 'max_iters': 10000, 'warmup_iters': 2000, 'lr_decay_iters': 8000, 'min_lr': 3e-05, 'beta1': 0.9, 'beta2': 0.99, 'weight_decay': 0.02, 'grad_clip': 0.0, 'decay_lr': True, 'backend': 'nccl', 'device': 'cuda', 'dtype': 'float16', 'compile': True, 'start_iter_num': 0}
config_file = .\config\shkspr_char_diff\optimal7.py
config_keys = ['out_dir', 'training_type', 'eval_interval', 'log_interval', 'eval_iters', 'eval_only', 'always_save_checkpoint', 'init_from', 'ckpt_filename', 'n_layer', 'n_head', 'n_embd', 'dropout', 'bias', 'attention_type', 'use_rope', 'wandb_log', 'wandb_project', 'wandb_run_name', 'dataset', 'gradient_accumulation_steps', 'batch_size', 'block_size', 'use_paragraph_boundaries', 'use_all_stages_for_training', 'weight_loss_by_mask_ratio', 'enable_entropy_penalty', 'max_entropy_penalty', 'entropy_penalty_start_iter', 'uncertainty_factor', 'learning_rate', 'max_iters', 'warmup_iters', 'lr_decay_iters', 'min_lr', 'beta1', 'beta2', 'weight_decay', 'grad_clip', 'decay_lr', 'backend', 'device', 'dtype', 'compile', 'start_iter_num']
dataset = shakespeare_char
decay_lr = True
device = cuda
dropout = 0.2
dtype = float16
enable_entropy_penalty = False
entropy_penalty_start_iter = 2500
eval_interval = 200
eval_iters = 20
eval_only = False
f = <_io.TextIOWrapper name='utils.py' mode='r' encoding='cp1250'>
file_path = C:\Users\Adam\AppData\Local\Programs\Python\Python310\lib\site-packages\torch\_dynamo\__init__.py
filename = utils.py
grad_clip = 0.0
gradient_accumulation_steps = 12
init_from = scratch
learning_rate = 0.001
local_files = ['model.py', 'train_run.py', 'train_utils.py', 'utils.py']
log_interval = 20
lr_decay_iters = 8000
math = <module 'math' (built-in)>
max_entropy_penalty = 3
max_iters = 10000
min_lr = 3e-05
module = <module 'torch._dynamo' from 'C:\\Users\\Adam\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\_dynamo\\__init__.py'>
module_name = torch._dynamo
n_embd = 384
n_head = 6
n_layer = 6
np = <module 'numpy' from 'C:\\Users\\Adam\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\numpy\\__init__.py'>
os = <module 'os' from 'C:\\Users\\Adam\\AppData\\Local\\Programs\\Python\\Python310\\lib\\os.py'>
out_dir = out
pickle = <module 'pickle' from 'C:\\Users\\Adam\\AppData\\Local\\Programs\\Python\\Python310\\lib\\pickle.py'>
start_iter_num = 0
sys = <module 'sys' (built-in)>
threading = <module 'threading' from 'C:\\Users\\Adam\\AppData\\Local\\Programs\\Python\\Python310\\lib\\threading.py'>
time = <module 'time' (built-in)>
timer = <utils.Timer object at 0x00000251FFD49DE0>
torch = <module 'torch' from 'C:\\Users\\Adam\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\__init__.py'>
training_type = unmasking
uncertainty_factor = 0.03
unmasking_stages = [{'type': 'sticky', 'target_masked_ratio': 0.4, 'p1_probability': 0.15, 'p2_probability': 0.3, 'val_loss_stale_count': 6}, {'type': 'sticky', 'target_masked_ratio': 0.6, 'p1_probability': 0.1, 'p2_probability': 0.5, 'val_loss_stale_count': 8}, {'type': 'random', 'max_masked_ratio': 0.5, 'val_loss_stale_count': 10}, {'type': 'sticky', 'target_masked_ratio': 0.6, 'p1_probability': 0.3, 'p2_probability': 0.1, 'val_loss_stale_count': 8}, {'type': 'sticky', 'target_masked_ratio': 0.6, 'p1_probability': 0.1, 'p2_probability': 0.5, 'val_loss_stale_count': 8}, {'type': 'random', 'max_masked_ratio': 0.2, 'val_loss_stale_count': 10}, {'type': 'sticky', 'target_masked_ratio': 0.55, 'p1_probability': 0.1, 'p2_probability': 0.6, 'val_loss_stale_count': 10}, {'type': 'sticky', 'target_masked_ratio': 0.9, 'p1_probability': 0.1, 'p2_probability': 0.9, 'val_loss_stale_count': 20}]
use_all_stages_for_training = True
use_paragraph_boundaries = False
use_rope = True
validation_stages = [{'type': 'sticky', 'target_masked_ratio': 0.4, 'p1_probability': 0.15, 'p2_probability': 0.3, 'val_loss_stale_count': 6}, {'type': 'sticky', 'target_masked_ratio': 0.6, 'p1_probability': 0.1, 'p2_probability': 0.5, 'val_loss_stale_count': 8}, {'type': 'random', 'max_masked_ratio': 0.5, 'val_loss_stale_count': 10}, {'type': 'sticky', 'target_masked_ratio': 0.6, 'p1_probability': 0.3, 'p2_probability': 0.1, 'val_loss_stale_count': 8}, {'type': 'sticky', 'target_masked_ratio': 0.6, 'p1_probability': 0.1, 'p2_probability': 0.5, 'val_loss_stale_count': 8}, {'type': 'random', 'max_masked_ratio': 0.2, 'val_loss_stale_count': 10}, {'type': 'sticky', 'target_masked_ratio': 0.2, 'p1_probability': 0.3, 'p2_probability': 0.0, 'val_loss_stale_count': 2}, {'type': 'sticky', 'target_masked_ratio': 0.4, 'p1_probability': 0.3, 'p2_probability': 0.0, 'val_loss_stale_count': 4}, {'type': 'sticky', 'target_masked_ratio': 0.4, 'p1_probability': 0.15, 'p2_probability': 0.3, 'val_loss_stale_count': 6}, {'type': 'sticky', 'target_masked_ratio': 0.55, 'p1_probability': 0.1, 'p2_probability': 0.6, 'val_loss_stale_count': 10}, {'type': 'sticky', 'target_masked_ratio': 0.7, 'p1_probability': 0.2, 'p2_probability': 0.4, 'val_loss_stale_count': 15}, {'type': 'sticky', 'target_masked_ratio': 0.8, 'p1_probability': 0.2, 'p2_probability': 0.4, 'val_loss_stale_count': 20}, {'type': 'sticky', 'target_masked_ratio': 0.8, 'p1_probability': 0.1, 'p2_probability': 0.9, 'val_loss_stale_count': 20}, {'type': 'sticky', 'target_masked_ratio': 0.9, 'p1_probability': 0.1, 'p2_probability': 0.9, 'val_loss_stale_count': 20}]
wandb_log = True
wandb_project = experiments_diffusion
wandb_run_name = shkspr_char_diff_moderate_first_unmasking
warmup_iters = 2000
weight_decay = 0.02
weight_loss_by_mask_ratio = True

================================================================================
tokens per iteration will be: 196,608
found vocab_size = 65 (inside data\shakespeare_char\meta.pkl)
mask_token_id = 65, extended_vocab_size = 80
DEBUG: init_from='scratch', checkpoint_training_context=None
DEBUG: NOT applying training context. init_from='scratch', checkpoint_training_context=False
Initializing a new model from scratch
Using bidirectional attention
Using bidirectional attention
Using bidirectional attention
Using bidirectional attention
Using bidirectional attention
Using bidirectional attention
number of parameters: 10.65M
num decayed parameter tensors: 25, with 10,647,552 parameters
num non-decayed parameter tensors: 13, with 4,992 parameters
using fused AdamW: True
compiling the model... (takes a ~minute)

*** STAGE-BASED UNMASKING TRAINING INITIALIZED ***
Starting at Stage 0:
  Stage type: sticky
  Target masked ratio: 0.4
  P1 probability: 0.15
  P2 probability: 0.3
  Val loss stale count limit: 6
Total stages configured: 8
*** STAGE INITIALIZATION COMPLETE ***

Pre-creating validation set...
Creating validation set with samples from all stages...
  Stage 0 (sticky): 23 samples (target_ratio=0.4, p1=0.1, p2=0.3)
  Stage 1 (sticky): 23 samples (target_ratio=0.6, p1=0.1, p2=0.5)
  Stage 2 (random): 23 samples (max_ratio=0.5)
  Stage 3 (sticky): 23 samples (target_ratio=0.6, p1=0.3, p2=0.1)
  Stage 4 (sticky): 23 samples (target_ratio=0.6, p1=0.1, p2=0.5)
  Stage 5 (random): 23 samples (max_ratio=0.2)
  Stage 6 (sticky): 23 samples (target_ratio=0.2, p1=0.3, p2=0.0)
  Stage 7 (sticky): 23 samples (target_ratio=0.4, p1=0.3, p2=0.0)
  Stage 8 (sticky): 23 samples (target_ratio=0.4, p1=0.1, p2=0.3)
  Stage 9 (sticky): 23 samples (target_ratio=0.6, p1=0.1, p2=0.6)
  Stage 10 (sticky): 23 samples (target_ratio=0.7, p1=0.2, p2=0.4)
  Stage 11 (sticky): 23 samples (target_ratio=0.8, p1=0.2, p2=0.4)
  Stage 12 (sticky): 22 samples (target_ratio=0.8, p1=0.1, p2=0.9)
  Stage 13 (sticky): 22 samples (target_ratio=0.9, p1=0.1, p2=0.9)
Validation set created: 28 batches, 320 total samples
Training will generate fresh batches from all stages each iteration
Starting training loop...

--- Starting validation at iteration 0 ---
Using validation set with samples from all 14 stages
  Validation complete: 20 batches processed (320 samples), avg loss = 7.7496
  Per-stage validation losses:
    Stage 0 (sticky): 7.1546 (23 samples) - ratio=0.4
    Stage 1 (sticky): 5.8666 (16 samples) - ratio=0.6
    Stage 2 (random): 7.1348 (23 samples) - max_ratio=0.5
    Stage 3 (sticky): 10.0336 (7 samples) - ratio=0.6
    Stage 4 (sticky): 5.8585 (16 samples) - ratio=0.6
    Stage 5 (random): 5.8631 (23 samples) - max_ratio=0.2
    Stage 6 (sticky): 5.8612 (7 samples) - ratio=0.2
    Stage 7 (sticky): 14.0293 (16 samples) - ratio=0.4
    Stage 8 (sticky): 11.0727 (23 samples) - ratio=0.4
    Stage 9 (sticky): 10.0320 (7 samples) - ratio=0.6
    Stage 10 (sticky): 7.1558 (23 samples) - ratio=0.7
    Stage 11 (sticky): 7.1563 (16 samples) - ratio=0.8
    Stage 12 (sticky): 7.1445 (7 samples) - ratio=0.8
    Stage 13 (sticky): 6.1236 (23 samples) - ratio=0.9
--- Validation complete ---
step 0: train loss 6.4184, val loss 7.7496, lr 0.000000
Stage 0 (sticky): target_ratio=0.4, p1=0.1, p2=0.3, stale_count=0
  val model vs random: 0.94x better
  val avg correct prob: 0.0117 (random: 0.0125)
  val signal to noise: 0.37 (median: 0.33)
  Most likely guess correct P %: 1.4%
  Stage 0: New best val loss 7.7496, reset stale count to 0

iter 0: loss 6.4209, time 26347.08ms, mfu -100.00%
  data: 169.4ms, grad_accum: 924.1ms (fw: 13.7ms, bw: 124.0ms)
  grad_proc: 21.0ms, optimizer: 8.0ms, param_check: 18.0ms
  loss_proc: 0.0ms, instability: 47.0ms
  cleanup: 0.0ms, gpu_sync: 0.0ms
  measured: 971.1ms, unaccounted: 25376.0ms (96.3%)
  validation: 23782.73ms (data: 24.93ms, forward: 0.00ms, loss: 564.26ms)
Masking: stage=0 (sticky), actual_ratio=0.517, target=0.4, p1=0.1, p2=0.3
iter 20: loss 5.0753, time 967.52ms, mfu 6.01%
  data: 175.7ms, grad_accum: 966.2ms (fw: 12.0ms, bw: 129.3ms)
  grad_proc: 17.0ms, optimizer: 1.6ms, param_check: 17.6ms
  loss_proc: 0.0ms, instability: 50.1ms
  cleanup: 0.4ms, gpu_sync: 0.5ms
  measured: 1003.2ms, unaccounted: -35.7ms (-3.7%)
iter 40: loss 5.0294, time 1195.29ms, mfu 5.89%
  data: 201.8ms, grad_accum: 1124.2ms (fw: 14.7ms, bw: 149.4ms)
  grad_proc: 18.1ms, optimizer: 1.4ms, param_check: 17.7ms
  loss_proc: 0.0ms, instability: 64.0ms
  cleanup: 0.4ms, gpu_sync: 0.3ms
  measured: 1162.2ms, unaccounted: 33.1ms (2.8%)
iter 60: loss 4.7597, time 1263.65ms, mfu 5.76%
  data: 198.6ms, grad_accum: 1202.4ms (fw: 12.7ms, bw: 154.1ms)
  grad_proc: 16.4ms, optimizer: 1.5ms, param_check: 18.9ms
  loss_proc: 0.0ms, instability: 72.4ms
  cleanup: 0.4ms, gpu_sync: 0.3ms
  measured: 1239.9ms, unaccounted: 23.8ms (1.9%)
iter 80: loss 4.8563, time 1349.84ms, mfu 5.62%
  data: 203.5ms, grad_accum: 1252.2ms (fw: 12.7ms, bw: 157.7ms)
  grad_proc: 16.6ms, optimizer: 1.7ms, param_check: 18.1ms
  loss_proc: 0.0ms, instability: 79.1ms
  cleanup: 0.4ms, gpu_sync: 0.2ms
  measured: 1289.0ms, unaccounted: 60.8ms (4.5%)
iter 100: loss 4.8262, time 1339.42ms, mfu 5.49%
  data: 206.3ms, grad_accum: 1243.6ms (fw: 12.4ms, bw: 159.4ms)
  grad_proc: 19.1ms, optimizer: 1.6ms, param_check: 20.7ms
  loss_proc: 0.0ms, instability: 74.5ms
  cleanup: 0.7ms, gpu_sync: 0.3ms
  measured: 1286.0ms, unaccounted: 53.4ms (4.0%)
iter 120: loss 4.7667, time 1241.03ms, mfu 5.41%
  data: 198.1ms, grad_accum: 1194.0ms (fw: 14.1ms, bw: 154.3ms)
  grad_proc: 16.1ms, optimizer: 1.6ms, param_check: 17.8ms
  loss_proc: 0.0ms, instability: 68.7ms
  cleanup: 0.3ms, gpu_sync: 0.4ms
  measured: 1230.2ms, unaccounted: 10.8ms (0.9%)
iter 140: loss 4.7260, time 1271.18ms, mfu 5.33%
  data: 213.8ms, grad_accum: 1232.8ms (fw: 12.6ms, bw: 167.3ms)
  grad_proc: 17.2ms, optimizer: 1.3ms, param_check: 18.9ms
  loss_proc: 0.0ms, instability: 70.2ms
  cleanup: 0.4ms, gpu_sync: 0.4ms
  measured: 1270.9ms, unaccounted: 0.3ms (0.0%)
iter 160: loss 4.8007, time 1275.68ms, mfu 5.25%
  data: 222.8ms, grad_accum: 1277.3ms (fw: 12.1ms, bw: 176.3ms)
  grad_proc: 18.7ms, optimizer: 1.7ms, param_check: 19.8ms
  loss_proc: 0.0ms, instability: 70.7ms
  cleanup: 0.3ms, gpu_sync: 0.4ms
  measured: 1318.2ms, unaccounted: -42.5ms (-3.3%)
iter 180: loss 4.8214, time 1310.51ms, mfu 5.17%
  data: 212.4ms, grad_accum: 1237.8ms (fw: 12.9ms, bw: 167.9ms)
  grad_proc: 16.1ms, optimizer: 1.3ms, param_check: 18.5ms
  loss_proc: 0.0ms, instability: 74.0ms
  cleanup: 0.2ms, gpu_sync: 0.4ms
  measured: 1274.2ms, unaccounted: 36.3ms (2.8%)

--- Starting validation at iteration 200 ---
Using validation set with samples from all 14 stages
  Validation complete: 20 batches processed (320 samples), avg loss = 5.8766
  Per-stage validation losses:
    Stage 0 (sticky): 5.4377 (23 samples) - ratio=0.4
    Stage 1 (sticky): 4.4654 (16 samples) - ratio=0.6
    Stage 2 (random): 5.2723 (23 samples) - max_ratio=0.5
    Stage 3 (sticky): 7.6481 (7 samples) - ratio=0.6
    Stage 4 (sticky): 4.4727 (16 samples) - ratio=0.6
    Stage 5 (random): 4.4698 (23 samples) - max_ratio=0.2
    Stage 6 (sticky): 4.4699 (7 samples) - ratio=0.2
    Stage 7 (sticky): 10.6425 (16 samples) - ratio=0.4
    Stage 8 (sticky): 8.3729 (23 samples) - ratio=0.4
    Stage 9 (sticky): 7.6167 (7 samples) - ratio=0.6
    Stage 10 (sticky): 5.4906 (23 samples) - ratio=0.7
    Stage 11 (sticky): 5.4083 (16 samples) - ratio=0.8
    Stage 12 (sticky): 5.5141 (7 samples) - ratio=0.8
    Stage 13 (sticky): 4.6042 (23 samples) - ratio=0.9
--- Validation complete ---
step 200: train loss 4.7990, val loss 5.8766, lr 0.000100
Stage 0 (sticky): target_ratio=0.4, p1=0.1, p2=0.3, stale_count=0
  val model vs random: 4.60x better
  val avg correct prob: 0.0575 (random: 0.0125)
  val signal to noise: 0.72 (median: 0.32)
  Most likely guess correct P %: 15.3%
  Stage 0: New best val loss 5.8766, reset stale count to 0

saving checkpoint to out/ckpt_unmasking_200.pt
iter 200: loss 4.7779, time 4892.67ms, mfu 4.77%
  data: 216.8ms, grad_accum: 1243.1ms (fw: 12.9ms, bw: 172.2ms)
  grad_proc: 16.8ms, optimizer: 1.6ms, param_check: 19.6ms
  loss_proc: 0.0ms, instability: 70.0ms
  cleanup: 0.4ms, gpu_sync: 0.4ms
  measured: 1281.8ms, unaccounted: 3610.9ms (73.8%)
  validation: 13589.78ms (data: 23.83ms, forward: 0.00ms, loss: 310.84ms)
Masking: stage=0 (sticky), actual_ratio=0.504, target=0.4, p1=0.1, p2=0.3
iter 220: loss 4.7493, time 1263.89ms, mfu 4.75%
  data: 212.5ms, grad_accum: 1236.8ms (fw: 13.4ms, bw: 167.1ms)
  grad_proc: 16.9ms, optimizer: 1.3ms, param_check: 19.0ms
  loss_proc: 0.0ms, instability: 69.2ms
  cleanup: 0.3ms, gpu_sync: 0.3ms
  measured: 1274.6ms, unaccounted: -10.7ms (-0.8%)
iter 240: loss 4.7335, time 1282.68ms, mfu 4.73%
  data: 214.6ms, grad_accum: 1241.5ms (fw: 13.9ms, bw: 168.3ms)
  grad_proc: 17.5ms, optimizer: 1.5ms, param_check: 19.5ms
  loss_proc: 0.1ms, instability: 69.1ms
  cleanup: 0.2ms, gpu_sync: 0.4ms
  measured: 1280.7ms, unaccounted: 2.0ms (0.2%)
iter 260: loss 4.7146, time 1273.75ms, mfu 4.71%
  data: 213.8ms, grad_accum: 1234.9ms (fw: 12.5ms, bw: 170.3ms)
  grad_proc: 17.3ms, optimizer: 1.6ms, param_check: 19.1ms
  loss_proc: 0.0ms, instability: 70.1ms
  cleanup: 0.3ms, gpu_sync: 0.5ms
  measured: 1273.8ms, unaccounted: -0.0ms (-0.0%)
iter 280: loss 4.7586, time 1269.69ms, mfu 4.70%
  data: 212.9ms, grad_accum: 1239.2ms (fw: 12.9ms, bw: 167.5ms)
  grad_proc: 16.7ms, optimizer: 1.4ms, param_check: 18.2ms
  loss_proc: 0.0ms, instability: 70.1ms
  cleanup: 0.1ms, gpu_sync: 0.5ms
  measured: 1276.2ms, unaccounted: -6.5ms (-0.5%)
iter 300: loss 4.7059, time 1272.01ms, mfu 4.69%
  data: 217.5ms, grad_accum: 1253.1ms (fw: 12.6ms, bw: 170.4ms)
  grad_proc: 16.7ms, optimizer: 1.7ms, param_check: 20.2ms
  loss_proc: 0.0ms, instability: 70.2ms
  cleanup: 0.1ms, gpu_sync: 0.4ms
  measured: 1292.2ms, unaccounted: -20.2ms (-1.6%)
iter 320: loss 4.6058, time 1302.22ms, mfu 4.66%
  data: 216.9ms, grad_accum: 1255.1ms (fw: 12.9ms, bw: 169.0ms)
  grad_proc: 16.3ms, optimizer: 1.8ms, param_check: 18.6ms
  loss_proc: 0.0ms, instability: 71.2ms
  cleanup: 0.2ms, gpu_sync: 0.4ms
  measured: 1292.4ms, unaccounted: 9.8ms (0.8%)
iter 340: loss 4.5650, time 1424.26ms, mfu 4.61%
  data: 218.0ms, grad_accum: 1269.1ms (fw: 12.6ms, bw: 171.9ms)
  grad_proc: 17.5ms, optimizer: 1.5ms, param_check: 18.5ms
  loss_proc: 0.0ms, instability: 83.6ms
  cleanup: 0.4ms, gpu_sync: 0.4ms
  measured: 1307.5ms, unaccounted: 116.8ms (8.2%)
iter 360: loss 4.4304, time 1312.59ms, mfu 4.59%
  data: 216.0ms, grad_accum: 1248.0ms (fw: 13.6ms, bw: 170.4ms)
  grad_proc: 17.4ms, optimizer: 1.5ms, param_check: 20.3ms
  loss_proc: 0.0ms, instability: 72.2ms
  cleanup: 0.3ms, gpu_sync: 0.5ms
  measured: 1288.0ms, unaccounted: 24.6ms (1.9%)
iter 380: loss 4.2032, time 1274.15ms, mfu 4.59%
  data: 214.8ms, grad_accum: 1243.0ms (fw: 12.9ms, bw: 170.4ms)
  grad_proc: 15.8ms, optimizer: 1.5ms, param_check: 18.0ms
  loss_proc: 0.0ms, instability: 70.5ms
  cleanup: 0.3ms, gpu_sync: 0.5ms
  measured: 1279.1ms, unaccounted: -5.0ms (-0.4%)

--- Starting validation at iteration 400 ---
Using validation set with samples from all 14 stages
  Validation complete: 20 batches processed (320 samples), avg loss = 4.4103
  Per-stage validation losses:
    Stage 0 (sticky): 4.1129 (23 samples) - ratio=0.4
    Stage 1 (sticky): 3.7570 (16 samples) - ratio=0.6
    Stage 2 (random): 4.2168 (23 samples) - max_ratio=0.5
    Stage 3 (sticky): 5.4545 (7 samples) - ratio=0.6
    Stage 4 (sticky): 3.5042 (16 samples) - ratio=0.6
    Stage 5 (random): 3.6163 (23 samples) - max_ratio=0.2
    Stage 6 (sticky): 3.6824 (7 samples) - ratio=0.2
    Stage 7 (sticky): 7.3516 (16 samples) - ratio=0.4
    Stage 8 (sticky): 5.7969 (23 samples) - ratio=0.4
    Stage 9 (sticky): 5.3433 (7 samples) - ratio=0.6
    Stage 10 (sticky): 3.8599 (23 samples) - ratio=0.7
    Stage 11 (sticky): 4.0420 (16 samples) - ratio=0.8
    Stage 12 (sticky): 4.1859 (7 samples) - ratio=0.8
    Stage 13 (sticky): 3.8397 (23 samples) - ratio=0.9
--- Validation complete ---
step 400: train loss 4.0685, val loss 4.4103, lr 0.000200
Stage 0 (sticky): target_ratio=0.4, p1=0.1, p2=0.3, stale_count=0
  val model vs random: 13.04x better
  val avg correct prob: 0.1630 (random: 0.0125)
  val signal to noise: 2.78 (median: 0.44)
  Most likely guess correct P %: 29.4%
  Stage 0: New best val loss 4.4103, reset stale count to 0

saving checkpoint to out/ckpt_unmasking_400.pt
iter 400: loss 4.0532, time 6809.52ms, mfu 4.21%
  data: 216.3ms, grad_accum: 1253.7ms (fw: 24.0ms, bw: 169.8ms)
  grad_proc: 17.0ms, optimizer: 1.5ms, param_check: 19.7ms
  loss_proc: 0.0ms, instability: 60.6ms
  cleanup: 0.2ms, gpu_sync: 0.6ms
  measured: 1292.7ms, unaccounted: 5516.8ms (81.0%)
  validation: 10749.81ms (data: 18.68ms, forward: 0.00ms, loss: 67.12ms)
Masking: stage=0 (sticky), actual_ratio=0.509, target=0.4, p1=0.1, p2=0.3
iter 420: loss 4.1395, time 1274.81ms, mfu 4.25%
  data: 217.0ms, grad_accum: 1252.0ms (fw: 12.5ms, bw: 170.3ms)
  grad_proc: 16.3ms, optimizer: 1.8ms, param_check: 20.2ms
  loss_proc: 0.0ms, instability: 70.7ms
  cleanup: 0.2ms, gpu_sync: 0.6ms
  measured: 1291.0ms, unaccounted: -16.2ms (-1.3%)
iter 440: loss 4.0307, time 1380.31ms, mfu 4.24%
  data: 228.1ms, grad_accum: 1348.3ms (fw: 13.2ms, bw: 180.1ms)
  grad_proc: 16.9ms, optimizer: 1.5ms, param_check: 19.1ms
  loss_proc: 0.0ms, instability: 73.8ms
  cleanup: 0.5ms, gpu_sync: 0.5ms
  measured: 1386.8ms, unaccounted: -6.5ms (-0.5%)
iter 460: loss 3.9090, time 1605.98ms, mfu 4.18%
  data: 281.8ms, grad_accum: 1444.4ms (fw: 37.3ms, bw: 213.3ms)
  grad_proc: 19.5ms, optimizer: 1.8ms, param_check: 24.7ms
  loss_proc: 0.0ms, instability: 51.1ms
  cleanup: 0.1ms, gpu_sync: 0.4ms
  measured: 1491.0ms, unaccounted: 115.0ms (7.2%)
iter 480: loss 3.8805, time 1657.86ms, mfu 4.11%
  data: 255.0ms, grad_accum: 1517.5ms (fw: 12.3ms, bw: 210.1ms)
  grad_proc: 17.1ms, optimizer: 1.7ms, param_check: 20.8ms
  loss_proc: 0.0ms, instability: 74.4ms
  cleanup: 0.3ms, gpu_sync: 0.5ms
  measured: 1557.8ms, unaccounted: 100.0ms (6.0%)
iter 500: loss 3.8983, time 1275.36ms, mfu 4.16%
  data: 214.2ms, grad_accum: 1244.7ms (fw: 12.7ms, bw: 169.2ms)
  grad_proc: 16.0ms, optimizer: 1.4ms, param_check: 18.9ms
  loss_proc: 0.1ms, instability: 71.2ms
  cleanup: 0.4ms, gpu_sync: 0.5ms
  measured: 1281.9ms, unaccounted: -6.5ms (-0.5%)
iter 520: loss 3.8562, time 1354.69ms, mfu 4.17%
  data: 222.2ms, grad_accum: 1254.8ms (fw: 12.9ms, bw: 175.7ms)
  grad_proc: 16.7ms, optimizer: 1.6ms, param_check: 18.6ms
  loss_proc: 0.0ms, instability: 71.8ms
  cleanup: 0.1ms, gpu_sync: 0.5ms
  measured: 1292.4ms, unaccounted: 62.3ms (4.6%)
iter 540: loss 3.8512, time 1393.17ms, mfu 4.17%
  data: 310.3ms, grad_accum: 1507.4ms (fw: 13.3ms, bw: 252.7ms)
  grad_proc: 19.2ms, optimizer: 1.5ms, param_check: 20.1ms
  loss_proc: 0.0ms, instability: 76.4ms
  cleanup: 0.5ms, gpu_sync: 0.5ms
  measured: 1549.2ms, unaccounted: -156.0ms (-11.2%)
iter 560: loss 3.7358, time 1580.68ms, mfu 4.12%
  data: 290.6ms, grad_accum: 1720.1ms (fw: 12.7ms, bw: 239.7ms)
  grad_proc: 20.2ms, optimizer: 1.9ms, param_check: 23.8ms
  loss_proc: 0.0ms, instability: 73.6ms
  cleanup: 0.4ms, gpu_sync: 0.4ms
  measured: 1766.9ms, unaccounted: -186.2ms (-11.8%)
iter 580: loss 3.8469, time 1332.54ms, mfu 4.15%
  data: 238.2ms, grad_accum: 1481.6ms (fw: 13.4ms, bw: 186.2ms)
  grad_proc: 19.8ms, optimizer: 1.9ms, param_check: 22.9ms
  loss_proc: 0.0ms, instability: 71.3ms
  cleanup: 0.1ms, gpu_sync: 0.5ms
  measured: 1526.8ms, unaccounted: -194.3ms (-14.6%)

--- Starting validation at iteration 600 ---
Using validation set with samples from all 14 stages
  Validation complete: 20 batches processed (320 samples), avg loss = 3.8234
  Per-stage validation losses:
    Stage 0 (sticky): 3.5663 (23 samples) - ratio=0.4
    Stage 1 (sticky): 3.4529 (16 samples) - ratio=0.6
    Stage 2 (random): 3.7447 (23 samples) - max_ratio=0.5
    Stage 3 (sticky): 4.5221 (7 samples) - ratio=0.6
    Stage 4 (sticky): 3.1290 (16 samples) - ratio=0.6
    Stage 5 (random): 3.2741 (23 samples) - max_ratio=0.2
    Stage 6 (sticky): 3.3807 (7 samples) - ratio=0.2
    Stage 7 (sticky): 5.9740 (16 samples) - ratio=0.4
    Stage 8 (sticky): 4.7614 (23 samples) - ratio=0.4
    Stage 9 (sticky): 4.5182 (7 samples) - ratio=0.6
    Stage 10 (sticky): 3.2718 (23 samples) - ratio=0.7
    Stage 11 (sticky): 3.5347 (16 samples) - ratio=0.8
    Stage 12 (sticky): 3.6710 (7 samples) - ratio=0.8
    Stage 13 (sticky): 3.5244 (23 samples) - ratio=0.9
--- Validation complete ---
step 600: train loss 3.7415, val loss 3.8234, lr 0.000300
Stage 0 (sticky): target_ratio=0.4, p1=0.1, p2=0.3, stale_count=0
  val model vs random: 18.30x better
  val avg correct prob: 0.2288 (random: 0.0125)
  val signal to noise: 5.34 (median: 0.51)
  Most likely guess correct P %: 35.3%
  Stage 0: New best val loss 3.8234, reset stale count to 0

saving checkpoint to out/ckpt_unmasking_600.pt
iter 600: loss 3.8676, time 5376.93ms, mfu 3.84%
  data: 230.5ms, grad_accum: 1495.0ms (fw: 16.8ms, bw: 182.2ms)
  grad_proc: 16.4ms, optimizer: 1.7ms, param_check: 19.9ms
  loss_proc: 0.0ms, instability: 75.6ms
  cleanup: 0.4ms, gpu_sync: 0.4ms
  measured: 1533.9ms, unaccounted: 3843.0ms (71.5%)
  validation: 8998.78ms (data: 19.05ms, forward: 0.00ms, loss: 72.97ms)
Masking: stage=0 (sticky), actual_ratio=0.503, target=0.4, p1=0.1, p2=0.3
iter 620: loss 3.8854, time 1337.01ms, mfu 3.89%
  data: 236.4ms, grad_accum: 1467.8ms (fw: 16.5ms, bw: 177.6ms)
  grad_proc: 20.3ms, optimizer: 1.9ms, param_check: 24.1ms
  loss_proc: 0.0ms, instability: 70.9ms
  cleanup: 0.3ms, gpu_sync: 0.5ms
  measured: 1514.9ms, unaccounted: -177.9ms (-13.3%)
iter 640: loss 3.7178, time 1624.45ms, mfu 3.86%
  data: 253.4ms, grad_accum: 1397.2ms (fw: 13.8ms, bw: 190.4ms)
  grad_proc: 23.3ms, optimizer: 1.8ms, param_check: 31.9ms
  loss_proc: 0.0ms, instability: 86.3ms
  cleanup: 0.3ms, gpu_sync: 0.6ms
  measured: 1455.0ms, unaccounted: 169.4ms (10.4%)
iter 660: loss 3.6770, time 1306.87ms, mfu 3.92%
  data: 220.1ms, grad_accum: 1266.3ms (fw: 12.4ms, bw: 174.6ms)
  grad_proc: 16.6ms, optimizer: 1.3ms, param_check: 18.7ms
  loss_proc: 0.0ms, instability: 72.9ms
  cleanup: 0.2ms, gpu_sync: 0.5ms
  measured: 1303.7ms, unaccounted: 3.1ms (0.2%)
iter 680: loss 3.6694, time 1289.17ms, mfu 3.98%
  data: 224.4ms, grad_accum: 1333.7ms (fw: 12.4ms, bw: 179.8ms)
  grad_proc: 17.7ms, optimizer: 1.3ms, param_check: 20.1ms
  loss_proc: 0.1ms, instability: 71.3ms
  cleanup: 0.2ms, gpu_sync: 0.5ms
  measured: 1373.4ms, unaccounted: -84.3ms (-6.5%)
iter 700: loss 3.7635, time 1303.57ms, mfu 4.02%
  data: 213.8ms, grad_accum: 1236.8ms (fw: 12.4ms, bw: 168.0ms)
  grad_proc: 17.2ms, optimizer: 1.3ms, param_check: 18.3ms
  loss_proc: 0.0ms, instability: 71.2ms
  cleanup: 0.3ms, gpu_sync: 0.6ms
  measured: 1274.5ms, unaccounted: 29.1ms (2.2%)
iter 720: loss 3.6794, time 1271.39ms, mfu 4.08%
  data: 213.5ms, grad_accum: 1235.0ms (fw: 12.2ms, bw: 169.0ms)
  grad_proc: 16.2ms, optimizer: 1.4ms, param_check: 18.6ms
  loss_proc: 0.0ms, instability: 70.7ms
  cleanup: 0.3ms, gpu_sync: 0.5ms
  measured: 1271.9ms, unaccounted: -0.5ms (-0.0%)
iter 740: loss 3.7755, time 1298.79ms, mfu 4.12%
  data: 214.4ms, grad_accum: 1245.5ms (fw: 12.7ms, bw: 169.0ms)
  grad_proc: 16.6ms, optimizer: 1.4ms, param_check: 18.8ms
  loss_proc: 0.0ms, instability: 71.4ms
  cleanup: 0.1ms, gpu_sync: 0.6ms
  measured: 1283.0ms, unaccounted: 15.8ms (1.2%)
iter 760: loss 3.6867, time 1297.64ms, mfu 4.15%
  data: 227.3ms, grad_accum: 1433.4ms (fw: 13.2ms, bw: 180.3ms)
  grad_proc: 19.6ms, optimizer: 1.6ms, param_check: 20.2ms
  loss_proc: 0.0ms, instability: 72.3ms
  cleanup: 0.5ms, gpu_sync: 0.6ms
  measured: 1475.9ms, unaccounted: -178.2ms (-13.7%)
iter 780: loss 3.8483, time 1627.84ms, mfu 4.10%
  data: 278.4ms, grad_accum: 1534.0ms (fw: 16.1ms, bw: 198.7ms)
  grad_proc: 22.4ms, optimizer: 2.2ms, param_check: 31.4ms
  loss_proc: 0.0ms, instability: 73.8ms
  cleanup: 0.4ms, gpu_sync: 0.6ms
  measured: 1591.0ms, unaccounted: 36.9ms (2.3%)

--- Starting validation at iteration 800 ---
Using validation set with samples from all 14 stages
  Validation complete: 20 batches processed (320 samples), avg loss = 3.5821
  Per-stage validation losses:
    Stage 0 (sticky): 3.3737 (23 samples) - ratio=0.4
    Stage 1 (sticky): 3.3669 (16 samples) - ratio=0.6
    Stage 2 (random): 3.5932 (23 samples) - max_ratio=0.5
    Stage 3 (sticky): 4.1304 (7 samples) - ratio=0.6
    Stage 4 (sticky): 3.0023 (16 samples) - ratio=0.6
    Stage 5 (random): 3.1560 (23 samples) - max_ratio=0.2
    Stage 6 (sticky): 3.3090 (7 samples) - ratio=0.2
    Stage 7 (sticky): 5.3292 (16 samples) - ratio=0.4
    Stage 8 (sticky): 4.3014 (23 samples) - ratio=0.4
    Stage 9 (sticky): 4.0043 (7 samples) - ratio=0.6
    Stage 10 (sticky): 3.0192 (23 samples) - ratio=0.7
    Stage 11 (sticky): 3.3159 (16 samples) - ratio=0.8
    Stage 12 (sticky): 3.4686 (7 samples) - ratio=0.8
    Stage 13 (sticky): 3.4140 (23 samples) - ratio=0.9
--- Validation complete ---
step 800: train loss 3.5985, val loss 3.5821, lr 0.000400
Stage 0 (sticky): target_ratio=0.4, p1=0.1, p2=0.3, stale_count=0
  val model vs random: 21.20x better
  val avg correct prob: 0.2650 (random: 0.0125)
  val signal to noise: 7.74 (median: 0.56)
  Most likely guess correct P %: 38.3%
  Stage 0: New best val loss 3.5821, reset stale count to 0

saving checkpoint to out/ckpt_unmasking_800.pt
iter 800: loss 3.6470, time 5956.24ms, mfu 3.78%
  data: 215.9ms, grad_accum: 1388.4ms (fw: 33.7ms, bw: 169.1ms)
  grad_proc: 17.0ms, optimizer: 1.3ms, param_check: 19.1ms
  loss_proc: 0.0ms, instability: 53.5ms
  cleanup: 0.2ms, gpu_sync: 0.7ms
  measured: 1426.7ms, unaccounted: 4529.5ms (76.0%)
  validation: 8016.54ms (data: 19.82ms, forward: 0.01ms, loss: 75.84ms)
Masking: stage=0 (sticky), actual_ratio=0.495, target=0.4, p1=0.1, p2=0.3
iter 820: loss 3.8822, time 1281.28ms, mfu 3.86%
  data: 213.5ms, grad_accum: 1234.6ms (fw: 12.6ms, bw: 169.2ms)
  grad_proc: 16.9ms, optimizer: 1.5ms, param_check: 19.0ms
  loss_proc: 0.0ms, instability: 70.8ms
  cleanup: 0.3ms, gpu_sync: 0.6ms
  measured: 1272.8ms, unaccounted: 8.5ms (0.7%)
iter 840: loss 3.5175, time 1693.83ms, mfu 3.82%
  data: 281.2ms, grad_accum: 1460.6ms (fw: 15.4ms, bw: 196.2ms)
  grad_proc: 28.6ms, optimizer: 1.4ms, param_check: 53.3ms
  loss_proc: 0.0ms, instability: 85.7ms
  cleanup: 0.5ms, gpu_sync: 0.5ms
  measured: 1544.9ms, unaccounted: 148.9ms (8.8%)
iter 860: loss 3.6707, time 1303.28ms, mfu 3.88%
  data: 220.6ms, grad_accum: 1271.1ms (fw: 11.9ms, bw: 173.9ms)
  grad_proc: 16.4ms, optimizer: 1.3ms, param_check: 18.7ms
  loss_proc: 0.0ms, instability: 74.2ms
  cleanup: 0.4ms, gpu_sync: 0.5ms
  measured: 1308.5ms, unaccounted: -5.2ms (-0.4%)
iter 880: loss 3.7052, time 1361.53ms, mfu 3.92%
  data: 249.3ms, grad_accum: 1494.2ms (fw: 12.5ms, bw: 180.3ms)
  grad_proc: 19.0ms, optimizer: 2.0ms, param_check: 22.9ms
  loss_proc: 0.0ms, instability: 75.8ms
  cleanup: 0.6ms, gpu_sync: 0.4ms
  measured: 1539.2ms, unaccounted: -177.6ms (-13.0%)
iter 900: loss 3.6614, time 1345.64ms, mfu 3.96%
  data: 282.1ms, grad_accum: 1460.2ms (fw: 13.5ms, bw: 218.9ms)
  grad_proc: 21.0ms, optimizer: 2.0ms, param_check: 26.4ms
  loss_proc: 0.0ms, instability: 73.4ms
  cleanup: 0.6ms, gpu_sync: 0.4ms
  measured: 1510.6ms, unaccounted: -165.0ms (-12.3%)
iter 920: loss 3.6819, time 1267.24ms, mfu 4.02%
  data: 251.1ms, grad_accum: 1388.2ms (fw: 12.6ms, bw: 207.1ms)
  grad_proc: 17.4ms, optimizer: 1.5ms, param_check: 20.5ms
  loss_proc: 0.0ms, instability: 70.1ms
  cleanup: 0.5ms, gpu_sync: 0.5ms
  measured: 1428.4ms, unaccounted: -161.1ms (-12.7%)
iter 940: loss 3.5574, time 1271.22ms, mfu 4.08%
  data: 215.1ms, grad_accum: 1235.4ms (fw: 13.2ms, bw: 168.2ms)
  grad_proc: 16.7ms, optimizer: 1.6ms, param_check: 19.8ms
  loss_proc: 0.0ms, instability: 69.5ms
  cleanup: 0.1ms, gpu_sync: 0.5ms
  measured: 1274.1ms, unaccounted: -2.9ms (-0.2%)
iter 960: loss 3.6187, time 1262.19ms, mfu 4.13%
  data: 215.4ms, grad_accum: 1237.8ms (fw: 12.1ms, bw: 168.3ms)
  grad_proc: 16.8ms, optimizer: 1.6ms, param_check: 18.2ms
  loss_proc: 0.0ms, instability: 70.5ms
  cleanup: 0.3ms, gpu_sync: 0.5ms
  measured: 1275.2ms, unaccounted: -13.0ms (-1.0%)
iter 980: loss 3.6194, time 1276.27ms, mfu 4.17%
  data: 213.1ms, grad_accum: 1235.3ms (fw: 12.7ms, bw: 168.8ms)
  grad_proc: 16.2ms, optimizer: 1.3ms, param_check: 18.3ms
  loss_proc: 0.0ms, instability: 70.1ms
  cleanup: 0.1ms, gpu_sync: 0.4ms
  measured: 1271.6ms, unaccounted: 4.7ms (0.4%)

--- Starting validation at iteration 1000 ---
Using validation set with samples from all 14 stages
  Validation complete: 20 batches processed (320 samples), avg loss = 3.4353
  Per-stage validation losses:
    Stage 0 (sticky): 3.2319 (23 samples) - ratio=0.4
    Stage 1 (sticky): 3.2922 (16 samples) - ratio=0.6
    Stage 2 (random): 3.4841 (23 samples) - max_ratio=0.5
    Stage 3 (sticky): 3.9280 (7 samples) - ratio=0.6
    Stage 4 (sticky): 2.9323 (16 samples) - ratio=0.6
    Stage 5 (random): 3.0982 (23 samples) - max_ratio=0.2
    Stage 6 (sticky): 3.2630 (7 samples) - ratio=0.2
    Stage 7 (sticky): 4.9417 (16 samples) - ratio=0.4
    Stage 8 (sticky): 4.0003 (23 samples) - ratio=0.4
    Stage 9 (sticky): 3.7342 (7 samples) - ratio=0.6
    Stage 10 (sticky): 2.8777 (23 samples) - ratio=0.7
    Stage 11 (sticky): 3.1987 (16 samples) - ratio=0.8
    Stage 12 (sticky): 3.3587 (7 samples) - ratio=0.8
    Stage 13 (sticky): 3.3368 (23 samples) - ratio=0.9
--- Validation complete ---
step 1000: train loss 3.5283, val loss 3.4353, lr 0.000500
Stage 0 (sticky): target_ratio=0.4, p1=0.1, p2=0.3, stale_count=0
  val model vs random: 22.90x better
  val avg correct prob: 0.2862 (random: 0.0125)
  val signal to noise: 8.68 (median: 0.61)
  Most likely guess correct P %: 40.2%
  Stage 0: New best val loss 3.4353, reset stale count to 0

saving checkpoint to out/ckpt_unmasking_1000.pt
iter 1000: loss 3.4087, time 5257.43ms, mfu 3.87%
  data: 217.6ms, grad_accum: 1258.1ms (fw: 13.5ms, bw: 171.7ms)
  grad_proc: 17.1ms, optimizer: 1.4ms, param_check: 19.3ms
  loss_proc: 0.1ms, instability: 76.1ms
  cleanup: 0.2ms, gpu_sync: 0.4ms
  measured: 1296.5ms, unaccounted: 3961.0ms (75.3%)
  validation: 7286.64ms (data: 20.02ms, forward: 0.01ms, loss: 61.58ms)
Masking: stage=0 (sticky), actual_ratio=0.499, target=0.4, p1=0.1, p2=0.3
iter 1020: loss 3.6857, time 1620.99ms, mfu 3.84%
  data: 252.8ms, grad_accum: 1412.2ms (fw: 13.5ms, bw: 187.1ms)
  grad_proc: 22.4ms, optimizer: 1.4ms, param_check: 27.5ms
  loss_proc: 0.0ms, instability: 86.4ms
  cleanup: 0.3ms, gpu_sync: 0.5ms
  measured: 1464.3ms, unaccounted: 156.7ms (9.7%)
iter 1040: loss 3.5401, time 1299.64ms, mfu 3.90%
  data: 220.6ms, grad_accum: 1331.4ms (fw: 12.9ms, bw: 175.4ms)
  grad_proc: 18.7ms, optimizer: 1.7ms, param_check: 19.9ms
  loss_proc: 0.0ms, instability: 69.9ms
  cleanup: 0.3ms, gpu_sync: 0.6ms
  measured: 1372.6ms, unaccounted: -72.9ms (-5.6%)
iter 1060: loss 3.6928, time 1518.10ms, mfu 3.89%
  data: 241.5ms, grad_accum: 1332.9ms (fw: 35.3ms, bw: 176.2ms)
  grad_proc: 19.8ms, optimizer: 1.8ms, param_check: 25.0ms
  loss_proc: 0.0ms, instability: 54.8ms
  cleanup: 0.3ms, gpu_sync: 1.0ms
  measured: 1380.6ms, unaccounted: 137.5ms (9.1%)
iter 1080: loss 3.5280, time 1275.22ms, mfu 3.96%
  data: 214.7ms, grad_accum: 1241.5ms (fw: 12.5ms, bw: 170.5ms)
  grad_proc: 17.8ms, optimizer: 1.6ms, param_check: 20.3ms
  loss_proc: 0.0ms, instability: 71.0ms
  cleanup: 0.3ms, gpu_sync: 1.1ms
  measured: 1282.5ms, unaccounted: -7.3ms (-0.6%)
iter 1100: loss 3.6416, time 1272.68ms, mfu 4.02%
  data: 213.4ms, grad_accum: 1244.6ms (fw: 12.2ms, bw: 169.8ms)
  grad_proc: 17.2ms, optimizer: 1.6ms, param_check: 19.4ms
  loss_proc: 0.0ms, instability: 71.2ms
  cleanup: 0.3ms, gpu_sync: 1.0ms
  measured: 1284.1ms, unaccounted: -11.4ms (-0.9%)
iter 1120: loss 3.5790, time 1323.34ms, mfu 4.06%
  data: 217.5ms, grad_accum: 1244.7ms (fw: 12.9ms, bw: 171.6ms)
  grad_proc: 16.0ms, optimizer: 1.5ms, param_check: 18.8ms
  loss_proc: 0.0ms, instability: 71.9ms
  cleanup: 0.3ms, gpu_sync: 0.9ms
  measured: 1282.2ms, unaccounted: 41.1ms (3.1%)
iter 1140: loss 3.6099, time 1285.88ms, mfu 4.10%
  data: 218.1ms, grad_accum: 1322.1ms (fw: 13.2ms, bw: 170.8ms)
  grad_proc: 18.2ms, optimizer: 1.5ms, param_check: 19.9ms
  loss_proc: 0.0ms, instability: 70.0ms
  cleanup: 0.3ms, gpu_sync: 0.8ms
  measured: 1362.9ms, unaccounted: -77.0ms (-6.0%)
iter 1160: loss 3.4447, time 1310.55ms, mfu 4.14%
  data: 217.4ms, grad_accum: 1329.4ms (fw: 12.7ms, bw: 171.9ms)
  grad_proc: 17.0ms, optimizer: 1.9ms, param_check: 20.4ms
  loss_proc: 0.0ms, instability: 72.7ms
  cleanup: 0.6ms, gpu_sync: 0.9ms
  measured: 1370.2ms, unaccounted: -59.6ms (-4.6%)
iter 1180: loss 3.4923, time 1291.62ms, mfu 4.17%
  data: 248.8ms, grad_accum: 1372.9ms (fw: 14.3ms, bw: 177.5ms)
  grad_proc: 20.2ms, optimizer: 2.0ms, param_check: 26.8ms
  loss_proc: 0.0ms, instability: 69.4ms
  cleanup: 0.4ms, gpu_sync: 0.9ms
  measured: 1423.2ms, unaccounted: -131.6ms (-10.2%)

--- Starting validation at iteration 1200 ---
Using validation set with samples from all 14 stages
  Validation complete: 20 batches processed (320 samples), avg loss = 3.3545
  Per-stage validation losses:
    Stage 0 (sticky): 3.1390 (23 samples) - ratio=0.4
    Stage 1 (sticky): 3.2625 (16 samples) - ratio=0.6
    Stage 2 (random): 3.4046 (23 samples) - max_ratio=0.5
    Stage 3 (sticky): 3.7400 (7 samples) - ratio=0.6
    Stage 4 (sticky): 2.8940 (16 samples) - ratio=0.6
    Stage 5 (random): 3.0700 (23 samples) - max_ratio=0.2
    Stage 6 (sticky): 3.2374 (7 samples) - ratio=0.2
    Stage 7 (sticky): 4.6957 (16 samples) - ratio=0.4
    Stage 8 (sticky): 3.8466 (23 samples) - ratio=0.4
    Stage 9 (sticky): 3.6909 (7 samples) - ratio=0.6
    Stage 10 (sticky): 2.8104 (23 samples) - ratio=0.7
    Stage 11 (sticky): 3.1378 (16 samples) - ratio=0.8
    Stage 12 (sticky): 3.2895 (7 samples) - ratio=0.8
    Stage 13 (sticky): 3.3004 (23 samples) - ratio=0.9
--- Validation complete ---
step 1200: train loss 3.4166, val loss 3.3545, lr 0.000600
Stage 0 (sticky): target_ratio=0.4, p1=0.1, p2=0.3, stale_count=0
  val model vs random: 24.51x better
  val avg correct prob: 0.3064 (random: 0.0125)
  val signal to noise: 10.16 (median: 0.63)
  Most likely guess correct P %: 41.7%
  Stage 0: New best val loss 3.3545, reset stale count to 0

saving checkpoint to out/ckpt_unmasking_1200.pt
iter 1200: loss 3.4704, time 5872.78ms, mfu 3.85%
  data: 256.2ms, grad_accum: 1415.2ms (fw: 29.8ms, bw: 209.0ms)
  grad_proc: 19.5ms, optimizer: 1.5ms, param_check: 23.9ms
  loss_proc: 0.0ms, instability: 62.8ms
  cleanup: 0.5ms, gpu_sync: 0.9ms
  measured: 1461.5ms, unaccounted: 4411.3ms (75.1%)
  validation: 6814.29ms (data: 19.64ms, forward: 0.02ms, loss: 61.75ms)
Masking: stage=0 (sticky), actual_ratio=0.477, target=0.4, p1=0.1, p2=0.3
iter 1220: loss 3.5132, time 1593.34ms, mfu 3.83%
  data: 281.8ms, grad_accum: 1470.4ms (fw: 12.9ms, bw: 190.8ms)
  grad_proc: 43.9ms, optimizer: 1.6ms, param_check: 42.9ms
  loss_proc: 0.0ms, instability: 84.2ms
  cleanup: 0.5ms, gpu_sync: 0.8ms
  measured: 1560.0ms, unaccounted: 33.4ms (2.1%)
iter 1240: loss 3.4969, time 1283.20ms, mfu 3.90%
  data: 216.7ms, grad_accum: 1325.8ms (fw: 13.2ms, bw: 171.1ms)
  grad_proc: 18.5ms, optimizer: 1.6ms, param_check: 20.6ms
  loss_proc: 0.0ms, instability: 69.6ms
  cleanup: 0.1ms, gpu_sync: 0.8ms
  measured: 1367.4ms, unaccounted: -84.2ms (-6.6%)
iter 1260: loss 3.7409, time 1267.03ms, mfu 3.97%
  data: 214.8ms, grad_accum: 1256.2ms (fw: 12.5ms, bw: 171.2ms)
  grad_proc: 16.7ms, optimizer: 1.4ms, param_check: 19.9ms
  loss_proc: 0.0ms, instability: 70.1ms
  cleanup: 0.3ms, gpu_sync: 0.4ms
  measured: 1294.9ms, unaccounted: -27.8ms (-2.2%)
iter 1280: loss 3.6354, time 1286.53ms, mfu 4.03%
  data: 216.8ms, grad_accum: 1260.2ms (fw: 12.7ms, bw: 170.8ms)
  grad_proc: 18.2ms, optimizer: 1.6ms, param_check: 20.2ms
  loss_proc: 0.0ms, instability: 71.4ms
  cleanup: 0.5ms, gpu_sync: 0.3ms
  measured: 1301.0ms, unaccounted: -14.4ms (-1.1%)
iter 1300: loss 3.2950, time 1309.07ms, mfu 4.07%
  data: 221.6ms, grad_accum: 1338.6ms (fw: 14.1ms, bw: 173.3ms)
  grad_proc: 16.7ms, optimizer: 1.4ms, param_check: 19.5ms
  loss_proc: 0.0ms, instability: 69.3ms
  cleanup: 0.4ms, gpu_sync: 0.4ms
  measured: 1377.0ms, unaccounted: -67.9ms (-5.2%)
iter 1320: loss 3.4516, time 1293.68ms, mfu 4.11%
  data: 256.2ms, grad_accum: 1378.2ms (fw: 12.4ms, bw: 211.3ms)
  grad_proc: 18.0ms, optimizer: 1.6ms, param_check: 21.5ms
  loss_proc: 0.0ms, instability: 71.6ms
  cleanup: 0.2ms, gpu_sync: 0.3ms
  measured: 1419.8ms, unaccounted: -126.1ms (-9.7%)
iter 1340: loss 3.5705, time 2196.92ms, mfu 3.96%
  data: 252.7ms, grad_accum: 1336.7ms (fw: 13.8ms, bw: 209.2ms)
  grad_proc: 17.2ms, optimizer: 1.3ms, param_check: 21.5ms
  loss_proc: 0.0ms, instability: 120.4ms
  cleanup: 0.3ms, gpu_sync: 0.4ms
  measured: 1377.4ms, unaccounted: 819.5ms (37.3%)
iter 1360: loss 3.4891, time 1291.54ms, mfu 4.02%
  data: 217.7ms, grad_accum: 1253.3ms (fw: 12.9ms, bw: 171.4ms)
  grad_proc: 17.6ms, optimizer: 1.4ms, param_check: 19.3ms
  loss_proc: 0.0ms, instability: 71.4ms
  cleanup: 0.3ms, gpu_sync: 0.4ms
  measured: 1292.1ms, unaccounted: -0.6ms (-0.0%)
iter 1380: loss 3.3637, time 1386.20ms, mfu 4.03%
  data: 229.6ms, grad_accum: 1305.7ms (fw: 13.1ms, bw: 180.5ms)
  grad_proc: 18.6ms, optimizer: 1.3ms, param_check: 20.3ms
  loss_proc: 0.0ms, instability: 75.5ms
  cleanup: 0.5ms, gpu_sync: 0.4ms
  measured: 1346.8ms, unaccounted: 39.4ms (2.8%)

--- Starting validation at iteration 1400 ---
Using validation set with samples from all 14 stages
  Validation complete: 20 batches processed (320 samples), avg loss = 3.2672
  Per-stage validation losses:
    Stage 0 (sticky): 3.0532 (23 samples) - ratio=0.4
    Stage 1 (sticky): 3.2201 (16 samples) - ratio=0.6
    Stage 2 (random): 3.3218 (23 samples) - max_ratio=0.5
    Stage 3 (sticky): 3.6023 (7 samples) - ratio=0.6
    Stage 4 (sticky): 2.8338 (16 samples) - ratio=0.6
    Stage 5 (random): 3.0380 (23 samples) - max_ratio=0.2
    Stage 6 (sticky): 3.1883 (7 samples) - ratio=0.2
    Stage 7 (sticky): 4.5467 (16 samples) - ratio=0.4
    Stage 8 (sticky): 3.6982 (23 samples) - ratio=0.4
    Stage 9 (sticky): 3.5526 (7 samples) - ratio=0.6
    Stage 10 (sticky): 2.7203 (23 samples) - ratio=0.7
    Stage 11 (sticky): 3.0456 (16 samples) - ratio=0.8
    Stage 12 (sticky): 3.2112 (7 samples) - ratio=0.8
    Stage 13 (sticky): 3.2400 (23 samples) - ratio=0.9
--- Validation complete ---
step 1400: train loss 3.3425, val loss 3.2672, lr 0.000700
Stage 0 (sticky): target_ratio=0.4, p1=0.1, p2=0.3, stale_count=0
  val model vs random: 25.66x better
  val avg correct prob: 0.3208 (random: 0.0125)
  val signal to noise: 10.85 (median: 0.70)
  Most likely guess correct P %: 43.3%
  Stage 0: New best val loss 3.2672, reset stale count to 0

saving checkpoint to out/ckpt_unmasking_1400.pt
iter 1400: loss 3.2972, time 6206.22ms, mfu 3.73%
  data: 230.4ms, grad_accum: 1322.6ms (fw: 30.5ms, bw: 176.4ms)
  grad_proc: 18.5ms, optimizer: 1.9ms, param_check: 20.2ms
  loss_proc: 0.0ms, instability: 61.3ms
  cleanup: 0.3ms, gpu_sync: 2.6ms
  measured: 1366.1ms, unaccounted: 4840.1ms (78.0%)
  validation: 6500.35ms (data: 21.92ms, forward: 0.02ms, loss: 62.73ms)
Masking: stage=0 (sticky), actual_ratio=0.497, target=0.4, p1=0.1, p2=0.3
iter 1420: loss 3.3360, time 2846.30ms, mfu 3.56%
  data: 226.1ms, grad_accum: 1445.7ms (fw: 13.6ms, bw: 178.0ms)
  grad_proc: 17.0ms, optimizer: 1.4ms, param_check: 19.4ms
  loss_proc: 0.0ms, instability: 221.0ms
  cleanup: 0.6ms, gpu_sync: 2.6ms
  measured: 1486.7ms, unaccounted: 1359.6ms (47.8%)
iter 1440: loss 3.4747, time 2752.23ms, mfu 3.41%
  data: 245.1ms, grad_accum: 1494.5ms (fw: 13.5ms, bw: 179.7ms)
  grad_proc: 20.1ms, optimizer: 2.0ms, param_check: 23.5ms
  loss_proc: 0.0ms, instability: 214.0ms
  cleanup: 0.3ms, gpu_sync: 2.5ms
  measured: 1542.8ms, unaccounted: 1209.4ms (43.9%)
iter 1460: loss 3.4956, time 1301.15ms, mfu 3.52%
  data: 221.0ms, grad_accum: 1278.0ms (fw: 12.3ms, bw: 175.7ms)
  grad_proc: 17.3ms, optimizer: 1.6ms, param_check: 20.1ms
  loss_proc: 0.0ms, instability: 72.6ms
  cleanup: 0.2ms, gpu_sync: 2.6ms
  measured: 1319.7ms, unaccounted: -18.6ms (-1.4%)
iter 1480: loss 3.3320, time 1264.86ms, mfu 3.63%
  data: 214.2ms, grad_accum: 1410.0ms (fw: 12.4ms, bw: 169.4ms)
  grad_proc: 17.5ms, optimizer: 1.3ms, param_check: 19.7ms
  loss_proc: 0.1ms, instability: 70.4ms
  cleanup: 0.5ms, gpu_sync: 2.6ms
  measured: 1451.5ms, unaccounted: -186.6ms (-14.8%)
iter 1500: loss 3.4030, time 1282.94ms, mfu 3.72%
  data: 214.7ms, grad_accum: 1235.4ms (fw: 14.4ms, bw: 169.1ms)
  grad_proc: 17.4ms, optimizer: 1.8ms, param_check: 19.6ms
  loss_proc: 0.0ms, instability: 68.3ms
  cleanup: 0.4ms, gpu_sync: 2.5ms
  measured: 1277.0ms, unaccounted: 5.9ms (0.5%)
iter 1520: loss 3.3928, time 1294.80ms, mfu 3.79%
  data: 212.7ms, grad_accum: 1279.1ms (fw: 12.6ms, bw: 168.4ms)
  grad_proc: 16.3ms, optimizer: 1.7ms, param_check: 19.5ms
  loss_proc: 0.0ms, instability: 70.3ms
  cleanup: 0.4ms, gpu_sync: 2.5ms
  measured: 1319.4ms, unaccounted: -24.6ms (-1.9%)
iter 1540: loss 3.2222, time 1276.28ms, mfu 3.87%
  data: 212.2ms, grad_accum: 1233.7ms (fw: 12.3ms, bw: 167.5ms)
  grad_proc: 17.8ms, optimizer: 1.5ms, param_check: 18.8ms
  loss_proc: 0.0ms, instability: 70.6ms
  cleanup: 0.3ms, gpu_sync: 2.5ms
  measured: 1274.5ms, unaccounted: 1.8ms (0.1%)
iter 1560: loss 3.4759, time 1273.16ms, mfu 3.94%
  data: 214.6ms, grad_accum: 1305.3ms (fw: 12.4ms, bw: 168.4ms)
  grad_proc: 16.7ms, optimizer: 1.5ms, param_check: 18.1ms
  loss_proc: 0.1ms, instability: 70.4ms
  cleanup: 0.3ms, gpu_sync: 2.5ms
  measured: 1344.4ms, unaccounted: -71.2ms (-5.6%)
iter 1580: loss 3.4181, time 1280.76ms, mfu 4.00%
  data: 213.0ms, grad_accum: 1234.8ms (fw: 13.1ms, bw: 168.3ms)
  grad_proc: 16.5ms, optimizer: 1.7ms, param_check: 19.6ms
  loss_proc: 0.0ms, instability: 69.7ms
  cleanup: 0.2ms, gpu_sync: 2.5ms
  measured: 1275.3ms, unaccounted: 5.5ms (0.4%)

--- Starting validation at iteration 1600 ---
Using validation set with samples from all 14 stages
  Validation complete: 20 batches processed (320 samples), avg loss = 3.2039
  Per-stage validation losses:
    Stage 0 (sticky): 3.0280 (23 samples) - ratio=0.4
    Stage 1 (sticky): 3.2111 (16 samples) - ratio=0.6
    Stage 2 (random): 3.3286 (23 samples) - max_ratio=0.5
    Stage 3 (sticky): 3.4516 (7 samples) - ratio=0.6
    Stage 4 (sticky): 2.7989 (16 samples) - ratio=0.6
    Stage 5 (random): 3.0116 (23 samples) - max_ratio=0.2
    Stage 6 (sticky): 3.1870 (7 samples) - ratio=0.2
    Stage 7 (sticky): 4.3007 (16 samples) - ratio=0.4
    Stage 8 (sticky): 3.5386 (23 samples) - ratio=0.4
    Stage 9 (sticky): 3.4128 (7 samples) - ratio=0.6
    Stage 10 (sticky): 2.6571 (23 samples) - ratio=0.7
    Stage 11 (sticky): 3.0038 (16 samples) - ratio=0.8
    Stage 12 (sticky): 3.1279 (7 samples) - ratio=0.8
    Stage 13 (sticky): 3.2284 (23 samples) - ratio=0.9
--- Validation complete ---
step 1600: train loss 3.2778, val loss 3.2039, lr 0.000800
Stage 0 (sticky): target_ratio=0.4, p1=0.1, p2=0.3, stale_count=0
  val model vs random: 26.85x better
  val avg correct prob: 0.3357 (random: 0.0125)
  val signal to noise: 12.10 (median: 0.70)
  Most likely guess correct P %: 44.0%
  Stage 0: New best val loss 3.2039, reset stale count to 0

saving checkpoint to out/ckpt_unmasking_1600.pt
iter 1600: loss 3.4077, time 6762.24ms, mfu 3.68%
  data: 232.4ms, grad_accum: 1309.6ms (fw: 19.4ms, bw: 185.4ms)
  grad_proc: 16.7ms, optimizer: 1.7ms, param_check: 18.4ms
  loss_proc: 0.0ms, instability: 64.7ms
  cleanup: 0.3ms, gpu_sync: 0.3ms
  measured: 1347.1ms, unaccounted: 5415.1ms (80.1%)
  validation: 6337.20ms (data: 22.57ms, forward: 0.01ms, loss: 74.29ms)
Masking: stage=0 (sticky), actual_ratio=0.499, target=0.4, p1=0.1, p2=0.3
iter 1620: loss 3.4500, time 1276.67ms, mfu 3.77%
  data: 214.7ms, grad_accum: 1235.9ms (fw: 12.5ms, bw: 168.5ms)
  grad_proc: 16.7ms, optimizer: 1.5ms, param_check: 18.3ms
  loss_proc: 0.0ms, instability: 70.2ms
  cleanup: 0.2ms, gpu_sync: 0.4ms
  measured: 1273.1ms, unaccounted: 3.6ms (0.3%)
iter 1640: loss 3.2543, time 1280.65ms, mfu 3.85%
  data: 212.7ms, grad_accum: 1233.6ms (fw: 12.3ms, bw: 169.3ms)
  grad_proc: 18.3ms, optimizer: 1.7ms, param_check: 19.8ms
  loss_proc: 0.0ms, instability: 70.4ms
  cleanup: 0.3ms, gpu_sync: 0.5ms
  measured: 1274.2ms, unaccounted: 6.4ms (0.5%)
iter 1660: loss 3.3703, time 1978.66ms, mfu 3.76%
  data: 212.6ms, grad_accum: 1309.1ms (fw: 12.5ms, bw: 167.9ms)
  grad_proc: 16.3ms, optimizer: 1.2ms, param_check: 19.4ms
  loss_proc: 0.0ms, instability: 141.4ms
  cleanup: 0.3ms, gpu_sync: 0.4ms
  measured: 1346.6ms, unaccounted: 632.0ms (31.9%)
iter 1680: loss 3.3881, time 1267.35ms, mfu 3.84%
  data: 211.5ms, grad_accum: 1302.6ms (fw: 12.7ms, bw: 167.0ms)
  grad_proc: 17.4ms, optimizer: 1.5ms, param_check: 18.5ms
  loss_proc: 0.0ms, instability: 70.0ms
  cleanup: 0.2ms, gpu_sync: 0.5ms
  measured: 1340.6ms, unaccounted: -73.3ms (-5.8%)
iter 1700: loss 3.3745, time 1277.89ms, mfu 3.91%
  data: 212.6ms, grad_accum: 1374.6ms (fw: 12.1ms, bw: 167.4ms)
  grad_proc: 17.6ms, optimizer: 1.2ms, param_check: 19.0ms
  loss_proc: 0.0ms, instability: 70.9ms
  cleanup: 0.3ms, gpu_sync: 0.6ms
  measured: 1413.3ms, unaccounted: -135.4ms (-10.6%)
iter 1720: loss 3.6134, time 1273.25ms, mfu 3.98%
  data: 214.0ms, grad_accum: 1236.3ms (fw: 12.4ms, bw: 168.8ms)
  grad_proc: 17.5ms, optimizer: 1.6ms, param_check: 18.7ms
  loss_proc: 0.0ms, instability: 70.9ms
  cleanup: 0.2ms, gpu_sync: 0.7ms
  measured: 1275.0ms, unaccounted: -1.8ms (-0.1%)
iter 1740: loss 3.3986, time 1268.80ms, mfu 4.04%
  data: 214.6ms, grad_accum: 1236.7ms (fw: 12.1ms, bw: 169.1ms)
  grad_proc: 16.5ms, optimizer: 1.5ms, param_check: 19.1ms
  loss_proc: 0.0ms, instability: 70.6ms
  cleanup: 0.4ms, gpu_sync: 0.6ms
  measured: 1274.7ms, unaccounted: -5.9ms (-0.5%)
iter 1760: loss 3.5350, time 1263.63ms, mfu 4.09%
  data: 211.4ms, grad_accum: 1232.9ms (fw: 12.5ms, bw: 168.4ms)
  grad_proc: 17.9ms, optimizer: 1.7ms, param_check: 19.1ms
  loss_proc: 0.0ms, instability: 70.2ms
  cleanup: 0.4ms, gpu_sync: 0.7ms
  measured: 1272.7ms, unaccounted: -9.0ms (-0.7%)
iter 1780: loss 3.4718, time 1294.48ms, mfu 4.13%
  data: 214.1ms, grad_accum: 1235.3ms (fw: 12.6ms, bw: 167.4ms)
  grad_proc: 17.5ms, optimizer: 1.5ms, param_check: 19.6ms
  loss_proc: 0.0ms, instability: 70.1ms
  cleanup: 0.4ms, gpu_sync: 0.6ms
  measured: 1274.9ms, unaccounted: 19.6ms (1.5%)

--- Starting validation at iteration 1800 ---
Using validation set with samples from all 14 stages
  Validation complete: 20 batches processed (320 samples), avg loss = 3.1412
  Per-stage validation losses:
    Stage 0 (sticky): 2.9318 (23 samples) - ratio=0.4
    Stage 1 (sticky): 3.1550 (16 samples) - ratio=0.6
    Stage 2 (random): 3.2526 (23 samples) - max_ratio=0.5
    Stage 3 (sticky): 3.3422 (7 samples) - ratio=0.6
    Stage 4 (sticky): 2.7627 (16 samples) - ratio=0.6
    Stage 5 (random): 2.9816 (23 samples) - max_ratio=0.2
    Stage 6 (sticky): 3.1396 (7 samples) - ratio=0.2
    Stage 7 (sticky): 4.2362 (16 samples) - ratio=0.4
    Stage 8 (sticky): 3.4620 (23 samples) - ratio=0.4
    Stage 9 (sticky): 3.3255 (7 samples) - ratio=0.6
    Stage 10 (sticky): 2.6022 (23 samples) - ratio=0.7
    Stage 11 (sticky): 2.9423 (16 samples) - ratio=0.8
    Stage 12 (sticky): 3.0798 (7 samples) - ratio=0.8
    Stage 13 (sticky): 3.1905 (23 samples) - ratio=0.9
--- Validation complete ---
step 1800: train loss 3.2914, val loss 3.1412, lr 0.000900
Stage 0 (sticky): target_ratio=0.4, p1=0.1, p2=0.3, stale_count=0
  val model vs random: 27.41x better
  val avg correct prob: 0.3426 (random: 0.0125)
  val signal to noise: 12.51 (median: 0.75)
  Most likely guess correct P %: 45.2%
  Stage 0: New best val loss 3.1412, reset stale count to 0

saving checkpoint to out/ckpt_unmasking_1800.pt
iter 1800: loss 3.3175, time 5967.93ms, mfu 3.82%
  data: 225.3ms, grad_accum: 1289.7ms (fw: 31.6ms, bw: 171.8ms)
  grad_proc: 18.2ms, optimizer: 1.9ms, param_check: 20.5ms
  loss_proc: 0.1ms, instability: 57.7ms
  cleanup: 0.4ms, gpu_sync: 0.8ms
  measured: 1331.4ms, unaccounted: 4636.5ms (77.7%)
  validation: 6114.42ms (data: 21.44ms, forward: 0.01ms, loss: 73.39ms)
Masking: stage=0 (sticky), actual_ratio=0.499, target=0.4, p1=0.1, p2=0.3
iter 1820: loss 3.3594, time 1467.79ms, mfu 3.83%
  data: 296.5ms, grad_accum: 1499.2ms (fw: 20.5ms, bw: 205.7ms)
  grad_proc: 42.9ms, optimizer: 1.7ms, param_check: 35.1ms
  loss_proc: 0.0ms, instability: 64.3ms
  cleanup: 0.4ms, gpu_sync: 0.7ms
  measured: 1579.8ms, unaccounted: -112.1ms (-7.6%)
iter 1840: loss 3.3131, time 1277.36ms, mfu 3.90%
  data: 252.6ms, grad_accum: 1375.5ms (fw: 12.6ms, bw: 206.2ms)
  grad_proc: 16.7ms, optimizer: 1.4ms, param_check: 20.7ms
  loss_proc: 0.0ms, instability: 70.2ms
  cleanup: 0.5ms, gpu_sync: 0.6ms
  measured: 1415.4ms, unaccounted: -138.0ms (-10.8%)
iter 1860: loss 3.3218, time 1295.53ms, mfu 3.96%
  data: 218.5ms, grad_accum: 1253.5ms (fw: 12.8ms, bw: 172.6ms)
  grad_proc: 17.1ms, optimizer: 1.4ms, param_check: 19.9ms
  loss_proc: 0.0ms, instability: 71.2ms
  cleanup: 0.3ms, gpu_sync: 0.7ms
  measured: 1292.8ms, unaccounted: 2.7ms (0.2%)
iter 1880: loss 3.3652, time 1502.96ms, mfu 3.95%
  data: 227.6ms, grad_accum: 1358.3ms (fw: 20.5ms, bw: 176.7ms)
  grad_proc: 17.2ms, optimizer: 1.8ms, param_check: 19.6ms
  loss_proc: 0.0ms, instability: 76.9ms
  cleanup: 0.5ms, gpu_sync: 0.6ms
  measured: 1398.0ms, unaccounted: 104.9ms (7.0%)
iter 1900: loss 3.2616, time 1509.74ms, mfu 3.94%
  data: 261.9ms, grad_accum: 1591.5ms (fw: 17.0ms, bw: 196.8ms)
  grad_proc: 18.7ms, optimizer: 2.0ms, param_check: 22.9ms
  loss_proc: 0.0ms, instability: 81.2ms
  cleanup: 0.3ms, gpu_sync: 0.6ms
  measured: 1635.9ms, unaccounted: -126.2ms (-8.4%)
iter 1920: loss 3.3905, time 1969.15ms, mfu 3.84%
  data: 354.7ms, grad_accum: 1655.5ms (fw: 14.2ms, bw: 259.8ms)
  grad_proc: 22.0ms, optimizer: 2.7ms, param_check: 35.7ms
  loss_proc: 0.0ms, instability: 101.0ms
  cleanup: 0.4ms, gpu_sync: 0.5ms
  measured: 1716.9ms, unaccounted: 252.3ms (12.8%)
iter 1940: loss 3.2932, time 1330.00ms, mfu 3.90%
  data: 282.2ms, grad_accum: 1448.6ms (fw: 14.0ms, bw: 218.8ms)
  grad_proc: 19.5ms, optimizer: 1.7ms, param_check: 26.6ms
  loss_proc: 0.0ms, instability: 71.7ms
  cleanup: 0.3ms, gpu_sync: 0.6ms
  measured: 1497.3ms, unaccounted: -167.3ms (-12.6%)
iter 1960: loss 3.4332, time 1385.13ms, mfu 3.93%
  data: 231.9ms, grad_accum: 1456.9ms (fw: 12.8ms, bw: 185.3ms)
  grad_proc: 19.5ms, optimizer: 1.9ms, param_check: 21.0ms
  loss_proc: 0.0ms, instability: 78.1ms
  cleanup: 0.4ms, gpu_sync: 0.6ms
  measured: 1500.2ms, unaccounted: -115.0ms (-8.3%)
iter 1980: loss 3.3306, time 1418.92ms, mfu 3.94%
  data: 251.2ms, grad_accum: 1511.9ms (fw: 31.1ms, bw: 183.5ms)
  grad_proc: 20.5ms, optimizer: 1.9ms, param_check: 24.0ms
  loss_proc: 0.0ms, instability: 58.5ms
  cleanup: 0.6ms, gpu_sync: 0.6ms
  measured: 1559.4ms, unaccounted: -140.5ms (-9.9%)

--- Starting validation at iteration 2000 ---
Using validation set with samples from all 14 stages
  Validation complete: 20 batches processed (320 samples), avg loss = 3.0982
  Per-stage validation losses:
    Stage 0 (sticky): 2.9029 (23 samples) - ratio=0.4
    Stage 1 (sticky): 3.1697 (16 samples) - ratio=0.6
    Stage 2 (random): 3.2179 (23 samples) - max_ratio=0.5
    Stage 3 (sticky): 3.3027 (7 samples) - ratio=0.6
    Stage 4 (sticky): 2.7618 (16 samples) - ratio=0.6
    Stage 5 (random): 2.9788 (23 samples) - max_ratio=0.2
    Stage 6 (sticky): 3.1102 (7 samples) - ratio=0.2
    Stage 7 (sticky): 4.0852 (16 samples) - ratio=0.4
    Stage 8 (sticky): 3.3644 (23 samples) - ratio=0.4
    Stage 9 (sticky): 3.2408 (7 samples) - ratio=0.6
    Stage 10 (sticky): 2.5431 (23 samples) - ratio=0.7
    Stage 11 (sticky): 2.9104 (16 samples) - ratio=0.8
    Stage 12 (sticky): 3.0690 (7 samples) - ratio=0.8
    Stage 13 (sticky): 3.1503 (23 samples) - ratio=0.9
--- Validation complete ---
step 2000: train loss 3.2231, val loss 3.0982, lr 0.001000
Stage 0 (sticky): target_ratio=0.4, p1=0.1, p2=0.3, stale_count=0
  val model vs random: 27.78x better
  val avg correct prob: 0.3473 (random: 0.0125)
  val signal to noise: 13.11 (median: 0.77)
  Most likely guess correct P %: 45.6%
  Stage 0: New best val loss 3.0982, reset stale count to 0

saving checkpoint to out/ckpt_unmasking_2000.pt
iter 2000: loss 3.4034, time 6576.59ms, mfu 3.64%
  data: 241.8ms, grad_accum: 1606.5ms (fw: 38.4ms, bw: 184.9ms)
  grad_proc: 19.5ms, optimizer: 1.7ms, param_check: 19.9ms
  loss_proc: 0.0ms, instability: 64.7ms
  cleanup: 0.3ms, gpu_sync: 0.4ms
  measured: 1648.4ms, unaccounted: 4928.2ms (74.9%)
  validation: 5962.31ms (data: 22.52ms, forward: 0.00ms, loss: 71.95ms)
Masking: stage=0 (sticky), actual_ratio=0.505, target=0.4, p1=0.1, p2=0.3
iter 2020: loss 3.3552, time 1397.34ms, mfu 3.69%
  data: 247.6ms, grad_accum: 1404.3ms (fw: 13.3ms, bw: 194.8