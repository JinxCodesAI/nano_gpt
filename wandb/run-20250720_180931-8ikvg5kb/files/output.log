Calculating target architecture based on schedule...

============================================================
       TARGET MODEL ARCHITECTURE (at end of schedule)
============================================================
  n_layer                | 6
  n_head                 | 12
  n_embd                 | 768
  n_hidden               | 1536
  block_size             | 1024
  vocab_size             | 50304
  dropout                | 0.0
  bias                   | False
  embedding_mode         | standard
  attn_lora_rank         | 0
  embedding_rank         | 0
  lora_alpha             | 0.0
============================================================


============================================================
           INITIAL MODEL ARCHITECTURE (at Iter 0)
============================================================
  n_layer                | 3
  n_head                 | 12
  n_embd                 | 768
  n_hidden               | 768
  block_size             | 1024
  vocab_size             | 50304
  dropout                | 0.0
  bias                   | False
  embedding_mode         | lora
  attn_lora_rank         | 48
  embedding_rank         | 48
  lora_alpha             | 1.0
============================================================


Detailed parameter count:
  total                  | Total:   52,149,504 | Trainable:    8,207,616
  token_embeddings       | Total:    2,451,456 | Trainable:    2,451,456
  position_embeddings    | Total:            0 | Trainable:            0
  attention_layers       | Total:    7,520,256 | Trainable:    2,211,840
  feed_forward_layers    | Total:    3,538,944 | Trainable:    3,538,944
  layer_norms            | Total:        4,608 | Trainable:        4,608
  final_layer_norm       | Total:          768 | Trainable:          768
  language_model_head    | Total:   38,633,472 | Trainable:            0
------------------------------------------------------------
eval every:200
step 1800: train loss 7.5807, val loss 7.5327
  MLP Rank Utilization (L0): 59.77% (459/768)
Attn LoRA    | 6/48            | 12.50%
  MLP Rank Utilization (L1): 50.78% (390/768)
Attn LoRA    | 3/48            | 6.25%
  MLP Rank Utilization (L2): 49.74% (382/768)
Attn LoRA    | 4/48            | 8.33%
--- Model Analysis ---
  Embedding Utilization (L2): 97.53% (749/768)
Embed LoRA   | 4/48            | 8.33%
  Average Attention Entropy:  4.4033
----------------------
saving checkpoint to out
merge_lora_weights 7.532706260681152 4.0 10500
iter 1800: loss 7.4616, lr 0.00050, time 10126.06ms, mfu -100.00%
iter 1810: loss 7.5561, lr 0.00050, time 291.05ms, mfu 31.75%
iter 1820: loss 7.5642, lr 0.00050, time 294.40ms, mfu 31.71%
iter 1830: loss 7.6324, lr 0.00050, time 290.76ms, mfu 31.72%
iter 1840: loss 7.5989, lr 0.00050, time 282.16ms, mfu 31.82%
iter 1850: loss 7.3693, lr 0.00050, time 290.41ms, mfu 31.82%
iter 1860: loss 7.4993, lr 0.00050, time 289.31ms, mfu 31.83%
iter 1870: loss 7.5815, lr 0.00050, time 293.19ms, mfu 31.80%
iter 1880: loss 7.4452, lr 0.00050, time 295.49ms, mfu 31.75%
iter 1890: loss 7.4830, lr 0.00050, time 294.89ms, mfu 31.71%
iter 1900: loss 7.6390, lr 0.00050, time 283.43ms, mfu 31.80%
iter 1910: loss 7.5295, lr 0.00050, time 287.33ms, mfu 31.83%
iter 1920: loss 7.3033, lr 0.00050, time 290.58ms, mfu 31.83%
iter 1930: loss 7.3984, lr 0.00050, time 288.18ms, mfu 31.85%
iter 1940: loss 7.4538, lr 0.00050, time 290.89ms, mfu 31.84%
iter 1950: loss 7.7009, lr 0.00050, time 291.76ms, mfu 31.83%
iter 1960: loss 7.5049, lr 0.00050, time 288.51ms, mfu 31.85%
iter 1970: loss 7.4165, lr 0.00050, time 289.86ms, mfu 31.85%
iter 1980: loss 7.6789, lr 0.00050, time 289.41ms, mfu 31.86%
iter 1990: loss 7.4120, lr 0.00050, time 287.25ms, mfu 31.89%
step 2000: train loss 7.5232, val loss 7.5349
  MLP Rank Utilization (L0): 59.77% (459/768)
Attn LoRA    | 6/48            | 12.50%
  MLP Rank Utilization (L1): 51.04% (392/768)
Attn LoRA    | 4/48            | 8.33%
  MLP Rank Utilization (L2): 49.87% (383/768)
Attn LoRA    | 4/48            | 8.33%
--- Model Analysis ---
  Embedding Utilization (L2): 97.53% (749/768)
Embed LoRA   | 5/48            | 10.42%
  Average Attention Entropy:  4.4081
----------------------
merge_lora_weights 7.534876346588135 4.0 10500
iter 2000: loss 7.3956, lr 0.00050, time 4925.56ms, mfu 28.89%
iter 2010: loss 7.4767, lr 0.00050, time 298.94ms, mfu 29.09%
iter 2020: loss 7.6695, lr 0.00050, time 289.67ms, mfu 29.37%
iter 2030: loss 7.2398, lr 0.00050, time 289.24ms, mfu 29.63%
iter 2040: loss 7.5558, lr 0.00050, time 288.45ms, mfu 29.87%
iter 2050: loss 7.5129, lr 0.00050, time 284.50ms, mfu 30.13%
iter 2060: loss 7.5050, lr 0.00050, time 286.80ms, mfu 30.34%
iter 2070: loss 7.4292, lr 0.00050, time 288.93ms, mfu 30.50%
iter 2080: loss 7.5676, lr 0.00050, time 291.61ms, mfu 30.62%
iter 2090: loss 7.4099, lr 0.00050, time 284.40ms, mfu 30.81%
iter 2100: loss 7.4650, lr 0.00050, time 289.58ms, mfu 30.92%
iter 2110: loss 7.5500, lr 0.00050, time 291.93ms, mfu 30.99%
iter 2120: loss 7.4661, lr 0.00050, time 287.68ms, mfu 31.10%
iter 2130: loss 7.6338, lr 0.00050, time 290.04ms, mfu 31.18%
iter 2140: loss 7.4798, lr 0.00050, time 287.70ms, mfu 31.27%
iter 2150: loss 7.5051, lr 0.00050, time 289.15ms, mfu 31.34%
iter 2160: loss 7.5127, lr 0.00050, time 290.29ms, mfu 31.39%
iter 2170: loss 7.2814, lr 0.00050, time 291.03ms, mfu 31.43%
iter 2180: loss 7.3488, lr 0.00050, time 287.94ms, mfu 31.49%
iter 2190: loss 7.5917, lr 0.00050, time 287.50ms, mfu 31.56%
step 2200: train loss 7.4303, val loss 7.4612
  MLP Rank Utilization (L0): 59.77% (459/768)
Attn LoRA    | 6/48            | 12.50%
  MLP Rank Utilization (L1): 51.17% (393/768)
Attn LoRA    | 4/48            | 8.33%
  MLP Rank Utilization (L2): 50.13% (385/768)
Attn LoRA    | 4/48            | 8.33%
--- Model Analysis ---
  Embedding Utilization (L2): 97.53% (749/768)
Embed LoRA   | 5/48            | 10.42%
  Average Attention Entropy:  4.3470
----------------------
saving checkpoint to out
merge_lora_weights 7.461249351501465 4.0 10500
iter 2200: loss 7.4719, lr 0.00050, time 5994.85ms, mfu 28.56%
iter 2210: loss 7.4445, lr 0.00050, time 283.09ms, mfu 28.96%
iter 2220: loss 7.3962, lr 0.00050, time 291.19ms, mfu 29.24%
iter 2230: loss 7.4395, lr 0.00050, time 293.36ms, mfu 29.47%
iter 2240: loss 7.3852, lr 0.00050, time 292.35ms, mfu 29.68%
iter 2250: loss 7.3816, lr 0.00050, time 289.90ms, mfu 29.90%
iter 2260: loss 7.3095, lr 0.00050, time 289.93ms, mfu 30.10%
iter 2270: loss 7.3364, lr 0.00050, time 289.08ms, mfu 30.28%
iter 2280: loss 7.4113, lr 0.00050, time 289.29ms, mfu 30.45%
iter 2290: loss 7.3620, lr 0.00050, time 290.80ms, mfu 30.58%
iter 2300: loss 7.4219, lr 0.00050, time 292.09ms, mfu 30.69%
iter 2310: loss 7.2983, lr 0.00050, time 288.51ms, mfu 30.82%
iter 2320: loss 7.6515, lr 0.00050, time 287.42ms, mfu 30.95%
iter 2330: loss 7.4412, lr 0.00050, time 290.02ms, mfu 31.05%
iter 2340: loss 7.3575, lr 0.00050, time 289.20ms, mfu 31.14%
iter 2350: loss 7.3300, lr 0.00050, time 291.10ms, mfu 31.20%
iter 2360: loss 7.3849, lr 0.00050, time 288.23ms, mfu 31.28%
iter 2370: loss 7.4217, lr 0.00050, time 289.70ms, mfu 31.34%
iter 2380: loss 7.2879, lr 0.00050, time 289.15ms, mfu 31.41%
iter 2390: loss 7.4294, lr 0.00050, time 291.63ms, mfu 31.43%
step 2400: train loss 7.3983, val loss 7.4042
  MLP Rank Utilization (L0): 59.90% (460/768)
Attn LoRA    | 6/48            | 12.50%
  MLP Rank Utilization (L1): 51.30% (394/768)
Attn LoRA    | 4/48            | 8.33%
  MLP Rank Utilization (L2): 50.26% (386/768)
Attn LoRA    | 4/48            | 8.33%
--- Model Analysis ---
  Embedding Utilization (L2): 97.53% (749/768)
Embed LoRA   | 5/48            | 10.42%
  Average Attention Entropy:  4.3230
----------------------
saving checkpoint to out
merge_lora_weights 7.404179573059082 4.0 10500
iter 2400: loss 7.4365, lr 0.00050, time 6003.63ms, mfu 28.44%
iter 2410: loss 7.4701, lr 0.00050, time 283.07ms, mfu 28.86%
iter 2420: loss 7.2564, lr 0.00050, time 287.05ms, mfu 29.20%
iter 2430: loss 7.3681, lr 0.00050, time 292.78ms, mfu 29.43%
iter 2440: loss 7.4472, lr 0.00050, time 294.31ms, mfu 29.63%
iter 2450: loss 7.3548, lr 0.00050, time 289.98ms, mfu 29.85%
iter 2460: loss 7.3978, lr 0.00050, time 289.76ms, mfu 30.06%
iter 2470: loss 7.5031, lr 0.00050, time 289.97ms, mfu 30.24%
iter 2480: loss 7.5408, lr 0.00050, time 286.21ms, mfu 30.44%
iter 2490: loss 7.2210, lr 0.00050, time 287.86ms, mfu 30.61%
iter 2500: loss 7.3351, lr 0.00050, time 284.88ms, mfu 30.79%
iter 2510: loss 7.2765, lr 0.00050, time 289.12ms, mfu 30.91%
iter 2520: loss 7.3436, lr 0.00050, time 292.05ms, mfu 30.98%
iter 2530: loss 7.3846, lr 0.00050, time 285.92ms, mfu 31.11%
iter 2540: loss 7.3855, lr 0.00050, time 287.33ms, mfu 31.22%
iter 2550: loss 7.3159, lr 0.00050, time 287.44ms, mfu 31.31%
iter 2560: loss 7.4263, lr 0.00050, time 289.16ms, mfu 31.38%
iter 2570: loss 7.4116, lr 0.00050, time 287.66ms, mfu 31.45%
iter 2580: loss 7.3428, lr 0.00050, time 288.49ms, mfu 31.51%
iter 2590: loss 7.3745, lr 0.00050, time 290.09ms, mfu 31.54%
step 2600: train loss 7.3620, val loss 7.2924
  MLP Rank Utilization (L0): 59.90% (460/768)
Attn LoRA    | 6/48            | 12.50%
  MLP Rank Utilization (L1): 51.43% (395/768)
Attn LoRA    | 4/48            | 8.33%
  MLP Rank Utilization (L2): 50.52% (388/768)
Attn LoRA    | 4/48            | 8.33%
--- Model Analysis ---
  Embedding Utilization (L2): 97.53% (749/768)
Embed LoRA   | 5/48            | 10.42%
  Average Attention Entropy:  4.3214
----------------------
saving checkpoint to out
merge_lora_weights 7.292366027832031 4.0 10500
iter 2600: loss 7.2915, lr 0.00050, time 5985.78ms, mfu 28.54%
iter 2610: loss 7.2531, lr 0.00050, time 282.68ms, mfu 28.96%
iter 2620: loss 7.3506, lr 0.00050, time 288.03ms, mfu 29.27%
iter 2630: loss 7.3388, lr 0.00050, time 288.13ms, mfu 29.55%
iter 2640: loss 7.2493, lr 0.00050, time 289.82ms, mfu 29.78%
iter 2650: loss 7.2585, lr 0.00050, time 293.76ms, mfu 29.95%
iter 2660: loss 7.3984, lr 0.00050, time 287.21ms, mfu 30.17%
iter 2670: loss 7.3983, lr 0.00050, time 292.93ms, mfu 30.31%
iter 2680: loss 7.2477, lr 0.00050, time 289.33ms, mfu 30.47%
iter 2690: loss 7.2967, lr 0.00050, time 292.12ms, mfu 30.59%
iter 2700: loss 7.1952, lr 0.00050, time 288.30ms, mfu 30.73%
iter 2710: loss 7.2722, lr 0.00050, time 291.90ms, mfu 30.83%
iter 2720: loss 7.2032, lr 0.00050, time 289.09ms, mfu 30.94%
iter 2730: loss 7.0867, lr 0.00050, time 291.95ms, mfu 31.01%
iter 2740: loss 7.3452, lr 0.00050, time 289.93ms, mfu 31.10%
iter 2750: loss 7.3487, lr 0.00050, time 292.93ms, mfu 31.14%
iter 2760: loss 7.2699, lr 0.00050, time 288.94ms, mfu 31.23%
iter 2770: loss 7.2398, lr 0.00050, time 292.86ms, mfu 31.26%
iter 2780: loss 7.4118, lr 0.00050, time 287.57ms, mfu 31.35%
iter 2790: loss 7.2553, lr 0.00050, time 291.81ms, mfu 31.38%
step 2800: train loss 7.3107, val loss 7.2936
  MLP Rank Utilization (L0): 60.03% (461/768)
Attn LoRA    | 6/48            | 12.50%
  MLP Rank Utilization (L1): 51.69% (397/768)
Attn LoRA    | 4/48            | 8.33%
  MLP Rank Utilization (L2): 50.65% (389/768)
Attn LoRA    | 4/48            | 8.33%
--- Model Analysis ---
  Embedding Utilization (L2): 97.53% (749/768)
Embed LoRA   | 5/48            | 10.42%
  Average Attention Entropy:  4.3259
----------------------
merge_lora_weights 7.2936201095581055 4.0 10500
iter 2800: loss 7.3275, lr 0.00050, time 4873.80ms, mfu 28.43%
iter 2810: loss 7.1251, lr 0.00050, time 296.48ms, mfu 28.70%
iter 2820: loss 7.4727, lr 0.00050, time 289.95ms, mfu 29.02%
iter 2830: loss 7.3663, lr 0.00050, time 289.84ms, mfu 29.31%
iter 2840: loss 7.3183, lr 0.00050, time 285.70ms, mfu 29.61%
iter 2850: loss 7.1733, lr 0.00050, time 289.49ms, mfu 29.84%
iter 2860: loss 7.1786, lr 0.00050, time 288.21ms, mfu 30.06%
iter 2870: loss 7.2961, lr 0.00050, time 288.69ms, mfu 30.26%
iter 2880: loss 7.3305, lr 0.00050, time 284.53ms, mfu 30.48%
iter 2890: loss 7.3064, lr 0.00050, time 287.34ms, mfu 30.65%
iter 2900: loss 7.2626, lr 0.00050, time 290.41ms, mfu 30.76%
iter 2910: loss 7.2965, lr 0.00050, time 292.85ms, mfu 30.84%
iter 2920: loss 7.3725, lr 0.00050, time 292.71ms, mfu 30.92%
iter 2930: loss 7.4161, lr 0.00050, time 289.56ms, mfu 31.01%
iter 2940: loss 7.3704, lr 0.00050, time 290.75ms, mfu 31.09%
iter 2950: loss 7.3264, lr 0.00050, time 288.04ms, mfu 31.19%
iter 2960: loss 7.2921, lr 0.00050, time 288.58ms, mfu 31.27%
iter 2970: loss 7.2189, lr 0.00050, time 288.03ms, mfu 31.35%
iter 2980: loss 7.2795, lr 0.00050, time 286.84ms, mfu 31.44%
iter 2990: loss 7.1714, lr 0.00050, time 287.92ms, mfu 31.51%
step 3000: train loss 7.3040, val loss 7.2712
  MLP Rank Utilization (L0): 60.03% (461/768)
Attn LoRA    | 6/48            | 12.50%
  MLP Rank Utilization (L1): 51.82% (398/768)
Attn LoRA    | 4/48            | 8.33%
  MLP Rank Utilization (L2): 50.78% (390/768)
Attn LoRA    | 4/48            | 8.33%
--- Model Analysis ---
  Embedding Utilization (L2): 97.53% (749/768)
Embed LoRA   | 5/48            | 10.42%
  Average Attention Entropy:  4.3225
----------------------
saving checkpoint to out
merge_lora_weights 7.2711639404296875 4.0 10500
iter 3000: loss 7.2739, lr 0.00050, time 6055.20ms, mfu 28.51%
iter 3010: loss 7.2318, lr 0.00050, time 282.36ms, mfu 28.93%
iter 3020: loss 7.2639, lr 0.00050, time 285.51ms, mfu 29.27%
iter 3030: loss 7.1985, lr 0.00050, time 287.09ms, mfu 29.56%
iter 3040: loss 7.3248, lr 0.00050, time 290.14ms, mfu 29.79%
iter 3050: loss 7.2008, lr 0.00050, time 292.22ms, mfu 29.98%
iter 3060: loss 7.4478, lr 0.00050, time 286.62ms, mfu 30.20%
iter 3070: loss 7.1249, lr 0.00050, time 290.33ms, mfu 30.36%
