# Reward Model Training Infrastructure

This document provides comprehensive documentation for the reward model training infrastructure used in GRPO (Group Reward Policy Optimization). The infrastructure enables training reward models that can distinguish between natural and synthetic text by learning to predict the proportion of tokens that came from the original dataset versus those generated by a base model.

## Table of Contents

1. [Overview](#overview)
2. [Architecture](#architecture)
3. [Data Preparation](#data-preparation)
4. [Dataset Loading](#dataset-loading)
5. [Training Integration](#training-integration)
6. [Command-Line Usage](#command-line-usage)
7. [Step-by-Step Guide](#step-by-step-guide)
8. [Troubleshooting](#troubleshooting)
9. [Best Practices](#best-practices)

## Overview

The reward model training infrastructure consists of three main components:

1. **Data Preparation Script** (`prepare_reward_data.py`) - Generates mixed natural/synthetic datasets
2. **Dataset Loader** (`reward_dataset_loader.py`) - PyTorch utilities for loading reward training data
3. **Dual-Mode GPT Architecture** - Configurable model that can operate as either generator or reward model

### Key Features

- **Mixed Dataset Generation**: Creates sequences with random crossover points between natural and synthetic text
- **Probability Target Labels**: Generates precise probability distributions [K/N, (N-K)/N] for training
- **Data Split Alignment**: Maintains perfect alignment with base model's train/validation splits
- **Configurable Architecture**: Supports configurable reward head hidden dimensions
- **Efficient Storage**: Binary file format with metadata for fast loading
- **PyTorch Integration**: Standard Dataset and DataLoader compatibility
- **ðŸ†• Configurable Tokenization**: Support for both BPE and character-level tokenization with auto-detection
- **ðŸ†• Flexible Input Modes**: Text mode (raw files) and binary mode (preprocessed files) for optimal performance
- **ðŸ†• Tokenization Compatibility**: Automatic validation of tokenization consistency across datasets
- **ðŸ†• Enhanced Error Handling**: Comprehensive validation and user-friendly error messages

## Architecture

### Dual-Mode GPT Configuration

The GPT model supports two operational modes through the `GPTConfig` class:

```python
@dataclass
class GPTConfig:
    # Standard GPT parameters
    block_size: int = 1024
    vocab_size: int = 50304
    n_layer: int = 12
    n_head: int = 12
    n_embd: int = 768
    dropout: float = 0.0
    bias: bool = True
    
    # Dual-mode parameters
    mode: str = 'generator'  # 'generator' or 'reward'
    reward_head_hidden_dim: int = 256  # Configurable reward head size
```

### Shared Architecture

Both modes share the same transformer "trunk":
- Token embeddings (`wte`)
- Dropout layer (`drop`)
- Transformer blocks (`h`)
- Final layer normalization (`ln_f`)

### Mode-Specific Heads

**Generator Mode:**
- `lm_head`: Linear layer for next-token prediction
- Tied weights with token embeddings

**Reward Mode:**
- `reward_head`: MLP with configurable hidden dimension
  - Input: `n_embd` â†’ Hidden: `reward_head_hidden_dim` â†’ Output: 2
  - ReLU activation between layers
  - Softmax output for probability distribution

## Data Preparation

### Algorithm Overview

The data preparation process creates training samples by mixing natural and synthetic text with configurable tokenization support:

1. **Tokenization Setup**: Auto-detect or configure tokenization method (BPE or character-level)
2. **Data Loading**: Load from raw text files or reuse existing binary files for performance
3. **Load Base Model**: Pre-trained model in generator mode for text generation
4. **Data Splitting**: Split raw text using same ratio as base model (prevents contamination)
5. **Sample Generation**: For each sequence:
   - Choose random crossover point K âˆˆ [1, block_size-1]
   - Take first K tokens from natural data
   - Generate remaining (block_size - K) tokens using base model
   - Create target labels [K/block_size, (block_size-K)/block_size]
6. **Binary Serialization**: Save sequences and labels to efficient binary format with tokenization metadata

### Data Format

**Input Sequences (X files):**
- Format: Binary uint16 arrays
- Shape: (num_samples, block_size)
- Content: Token IDs from mixed sequences

**Target Labels (Y files):**
- Format: Binary float32 arrays
- Shape: (num_samples, 2)
- Content: [P(natural), P(synthetic)] probability pairs

**Metadata Files:**
- Format: Plain text key-value pairs
- Content: Dataset dimensions, types, statistics, and tokenization information

### Directory Structure

```
data/reward_dataset/
â”œâ”€â”€ train_x.bin          # Training input sequences
â”œâ”€â”€ train_y.bin          # Training target probabilities
â”œâ”€â”€ train_metadata.txt   # Training set metadata
â”œâ”€â”€ val_x.bin           # Validation input sequences
â”œâ”€â”€ val_y.bin           # Validation target probabilities
â””â”€â”€ val_metadata.txt    # Validation set metadata
```

## Dataset Loading

### RewardDataset Class

The `RewardDataset` class provides PyTorch-compatible dataset functionality with tokenization validation:

```python
from reward_dataset_loader import RewardDataset
from reward_data_config import TokenizationInfo

# Load dataset with tokenization validation
expected_tokenization = TokenizationInfo(method='char', vocab_size=65, meta_path='data/shakespeare_char/meta.pkl')
dataset = RewardDataset('data/reward_dataset', split='train', expected_tokenization_info=expected_tokenization)

# Access samples
x, y = dataset[0]  # x: LongTensor, y: FloatTensor

# Get statistics including tokenization info
stats = dataset.get_stats()
print(f"Dataset size: {len(dataset)}")
print(f"Tokenization: {stats['tokenization_method']} (vocab_size={stats['vocab_size']})")
print(f"Probability sums: {stats['prob_sum_mean']:.6f}")
```

### DataLoader Creation

```python
from reward_dataset_loader import create_reward_dataloaders
from reward_data_config import TokenizationInfo

# Create train and validation loaders with tokenization validation
expected_tokenization = TokenizationInfo(method='bpe', vocab_size=50257)
train_loader, val_loader = create_reward_dataloaders(
    data_dir='data/reward_dataset',
    batch_size=32,
    num_workers=4,
    expected_tokenization_info=expected_tokenization
)

# Use in training loop
for batch_x, batch_y in train_loader:
    # batch_x: (batch_size, block_size) LongTensor
    # batch_y: (batch_size, 2) FloatTensor
    pass
```

## Training Integration

### Reward Model Training Example

```python
import torch
import torch.nn.functional as F
from model import GPT, GPTConfig
from reward_dataset_loader import create_reward_dataloaders

# Load base model checkpoint
checkpoint = torch.load('checkpoints/base_model.pt')
config = checkpoint['config']
config.mode = 'reward'  # Switch to reward mode

# Create reward model
model = GPT(config)
model.load_state_dict(checkpoint['model'], strict=False)

# Freeze transformer trunk (optional but recommended)
for param in model.transformer.parameters():
    param.requires_grad = False

# Create data loaders
train_loader, val_loader = create_reward_dataloaders('data/reward_dataset')

# Training loop
optimizer = torch.optim.Adam(model.reward_head.parameters(), lr=1e-4)

for epoch in range(num_epochs):
    model.train()
    for batch_x, batch_y in train_loader:
        batch_x, batch_y = batch_x.to(device), batch_y.to(device)
        
        # Forward pass
        probs, _ = model(batch_x)
        
        # Calculate loss (MSE between predicted and target probabilities)
        loss = F.mse_loss(probs, batch_y)
        
        # Backward pass
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
```

## Command-Line Usage

### prepare_reward_data.py

Generate reward model training datasets from any base model and text corpus with configurable tokenization support.

#### Basic Usage

```bash
# Text mode with BPE tokenization (default, backward compatible)
python prepare_reward_data.py \
    --model_path checkpoints/base_model.pt \
    --data_path data/shakespeare/input.txt \
    --output_dir data/reward_dataset

# Text mode with character tokenization
python prepare_reward_data.py \
    --model_path checkpoints/char_model.pt \
    --input_mode text \
    --data_path data/shakespeare_char/input.txt \
    --tokenization char \
    --meta_path data/shakespeare_char/meta.pkl

# Binary mode (reuse existing preprocessed files for performance)
python prepare_reward_data.py \
    --model_path checkpoints/base_model.pt \
    --input_mode binary \
    --train_bin data/shakespeare/train.bin \
    --val_bin data/shakespeare/val.bin
```

#### Full Parameter Reference

```bash
python prepare_reward_data.py \
    # Required parameters
    --model_path PATH           # Path to pre-trained base model checkpoint (required)

    # Input mode parameters
    --input_mode {text,binary}  # Input mode: text (raw files) or binary (preprocessed) (default: text)
    --data_path PATH            # Path to raw text data file (text mode only)
    --train_bin PATH            # Path to existing train.bin file (binary mode only)
    --val_bin PATH              # Path to existing val.bin file (binary mode only)

    # Tokenization configuration
    --tokenization {auto,bpe,char}  # Tokenization method (default: auto)
    --meta_path PATH            # Path to meta.pkl file for character tokenization

    # Generation parameters (existing)
    --output_dir PATH           # Output directory for reward dataset (default: data/reward_dataset)
    --train_split FLOAT         # Train/validation split ratio (default: 0.9)
    --samples_per_chunk INT     # Samples per data chunk (default: 10)
    --temperature FLOAT         # Temperature for text generation (default: 1.0)
    --top_k INT                 # Top-k sampling for generation (default: None)
    --device STRING             # Device to use: auto/cpu/cuda (default: auto)
    --seed INT                  # Random seed for reproducibility (default: 42)
```

#### Parameter Details

**Required Parameters:**
- `--model_path`: Path to the base model checkpoint (.pt file) used for generating synthetic text

**Input Mode Parameters:**
- `--input_mode`: Choose between 'text' (raw text files) or 'binary' (preprocessed files)
- `--data_path`: Raw text file to use as source of natural text (text mode only)
- `--train_bin`: Path to existing train.bin file (binary mode only)
- `--val_bin`: Path to existing val.bin file (binary mode only)

**Tokenization Parameters:**
- `--tokenization`: Method for tokenization ('auto', 'bpe', 'char')
  - 'auto': Automatically detect based on file structure
  - 'bpe': Use tiktoken GPT-2 BPE encoding
  - 'char': Use character-level tokenization from meta.pkl
- `--meta_path`: Path to meta.pkl file (required for character tokenization)

**Data Parameters:**
- `--output_dir`: Directory where binary dataset files will be saved
- `--train_split`: Fraction of data used for training (must match base model's split)

**Generation Parameters:**
- `--samples_per_chunk`: Number of reward samples to generate per data block
- `--temperature`: Controls randomness in text generation (higher = more random)
- `--top_k`: Limits sampling to top-k most likely tokens (None = no limit)

**System Parameters:**
- `--device`: Computation device (auto-detects CUDA if available)
- `--seed`: Random seed for reproducible dataset generation

#### Example Commands

**Basic dataset generation (BPE tokenization):**
```bash
python prepare_reward_data.py \
    --model_path checkpoints/shakespeare_model.pt \
    --data_path data/shakespeare/input.txt
```

**Character tokenization with explicit configuration:**
```bash
python prepare_reward_data.py \
    --model_path checkpoints/char_model.pt \
    --input_mode text \
    --data_path data/shakespeare_char/input.txt \
    --tokenization char \
    --meta_path data/shakespeare_char/meta.pkl
```

**Binary mode for faster processing:**
```bash
python prepare_reward_data.py \
    --model_path checkpoints/base_model.pt \
    --input_mode binary \
    --train_bin data/shakespeare/train.bin \
    --val_bin data/shakespeare/val.bin \
    --output_dir data/reward_dataset_fast
```

**Auto-detection with high-quality settings:**
```bash
python prepare_reward_data.py \
    --model_path checkpoints/base_model.pt \
    --data_path data/shakespeare_char \
    --tokenization auto \
    --output_dir data/reward_dataset_v2 \
    --samples_per_chunk 50 \
    --temperature 0.8 \
    --top_k 40
```

**Reproducible generation:**
```bash
python prepare_reward_data.py \
    --model_path checkpoints/model.pt \
    --data_path data/corpus.txt \
    --seed 12345 \
    --device cuda
```

### reward_dataset_loader.py

Utility script for inspecting and testing reward datasets with tokenization compatibility validation.

#### Dataset Information

```bash
python reward_dataset_loader.py
```

This will:
- Display comprehensive dataset statistics
- Show probability distribution analysis
- Display tokenization information and compatibility
- Test DataLoader functionality
- Validate data integrity and tokenization consistency

## Step-by-Step Guide

### Step 1: Prepare Your Base Model

Ensure you have a trained base model checkpoint:

```bash
# Train your base model first (if not already done)
python train.py --config config/train_shakespeare.py
```

### Step 2: Generate Reward Dataset

Create the mixed natural/synthetic dataset:

```bash
python prepare_reward_data.py \
    --model_path logs/model_checkpoint.pt \
    --data_path data/shakespeare/input.txt \
    --output_dir data/reward_dataset \
    --samples_per_chunk 20 \
    --temperature 1.0
```

**Expected Output:**
```
Configuration validation passed
Using device: cuda
Loading base model from logs/model_checkpoint.pt
Loaded model with 10.65M parameters
Data loaded successfully:
  Tokenization: bpe
  Vocab size: 50257
  Train tokens: 900,000
  Val tokens: 100,000

Generating reward dataset with block_size=1024
Tokenization method: bpe
Vocab size: 50257
Samples per chunk: 20
Temperature: 1.0

=== Generating Training Samples ===
Processing blocks: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 878/878 [02:15<00:00, 6.48it/s]
Created 17,560 reward model samples

=== Generating Validation Samples ===
Processing blocks: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 97/97 [00:15<00:00, 6.32it/s]
Created 1,940 reward model samples

=== Saving to data/reward_dataset ===
Saved train data:
  X: data/reward_dataset/train_x.bin (17560, 1024)
  Y: data/reward_dataset/train_y.bin (17560, 2)
Saved val data:
  X: data/reward_dataset/val_x.bin (1940, 1024)
  Y: data/reward_dataset/val_y.bin (1940, 2)

=== Dataset Creation Complete ===
Training samples: 17,560
Validation samples: 1,940
Block size: 1024
Output directory: data/reward_dataset
Tokenization: bpe (vocab_size=50257)
```

### Step 3: Inspect Dataset Quality

Verify the generated dataset:

```bash
python reward_dataset_loader.py
```

**Expected Output:**
```
=== Reward Dataset Info: data/reward_dataset ===

TRAIN SET:
  Samples: 17,560
  Block size: 1024
  Tokenization: bpe (vocab_size=50257)
  Y statistics:
    Min: [0.0009766 0.0009766]
    Max: [0.999023 0.999023]
    Mean: [0.5002 0.4998]
    Std: [0.2887 0.2887]
  Probability sums:
    Min: 1.000000
    Max: 1.000000
    Mean: 1.000000

VAL SET:
  Samples: 1,940
  Block size: 1024
  Tokenization: bpe (vocab_size=50257)
  Y statistics:
    Min: [0.0009766 0.0009766]
    Max: [0.999023 0.999023]
    Mean: [0.4995 0.5005]
    Std: [0.2891 0.2891]
  Probability sums:
    Min: 1.000000
    Max: 1.000000
    Mean: 1.000000

âœ… Tokenization consistency: Both splits use bpe with vocab_size=50257

=== DataLoader Test ===
Train batches: 4,390
Val batches: 485
Batch X shape: torch.Size([4, 1024])
Batch Y shape: torch.Size([4, 2])
Batch Y sample: tensor([0.7324, 0.2676])
```

### Step 4: Train Reward Model

Create a training script for the reward model:

```python
# train_reward_model.py
import torch
import torch.nn.functional as F
from model import GPT, GPTConfig
from reward_dataset_loader import create_reward_dataloaders

def train_reward_model():
    device = 'cuda' if torch.cuda.is_available() else 'cpu'
    
    # Load base model and switch to reward mode
    checkpoint = torch.load('logs/model_checkpoint.pt', map_location=device)
    config = checkpoint['config']
    config.mode = 'reward'
    config.reward_head_hidden_dim = 256  # Configurable
    
    model = GPT(config)
    model.load_state_dict(checkpoint['model'], strict=False)
    model.to(device)
    
    # Freeze transformer trunk (recommended)
    for param in model.transformer.parameters():
        param.requires_grad = False
    
    # Create data loaders
    train_loader, val_loader = create_reward_dataloaders(
        'data/reward_dataset', 
        batch_size=32
    )
    
    # Setup training
    optimizer = torch.optim.Adam(model.reward_head.parameters(), lr=1e-4)
    
    # Training loop
    for epoch in range(10):
        model.train()
        total_loss = 0
        
        for batch_x, batch_y in train_loader:
            batch_x, batch_y = batch_x.to(device), batch_y.to(device)
            
            probs, _ = model(batch_x)
            loss = F.mse_loss(probs, batch_y)
            
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()
            
            total_loss += loss.item()
        
        # Validation
        model.eval()
        val_loss = 0
        with torch.no_grad():
            for batch_x, batch_y in val_loader:
                batch_x, batch_y = batch_x.to(device), batch_y.to(device)
                probs, _ = model(batch_x)
                val_loss += F.mse_loss(probs, batch_y).item()
        
        print(f"Epoch {epoch+1}: Train Loss: {total_loss/len(train_loader):.4f}, "
              f"Val Loss: {val_loss/len(val_loader):.4f}")
    
    # Save reward model
    torch.save({
        'model': model.state_dict(),
        'config': config,
    }, 'logs/reward_model.pt')

if __name__ == '__main__':
    train_reward_model()
```

### Step 5: Use in GRPO Training

Integrate the reward model into your GRPO training loop:

```python
# Load both models
base_model = GPT(base_config)  # generator mode
reward_model = GPT(reward_config)  # reward mode
reward_model.eval()  # Freeze for inference

# GRPO training loop
for batch in data_loader:
    # Generate completions with base model
    completions = base_model.generate(prompts)
    
    # Get reward scores
    with torch.no_grad():
        rewards, _ = reward_model(completions)
        reward_scores = rewards[:, 0]  # P(natural)
    
    # Update base model using policy gradient
    # ... GRPO/PPO update logic ...
```

## Troubleshooting

### Common Issues and Solutions

#### 1. Model Loading Errors

**Problem:** `RuntimeError: Error(s) in loading state_dict`

**Solution:** When switching between generator and reward modes, use `strict=False`:
```python
model.load_state_dict(checkpoint['model'], strict=False)
```

**Explanation:** The reward head parameters don't exist in generator checkpoints, so strict loading fails.

#### 2. Memory Issues

**Problem:** `CUDA out of memory` during dataset generation

**Solutions:**
- Reduce `--samples_per_chunk` parameter
- Use smaller batch sizes in DataLoader
- Process data in smaller chunks
- Use CPU if GPU memory is insufficient

```bash
# Reduce memory usage
python prepare_reward_data.py \
    --model_path model.pt \
    --samples_per_chunk 5 \
    --device cpu
```

#### 3. Generation Failures

**Problem:** Text generation produces empty or invalid sequences

**Solutions:**
- Check model is in evaluation mode: `model.eval()`
- Verify temperature and top_k parameters are reasonable
- Ensure model checkpoint is valid and trained

```python
# Debug generation
model.eval()
with torch.no_grad():
    output = model.generate(prompt, temperature=0.8, top_k=40)
```

#### 4. Probability Sum Validation

**Problem:** Target probabilities don't sum to 1.0

**Check:** Run dataset validation:
```python
from reward_dataset_loader import RewardDataset
dataset = RewardDataset('data/reward_dataset')
stats = dataset.get_stats()
print(f"Prob sum range: {stats['prob_sum_min']:.6f} - {stats['prob_sum_max']:.6f}")
```

**Expected:** All probability sums should be exactly 1.000000

#### 5. Data Split Contamination

**Problem:** Reward model performs too well, suggesting data leakage

**Solution:** Ensure train/val splits match exactly:
- Use same `--train_split` ratio as base model training
- Verify split indices are identical
- Check that validation data wasn't used in base model training

#### 6. Poor Reward Model Performance

**Problem:** Reward model can't distinguish natural from synthetic text

**Potential Causes and Solutions:**

**Base model too weak:**
- Train base model longer before generating reward data
- Use higher quality base model checkpoint

**Generation parameters:**
- Adjust temperature (try 0.8-1.2 range)
- Experiment with top_k values (20-100)
- Increase samples_per_chunk for more diverse data

**Architecture issues:**
- Try different reward_head_hidden_dim values (128, 256, 512)
- Experiment with different loss functions (MSE, cross-entropy)
- Consider unfreezing some transformer layers

#### 7. File Format Issues

**Problem:** Binary files corrupted or unreadable

**Solutions:**
- Check disk space during generation
- Verify file permissions in output directory
- Re-run data preparation with different output directory

**Validation:**
```python
import numpy as np
# Check file integrity
x_data = np.fromfile('data/reward_dataset/train_x.bin', dtype=np.uint16)
y_data = np.fromfile('data/reward_dataset/train_y.bin', dtype=np.float32)
print(f"X shape: {x_data.shape}, Y shape: {y_data.shape}")
```

### Performance Optimization

#### Dataset Generation Speed

**Slow generation:**
- Use GPU for model inference: `--device cuda`
- Increase batch size in generation (modify script)
- Use faster sampling methods (reduce top_k)

**Memory optimization:**
- Process data in smaller chunks
- Clear GPU cache between chunks: `torch.cuda.empty_cache()`

#### Training Speed

**Slow reward model training:**
- Freeze transformer trunk: `param.requires_grad = False`
- Use larger batch sizes if memory allows
- Enable mixed precision training
- Use multiple GPUs with DataParallel

#### Storage Optimization

**Large dataset files:**
- Use uint16 for token IDs (vs int32/int64)
- Use float32 for probabilities (vs float64)
- Compress datasets with standard tools if needed

## Best Practices

### Dataset Generation

1. **Reproducibility:**
   - Always set random seeds: `--seed 42`
   - Document generation parameters
   - Version control your datasets

2. **Quality Control:**
   - Inspect generated samples manually
   - Validate probability distributions
   - Check for degenerate cases (all natural or all synthetic)

3. **Data Splits:**
   - Maintain perfect alignment with base model splits
   - Never mix train/val data during generation
   - Use consistent split ratios across experiments

### Model Training

1. **Reward Model Architecture:**
   - Start with default hidden_dim=256
   - Experiment with 128, 512 for different model sizes
   - Consider model capacity vs. overfitting trade-off

2. **Training Strategy:**
   - Freeze transformer trunk initially
   - Use early stopping on validation loss
   - Monitor probability distribution statistics

3. **Hyperparameters:**
   - Learning rate: Start with 1e-4 for reward head
   - Batch size: 32-128 depending on GPU memory
   - Loss function: MSE works well, but try others

### Integration with GRPO

1. **Reward Signal:**
   - Use P(natural) as reward score
   - Consider reward scaling/normalization
   - Monitor reward distribution during training

2. **Model Management:**
   - Keep reward model frozen during GRPO
   - Save checkpoints at each cycle
   - Track model versions carefully

3. **Evaluation:**
   - Monitor base model perplexity on held-out data
   - Generate samples for human evaluation
   - Track reward scores over training

### Experimental Design

1. **Baseline Establishment:**
   - Train strong base model first
   - Establish baseline metrics before GRPO
   - Document all hyperparameters

2. **Iterative Improvement:**
   - Start with small-scale experiments
   - Gradually increase dataset size and model complexity
   - A/B test different configurations

3. **Monitoring:**
   - Log all metrics and hyperparameters
   - Save intermediate checkpoints
   - Monitor for signs of mode collapse or instability

### Computational Resources

1. **Hardware Requirements:**
   - GPU recommended for model inference
   - Sufficient storage for binary datasets
   - RAM for dataset loading (consider num_workers)

2. **Scaling Considerations:**
   - Larger models need more memory
   - More samples_per_chunk increases dataset size
   - Balance quality vs. computational cost

3. **Efficiency Tips:**
   - Use mixed precision when possible
   - Optimize DataLoader parameters
   - Consider distributed training for large models

This comprehensive infrastructure provides a solid foundation for implementing GRPO with self-supervised reward models. The modular design allows for experimentation with different architectures, datasets, and training strategies while maintaining reproducibility and efficiency.