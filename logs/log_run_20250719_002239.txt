================================================================================
Training run started at 2025-07-19 00:22:39
================================================================================
Configuration:
----------------------------------------
always_save_checkpoint: True
backend: nccl
batch_size: 12
beta1: 0.9
beta2: 0.95
bias: False
block_size: 1024
compile: True
dataset: fineweb10B
decay_lr: True
device: cuda
dropout: 0.0
dtype: bfloat16
eval_interval: 50
eval_iters: 1
eval_only: False
file_logging: True
grad_clip: 1.0
gradient_accumulation_steps: 1
init_from: scratch
learning_rate: 0.0006
log_dir: logs
log_interval: 10
lr_decay_iters: 600000
max_iters: 600000
min_lr: 6e-05
n_embd: 192
n_head: 12
n_layer: 12
out_dir: out
rotary_base: 10000.0
rotary_max_position_embeddings: 2048
use_rotary_embeddings: True
wandb_log: True
wandb_project: owt
wandb_run_name: gpt2-124M
warmup_iters: 2000
weight_decay: 0.1
----------------------------------------
[2025-07-19 00:22:47] step 0: train loss 10.8473, val loss 10.8523
[2025-07-19 00:22:54] step 50: train loss 10.4758, val loss 10.4786
[2025-07-19 00:22:58] step 100: train loss 10.1777, val loss 10.1525
[2025-07-19 00:23:02] step 150: train loss 9.7823, val loss 9.8377
[2025-07-19 00:23:07] step 200: train loss 9.3384, val loss 9.3036
[2025-07-19 00:23:11] step 250: train loss 8.7125, val loss 8.7959
[2025-07-19 00:23:15] step 300: train loss 8.1840, val loss 8.2801
[2025-07-19 00:23:20] step 350: train loss 7.8320, val loss 7.7560
[2025-07-19 00:23:24] step 400: train loss 7.5064, val loss 7.5063
[2025-07-19 00:23:28] step 450: train loss 7.1899, val loss 7.0637
[2025-07-19 00:23:33] step 500: train loss 7.0918, val loss 7.1698
[2025-07-19 00:23:37] step 550: train loss 6.7893, val loss 6.8511
[2025-07-19 00:23:42] step 600: train loss 6.9614, val loss 6.8832
[2025-07-19 00:23:46] step 650: train loss 6.7762, val loss 6.8587
[2025-07-19 00:23:51] step 700: train loss 6.7074, val loss 6.6580
[2025-07-19 00:23:55] step 750: train loss 6.4269, val loss 6.6308
[2025-07-19 00:24:00] step 800: train loss 6.6216, val loss 6.6123
[2025-07-19 00:24:04] step 850: train loss 6.6725, val loss 6.4895
[2025-07-19 00:24:08] step 900: train loss 6.7802, val loss 6.4087
[2025-07-19 00:24:13] step 950: train loss 6.4776, val loss 6.2967
[2025-07-19 00:24:17] step 1000: train loss 6.2941, val loss 6.2941
[2025-07-19 00:24:22] step 1050: train loss 6.3392, val loss 6.3452
[2025-07-19 00:24:26] step 1100: train loss 6.0979, val loss 6.5650
[2025-07-19 00:24:31] step 1150: train loss 6.1933, val loss 6.0622
[2025-07-19 00:24:35] step 1200: train loss 6.1274, val loss 6.3026
[2025-07-19 00:24:39] step 1250: train loss 6.0407, val loss 6.2452
[2025-07-19 00:24:44] step 1300: train loss 6.1202, val loss 6.4619
[2025-07-19 00:24:48] step 1350: train loss 5.9710, val loss 6.0242
[2025-07-19 00:24:53] step 1400: train loss 6.1802, val loss 6.1098
[2025-07-19 00:24:57] step 1450: train loss 6.0880, val loss 6.2355
[2025-07-19 00:25:02] step 1500: train loss 5.9204, val loss 6.0909
[2025-07-19 00:25:06] step 1550: train loss 6.1254, val loss 5.8590
