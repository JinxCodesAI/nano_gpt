Calculating target architecture based on schedule...

============================================================
       TARGET MODEL ARCHITECTURE (at end of schedule)
============================================================
  n_layer                | 6
  n_head                 | 12
  n_embd                 | 768
  n_hidden               | 3072
  block_size             | 1024
  vocab_size             | 50304
  dropout                | 0.0
  bias                   | False
  embedding_mode         | lora
  attn_lora_rank         | 48
  embedding_rank         | 48
  lora_alpha             | 1.0
============================================================


============================================================
           INITIAL MODEL ARCHITECTURE (at Iter 0)
============================================================
  n_layer                | 3
  n_head                 | 12
  n_embd                 | 768
  n_hidden               | 768
  block_size             | 1024
  vocab_size             | 50304
  dropout                | 0.0
  bias                   | False
  embedding_mode         | lora
  attn_lora_rank         | 48
  embedding_rank         | 48
  lora_alpha             | 1.0
============================================================


Detailed parameter count:
  total                  | Total:   52,149,504 | Trainable:    8,207,616
  token_embeddings       | Total:    2,451,456 | Trainable:    2,451,456
  position_embeddings    | Total:            0 | Trainable:            0
  attention_layers       | Total:    7,520,256 | Trainable:    2,211,840
  feed_forward_layers    | Total:    3,538,944 | Trainable:    3,538,944
  layer_norms            | Total:        4,608 | Trainable:        4,608
  final_layer_norm       | Total:          768 | Trainable:          768
  language_model_head    | Total:   38,633,472 | Trainable:            0
------------------------------------------------------------
eval every:100
W0720 12:31:09.946000 2566 /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/_inductor/utils.py:1250] [0/0] Not enough SMs to use max_autotune_gemm mode
step 0: train loss 10.9689, val loss 10.9694
merge_lora_weights 10.969396591186523 8.0 500
iter 0: loss 10.9705, lr 0.00005, time 84865.36ms, mfu -100.00%
iter 10: loss 9.5731, lr 0.00055, time 538.26ms, mfu 34.33%
iter 20: loss 9.4088, lr 0.00104, time 538.03ms, mfu 34.34%
iter 30: loss 9.2388, lr 0.00154, time 549.77ms, mfu 34.26%
iter 40: loss 9.2057, lr 0.00204, time 547.81ms, mfu 34.21%
iter 50: loss 9.0541, lr 0.00254, time 559.73ms, mfu 34.09%
iter 60: loss 8.8265, lr 0.00303, time 560.01ms, mfu 33.98%
iter 70: loss 8.7809, lr 0.00353, time 540.32ms, mfu 34.00%
iter 80: loss 8.5961, lr 0.00403, time 563.40ms, mfu 33.88%
iter 90: loss 8.7008, lr 0.00453, time 559.78ms, mfu 33.80%
step 100: train loss 8.5294, val loss 8.5778
saving checkpoint to out
merge_lora_weights 8.577812194824219 8.0 500
iter 100: loss 8.5189, lr 0.00502, time 4401.69ms, mfu 30.84%
iter 110: loss 8.5503, lr 0.00552, time 572.53ms, mfu 30.98%
iter 120: loss 8.3277, lr 0.00602, time 579.65ms, mfu 31.07%
Traceback (most recent call last):
  File "/teamspace/studios/this_studio/nanoGPT/train.py", line 795, in <module>
    lossf = loss.item() * gradient_accumulation_steps
KeyboardInterrupt
Traceback (most recent call last):
  File "/teamspace/studios/this_studio/nanoGPT/train.py", line 795, in <module>
    lossf = loss.item() * gradient_accumulation_steps
KeyboardInterrupt
