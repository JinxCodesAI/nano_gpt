Calculating target architecture based on schedule...

============================================================
       TARGET MODEL ARCHITECTURE (at end of schedule)
============================================================
  n_layer                | 0
  n_head                 | 12
  n_embd                 | 768
  n_hidden               | 384
  block_size             | 1024
  vocab_size             | 50304
  dropout                | 0.0
  bias                   | False
  embedding_mode         | standard
  attn_lora_rank         | 0
  embedding_rank         | 0
  lora_alpha             | 0.0
============================================================


============================================================
           INITIAL MODEL ARCHITECTURE (at Iter 0)
============================================================
  n_layer                | 0
  n_head                 | 12
  n_embd                 | 768
  n_hidden               | 192
  block_size             | 1024
  vocab_size             | 50304
  dropout                | 0.0
  bias                   | False
  embedding_mode         | lora
  attn_lora_rank         | 0
  embedding_rank         | 0
  lora_alpha             | 1.0
============================================================


Detailed parameter count:
  total                  | Total:   38,634,240 | Trainable:   38,634,240
  token_embeddings       | Total:            0 | Trainable:            0
  position_embeddings    | Total:            0 | Trainable:            0
  attention_layers       | Total:            0 | Trainable:            0
  feed_forward_layers    | Total:            0 | Trainable:            0
  layer_norms            | Total:            0 | Trainable:            0
  final_layer_norm       | Total:          768 | Trainable:          768
  language_model_head    | Total:   38,633,472 | Trainable:   38,633,472
------------------------------------------------------------
eval every:200
step 0: train loss 15.1461, val loss 15.1569
--- Model Analysis ---
  Embedding Utilization: 97.53% (749/768)
W0720 21:07:34.984000 56281 /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/_inductor/utils.py:1250] [0/1] Not enough SMs to use max_autotune_gemm mode
Warning: Model did not return attention scores.
----------------------
merge_lora_weights 15.156898498535156 4.0 200
iter 0: loss 15.1495, lr 0.00000, time 8272.98ms, mfu -100.00%
iter 10: loss 15.1283, lr 0.00001, time 8.99ms, mfu 1396.44%
iter 20: loss 15.0924, lr 0.00001, time 13.83ms, mfu 1347.57%
iter 30: loss 15.0270, lr 0.00002, time 17.92ms, mfu 1282.89%
iter 40: loss 14.9372, lr 0.00002, time 9.93ms, mfu 1281.00%
iter 50: loss 14.8104, lr 0.00003, time 8.18ms, mfu 1306.31%
iter 60: loss 14.6893, lr 0.00003, time 9.19ms, mfu 1312.34%
iter 70: loss 14.4975, lr 0.00004, time 8.59ms, mfu 1327.31%
iter 80: loss 14.3303, lr 0.00004, time 8.00ms, mfu 1351.52%
iter 90: loss 14.1082, lr 0.00005, time 20.20ms, mfu 1278.52%
iter 100: loss 13.9037, lr 0.00005, time 18.01ms, mfu 1220.38%
iter 110: loss 13.6342, lr 0.00006, time 13.15ms, mfu 1193.78%
iter 120: loss 13.3568, lr 0.00006, time 9.59ms, mfu 1205.38%
iter 130: loss 13.1089, lr 0.00007, time 10.07ms, mfu 1209.55%
iter 140: loss 12.8312, lr 0.00007, time 8.67ms, mfu 1233.39%
iter 150: loss 12.4439, lr 0.00008, time 11.57ms, mfu 1218.57%
iter 160: loss 12.2335, lr 0.00008, time 18.57ms, mfu 1164.33%
iter 170: loss 11.9482, lr 0.00009, time 8.91ms, mfu 1188.80%
iter 180: loss 11.6212, lr 0.00009, time 13.53ms, mfu 1162.69%
iter 190: loss 11.4263, lr 0.00010, time 12.57ms, mfu 1146.26%
step 200: train loss 11.1120, val loss 11.1760
--- Model Analysis ---
  Embedding Utilization: 97.40% (748/768)
Warning: Model did not return attention scores.
----------------------
saving checkpoint to out

=== SCALING OPERATION TRIGGERED (DDP SYNC) ===
Operation: merge_lora_weights
Trigger reason: Timeout
Current val loss: 11.1760, Trigger loss: 4.0000
Iterations since last op: 200, Max wait: 200
Executing operation: merge_lora_weights first burn with value: None
Performing architectural operation: merge_lora_weights
Merging LoRA weights into main weights...
LoRA weights merged and reset.

Detailed parameter count:
  total                  | Total:   38,634,240 | Trainable:   38,634,240
  token_embeddings       | Total:            0 | Trainable:            0
  position_embeddings    | Total:            0 | Trainable:            0
  attention_layers       | Total:            0 | Trainable:            0
  feed_forward_layers    | Total:            0 | Trainable:            0
  layer_norms            | Total:            0 | Trainable:            0
  final_layer_norm       | Total:          768 | Trainable:          768
  language_model_head    | Total:   38,633,472 | Trainable:   38,633,472
------------------------------------------------------------
Re-configuring optimizer after architectural change...
num decayed parameter tensors: 1, with 38,633,472 parameters
num non-decayed parameter tensors: 1, with 768 parameters
using fused AdamW: True
Transferring optimizer state for existing parameters...
Transferred optimizer state for 0 / 2 parameters
Re-compiling the model...

============================================================
             ARCHITECTURE CHANGE (at Iter 200)
============================================================
  n_layer                | 0
  n_head                 | 12
  n_embd                 | 768
  n_hidden               | 192
  block_size             | 1024
  vocab_size             | 50304
  dropout                | 0.0
  bias                   | False
  embedding_mode         | lora
  attn_lora_rank         | 0
  embedding_rank         | 0
  lora_alpha             | 1.0
============================================================

Architectural operation completed successfully.
=== SCALING OPERATION COMPLETE ===

merge_lora_weights 11.175973892211914 4.0 200
iter 200: loss 11.1313, lr 0.00010, time 5328.00ms, mfu 1031.87%
iter 210: loss 10.9249, lr 0.00010, time 8.65ms, mfu 1073.83%
iter 220: loss 10.6511, lr 0.00010, time 9.74ms, mfu 1095.36%
iter 230: loss 10.4129, lr 0.00010, time 12.27ms, mfu 1088.12%
iter 240: loss 10.0930, lr 0.00010, time 13.07ms, mfu 1075.39%
iter 250: loss 9.8767, lr 0.00010, time 18.34ms, mfu 1036.30%
iter 260: loss 9.5019, lr 0.00010, time 8.54ms, mfu 1079.68%
iter 270: loss 9.3311, lr 0.00010, time 14.65ms, mfu 1057.40%
iter 280: loss 9.2441, lr 0.00010, time 17.51ms, mfu 1023.35%
iter 290: loss 9.2386, lr 0.00010, time 18.54ms, mfu 988.75%
iter 300: loss 9.0849, lr 0.00010, time 10.13ms, mfu 1013.78%
iter 310: loss 8.6896, lr 0.00010, time 10.43ms, mfu 1032.72%
iter 320: loss 8.7066, lr 0.00010, time 14.66ms, mfu 1015.07%
iter 330: loss 8.6516, lr 0.00010, time 13.12ms, mfu 1009.25%
iter 340: loss 8.6862, lr 0.00010, time 12.42ms, mfu 1009.39%
iter 350: loss 8.5671, lr 0.00010, time 13.28ms, mfu 1002.99%
iter 360: loss 8.3341, lr 0.00010, time 16.09ms, mfu 980.71%
iter 370: loss 8.4181, lr 0.00010, time 10.68ms, mfu 1000.17%
iter 380: loss 8.1542, lr 0.00010, time 15.20ms, mfu 982.77%
iter 390: loss 8.1581, lr 0.00010, time 8.38ms, mfu 1034.28%
step 400: train loss 8.0054, val loss 8.2813
--- Model Analysis ---
  Embedding Utilization: 97.14% (746/768)
Warning: Model did not return attention scores.
----------------------
saving checkpoint to out

=== SCALING OPERATION TRIGGERED (DDP SYNC) ===
Operation: merge_lora_weights
Trigger reason: Timeout
Current val loss: 8.2813, Trigger loss: 4.0000
Iterations since last op: 200, Max wait: 200
Executing operation: merge_lora_weights first burn with value: None
Performing architectural operation: merge_lora_weights
Merging LoRA weights into main weights...
LoRA weights merged and reset.

Detailed parameter count:
  total                  | Total:   38,634,240 | Trainable:   38,634,240
  token_embeddings       | Total:            0 | Trainable:            0
  position_embeddings    | Total:            0 | Trainable:            0
  attention_layers       | Total:            0 | Trainable:            0
  feed_forward_layers    | Total:            0 | Trainable:            0
  layer_norms            | Total:            0 | Trainable:            0
  final_layer_norm       | Total:          768 | Trainable:          768
  language_model_head    | Total:   38,633,472 | Trainable:   38,633,472
------------------------------------------------------------
Re-configuring optimizer after architectural change...
num decayed parameter tensors: 1, with 38,633,472 parameters
num non-decayed parameter tensors: 1, with 768 parameters
using fused AdamW: True
Transferring optimizer state for existing parameters...
Transferred optimizer state for 0 / 2 parameters
Re-compiling the model...

============================================================
             ARCHITECTURE CHANGE (at Iter 400)
============================================================
  n_layer                | 0
  n_head                 | 12
  n_embd                 | 768
  n_hidden               | 192
  block_size             | 1024
  vocab_size             | 50304
  dropout                | 0.0
  bias                   | False
  embedding_mode         | lora
  attn_lora_rank         | 0
  embedding_rank         | 0
  lora_alpha             | 1.0
============================================================

Architectural operation completed successfully.
=== SCALING OPERATION COMPLETE ===

merge_lora_weights 8.281304359436035 4.0 200
iter 400: loss 8.2279, lr 0.00010, time 5769.51ms, mfu 931.07%
iter 410: loss 8.1466, lr 0.00010, time 15.26ms, mfu 920.23%
iter 420: loss 8.1162, lr 0.00010, time 7.82ms, mfu 988.73%
iter 430: loss 8.0755, lr 0.00010, time 12.80ms, mfu 987.91%
iter 440: loss 7.8371, lr 0.00010, time 8.75ms, mfu 1032.63%
iter 450: loss 7.9749, lr 0.00010, time 10.81ms, mfu 1045.55%
iter 460: loss 8.1372, lr 0.00010, time 17.96ms, mfu 1010.90%
iter 470: loss 8.0459, lr 0.00010, time 12.29ms, mfu 1011.93%
iter 480: loss 7.8799, lr 0.00010, time 9.27ms, mfu 1046.12%
iter 490: loss 7.8609, lr 0.00010, time 10.18ms, mfu 1064.85%
iter 500: loss 7.8045, lr 0.00010, time 20.47ms, mfu 1019.69%
iter 510: loss 7.8625, lr 0.00010, time 9.66ms, mfu 1047.64%
iter 520: loss 7.8879, lr 0.00010, time 10.97ms, mfu 1057.36%
iter 530: loss 7.9942, lr 0.00010, time 10.66ms, mfu 1069.36%
iter 540: loss 7.7446, lr 0.00010, time 16.41ms, mfu 1038.93%
iter 550: loss 7.7512, lr 0.00010, time 8.52ms, mfu 1082.38%
iter 560: loss 7.6736, lr 0.00010, time 7.46ms, mfu 1142.46%
iter 570: loss 7.8895, lr 0.00010, time 16.54ms, mfu 1104.13%
iter 580: loss 7.6964, lr 0.00010, time 8.86ms, mfu 1135.46%
iter 590: loss 7.6542, lr 0.00010, time 9.39ms, mfu 1155.64%
step 600: train loss 7.6129, val loss 7.8607
--- Model Analysis ---
  Embedding Utilization: 96.88% (744/768)
Warning: Model did not return attention scores.
----------------------
saving checkpoint to out

=== SCALING OPERATION TRIGGERED (DDP SYNC) ===
Operation: merge_lora_weights
Trigger reason: Timeout
Current val loss: 7.8607, Trigger loss: 4.0000
Iterations since last op: 200, Max wait: 200
Executing operation: merge_lora_weights first burn with value: None
Performing architectural operation: merge_lora_weights
Merging LoRA weights into main weights...
LoRA weights merged and reset.

Detailed parameter count:
  total                  | Total:   38,634,240 | Trainable:   38,634,240
  token_embeddings       | Total:            0 | Trainable:            0
  position_embeddings    | Total:            0 | Trainable:            0
  attention_layers       | Total:            0 | Trainable:            0
  feed_forward_layers    | Total:            0 | Trainable:            0
  layer_norms            | Total:            0 | Trainable:            0
  final_layer_norm       | Total:          768 | Trainable:          768
  language_model_head    | Total:   38,633,472 | Trainable:   38,633,472
------------------------------------------------------------
Re-configuring optimizer after architectural change...
num decayed parameter tensors: 1, with 38,633,472 parameters
num non-decayed parameter tensors: 1, with 768 parameters
using fused AdamW: True
Transferring optimizer state for existing parameters...
Transferred optimizer state for 0 / 2 parameters
Re-compiling the model...

============================================================
             ARCHITECTURE CHANGE (at Iter 600)
============================================================
  n_layer                | 0
  n_head                 | 12
  n_embd                 | 768
  n_hidden               | 192
  block_size             | 1024
  vocab_size             | 50304
  dropout                | 0.0
  bias                   | False
  embedding_mode         | lora
  attn_lora_rank         | 0
  embedding_rank         | 0
  lora_alpha             | 1.0
============================================================

Architectural operation completed successfully.
=== SCALING OPERATION COMPLETE ===

merge_lora_weights 7.860712051391602 4.0 200
iter 600: loss 7.7443, lr 0.00010, time 5962.60ms, mfu 1040.29%
iter 610: loss 7.7057, lr 0.00010, time 10.24ms, mfu 1058.89%
iter 620: loss 7.6420, lr 0.00010, time 7.74ms, mfu 1115.28%
iter 630: loss 7.8724, lr 0.00010, time 9.37ms, mfu 1137.75%
iter 640: loss 7.8020, lr 0.00010, time 10.15ms, mfu 1147.71%
iter 650: loss 7.8026, lr 0.00010, time 8.44ms, mfu 1181.71%
iter 660: loss 7.5325, lr 0.00010, time 11.05ms, mfu 1177.18%
iter 670: loss 7.5992, lr 0.00010, time 12.66ms, mfu 1158.62%
iter 680: loss 7.6349, lr 0.00010, time 18.54ms, mfu 1110.48%
iter 690: loss 7.7949, lr 0.00010, time 8.21ms, mfu 1152.33%
iter 700: loss 7.6586, lr 0.00010, time 8.13ms, mfu 1191.46%
iter 710: loss 7.5776, lr 0.00010, time 14.00ms, mfu 1162.03%
iter 720: loss 7.5143, lr 0.00010, time 9.50ms, mfu 1177.91%
iter 730: loss 7.7421, lr 0.00010, time 16.46ms, mfu 1136.42%
iter 740: loss 7.3650, lr 0.00010, time 14.69ms, mfu 1108.23%
iter 750: loss 7.5084, lr 0.00010, time 12.10ms, mfu 1101.19%
iter 760: loss 7.5327, lr 0.00010, time 11.76ms, mfu 1097.82%
iter 770: loss 7.6534, lr 0.00010, time 20.92ms, mfu 1048.05%
iter 780: loss 7.5368, lr 0.00010, time 15.88ms, mfu 1022.33%
iter 790: loss 7.4526, lr 0.00010, time 13.85ms, mfu 1010.76%
step 800: train loss 7.4168, val loss 7.5630
--- Model Analysis ---
  Embedding Utilization: 96.61% (742/768)
Warning: Model did not return attention scores.
----------------------
saving checkpoint to out

=== SCALING OPERATION TRIGGERED (DDP SYNC) ===
Operation: merge_lora_weights
Trigger reason: Timeout
Current val loss: 7.5630, Trigger loss: 4.0000
Iterations since last op: 200, Max wait: 200
Executing operation: merge_lora_weights first burn with value: None
Performing architectural operation: merge_lora_weights
Merging LoRA weights into main weights...
LoRA weights merged and reset.

Detailed parameter count:
  total                  | Total:   38,634,240 | Trainable:   38,634,240
  token_embeddings       | Total:            0 | Trainable:            0
  position_embeddings    | Total:            0 | Trainable:            0
  attention_layers       | Total:            0 | Trainable:            0
  feed_forward_layers    | Total:            0 | Trainable:            0
  layer_norms            | Total:            0 | Trainable:            0
  final_layer_norm       | Total:          768 | Trainable:          768
  language_model_head    | Total:   38,633,472 | Trainable:   38,633,472
------------------------------------------------------------
Re-configuring optimizer after architectural change...
num decayed parameter tensors: 1, with 38,633,472 parameters
num non-decayed parameter tensors: 1, with 768 parameters
using fused AdamW: True
Transferring optimizer state for existing parameters...
Transferred optimizer state for 0 / 2 parameters
Re-compiling the model...

============================================================
             ARCHITECTURE CHANGE (at Iter 800)
============================================================
  n_layer                | 0
  n_head                 | 12
  n_embd                 | 768
  n_hidden               | 192
  block_size             | 1024
  vocab_size             | 50304
  dropout                | 0.0
  bias                   | False
  embedding_mode         | lora
  attn_lora_rank         | 0
  embedding_rank         | 0
  lora_alpha             | 1.0
============================================================

Architectural operation completed successfully.
=== SCALING OPERATION COMPLETE ===

merge_lora_weights 7.562980651855469 4.0 200
iter 800: loss 7.4964, lr 0.00010, time 5516.03ms, mfu 909.91%
iter 810: loss 7.3692, lr 0.00010, time 8.96ms, mfu 958.99%
iter 820: loss 7.5273, lr 0.00010, time 11.74ms, mfu 970.04%
iter 830: loss 7.3818, lr 0.00010, time 8.28ms, mfu 1024.71%
iter 840: loss 7.3342, lr 0.00010, time 11.15ms, mfu 1034.82%
iter 850: loss 7.5230, lr 0.00010, time 11.59ms, mfu 1039.66%
iter 860: loss 7.5638, lr 0.00010, time 11.89ms, mfu 1041.27%
iter 870: loss 7.4176, lr 0.00010, time 9.42ms, mfu 1070.48%
iter 880: loss 7.3718, lr 0.00010, time 14.87ms, mfu 1047.86%
iter 890: loss 7.6084, lr 0.00010, time 19.81ms, mfu 1006.44%
iter 900: loss 7.3702, lr 0.00010, time 12.13ms, mfu 1009.29%
iter 910: loss 7.2102, lr 0.00010, time 12.27ms, mfu 1010.72%
iter 920: loss 7.3333, lr 0.00010, time 12.19ms, mfu 1012.64%
iter 930: loss 7.4848, lr 0.00010, time 20.29ms, mfu 973.26%
iter 940: loss 7.4147, lr 0.00010, time 18.34ms, mfu 944.41%
iter 950: loss 7.3665, lr 0.00010, time 9.43ms, mfu 983.11%
iter 960: loss 7.2790, lr 0.00010, time 13.59ms, mfu 977.19%
iter 970: loss 7.3589, lr 0.00010, time 15.04ms, mfu 962.93%
iter 980: loss 7.2592, lr 0.00010, time 11.31ms, mfu 977.61%
iter 990: loss 7.2864, lr 0.00010, time 14.44ms, mfu 966.82%
step 1000: train loss 7.2971, val loss 7.3755
--- Model Analysis ---
  Embedding Utilization: 96.35% (740/768)
Warning: Model did not return attention scores.
----------------------
saving checkpoint to out

=== SCALING OPERATION TRIGGERED (DDP SYNC) ===
Operation: merge_lora_weights
Trigger reason: Timeout
Current val loss: 7.3755, Trigger loss: 4.0000
Iterations since last op: 200, Max wait: 200
Executing operation: merge_lora_weights first burn with value: None
Performing architectural operation: merge_lora_weights
Merging LoRA weights into main weights...
LoRA weights merged and reset.

Detailed parameter count:
  total                  | Total:   38,634,240 | Trainable:   38,634,240
  token_embeddings       | Total:            0 | Trainable:            0
  position_embeddings    | Total:            0 | Trainable:            0
  attention_layers       | Total:            0 | Trainable:            0
  feed_forward_layers    | Total:            0 | Trainable:            0
  layer_norms            | Total:            0 | Trainable:            0
  final_layer_norm       | Total:          768 | Trainable:          768
  language_model_head    | Total:   38,633,472 | Trainable:   38,633,472
------------------------------------------------------------
Re-configuring optimizer after architectural change...
num decayed parameter tensors: 1, with 38,633,472 parameters
num non-decayed parameter tensors: 1, with 768 parameters
using fused AdamW: True
Transferring optimizer state for existing parameters...
Transferred optimizer state for 0 / 2 parameters
Re-compiling the model...

============================================================
             ARCHITECTURE CHANGE (at Iter 1000)
============================================================
  n_layer                | 0
  n_head                 | 12
  n_embd                 | 768
  n_hidden               | 192
  block_size             | 1024
  vocab_size             | 50304
  dropout                | 0.0
  bias                   | False
  embedding_mode         | lora
  attn_lora_rank         | 0
  embedding_rank         | 0
  lora_alpha             | 1.0
============================================================

Architectural operation completed successfully.
=== SCALING OPERATION COMPLETE ===

merge_lora_weights 7.375514507293701 4.0 200
iter 1000: loss 7.2883, lr 0.00010, time 5768.22ms, mfu 870.36%
iter 1010: loss 7.4319, lr 0.00010, time 12.76ms, mfu 881.72%
iter 1020: loss 7.5549, lr 0.00010, time 9.50ms, mfu 925.66%
iter 1030: loss 7.2789, lr 0.00010, time 8.97ms, mfu 973.01%
iter 1040: loss 7.3527, lr 0.00010, time 17.15ms, mfu 948.93%
iter 1050: loss 7.4403, lr 0.00010, time 17.62ms, mfu 925.28%
iter 1060: loss 7.4041, lr 0.00010, time 15.03ms, mfu 916.30%
iter 1070: loss 7.2711, lr 0.00010, time 16.55ms, mfu 900.54%
iter 1080: loss 7.3447, lr 0.00010, time 13.29ms, mfu 904.95%
iter 1090: loss 7.1783, lr 0.00010, time 10.68ms, mfu 932.06%
iter 1100: loss 7.3547, lr 0.00010, time 8.43ms, mfu 987.80%
iter 1110: loss 7.4100, lr 0.00010, time 19.33ms, mfu 953.97%
iter 1120: loss 7.2667, lr 0.00010, time 13.96ms, mfu 948.52%
iter 1130: loss 7.1711, lr 0.00010, time 11.90ms, mfu 959.13%
iter 1140: loss 7.2289, lr 0.00010, time 21.32ms, mfu 922.11%
iter 1150: loss 7.2954, lr 0.00010, time 16.13ms, mfu 907.71%
iter 1160: loss 7.3080, lr 0.00010, time 11.52ms, mfu 925.94%
Traceback (most recent call last):
  File "/teamspace/studios/this_studio/nanoGPT/train.py", line 865, in <module>
    t0 = t1
KeyboardInterrupt
Traceback (most recent call last):
  File "/teamspace/studios/this_studio/nanoGPT/train.py", line 865, in <module>
    t0 = t1
KeyboardInterrupt
