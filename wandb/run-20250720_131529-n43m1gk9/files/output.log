Calculating target architecture based on schedule...

============================================================
       TARGET MODEL ARCHITECTURE (at end of schedule)
============================================================
  n_layer                | 24
  n_head                 | 12
  n_embd                 | 768
  n_hidden               | 3072
  block_size             | 1024
  vocab_size             | 50304
  dropout                | 0.0
  bias                   | False
  embedding_mode         | lora
  attn_lora_rank         | 48
  embedding_rank         | 48
  lora_alpha             | 1.0
============================================================


============================================================
           INITIAL MODEL ARCHITECTURE (at Iter 0)
============================================================
  n_layer                | 3
  n_head                 | 12
  n_embd                 | 768
  n_hidden               | 768
  block_size             | 1024
  vocab_size             | 50304
  dropout                | 0.0
  bias                   | False
  embedding_mode         | lora
  attn_lora_rank         | 48
  embedding_rank         | 48
  lora_alpha             | 1.0
============================================================


Detailed parameter count:
  total                  | Total:   52,149,504 | Trainable:    8,207,616
  token_embeddings       | Total:    2,451,456 | Trainable:    2,451,456
  position_embeddings    | Total:            0 | Trainable:            0
  attention_layers       | Total:    7,520,256 | Trainable:    2,211,840
  feed_forward_layers    | Total:    3,538,944 | Trainable:    3,538,944
  layer_norms            | Total:        4,608 | Trainable:        4,608
  final_layer_norm       | Total:          768 | Trainable:          768
  language_model_head    | Total:   38,633,472 | Trainable:            0
------------------------------------------------------------
eval every:100
step 0: train loss 10.9677, val loss 10.9694
merge_lora_weights 10.969396591186523 8.0 500
iter 0: loss 10.9616, lr 0.00005, time 6880.71ms, mfu -100.00%
iter 10: loss 9.5575, lr 0.00055, time 540.98ms, mfu 34.16%
iter 20: loss 9.5015, lr 0.00104, time 549.05ms, mfu 34.11%
iter 30: loss 9.3892, lr 0.00154, time 557.49ms, mfu 34.01%
iter 40: loss 9.2056, lr 0.00204, time 559.71ms, mfu 33.92%
iter 50: loss 9.0449, lr 0.00254, time 563.85ms, mfu 33.80%
iter 60: loss 8.9014, lr 0.00303, time 563.78ms, mfu 33.70%
iter 70: loss 8.8070, lr 0.00353, time 571.68ms, mfu 33.56%
iter 80: loss 8.6379, lr 0.00403, time 561.49ms, mfu 33.50%
iter 90: loss 8.6793, lr 0.00453, time 580.25ms, mfu 33.33%
step 100: train loss 8.5815, val loss 8.5840
saving checkpoint to out
merge_lora_weights 8.584029197692871 8.0 500
iter 100: loss 8.5377, lr 0.00502, time 4966.42ms, mfu 30.37%
iter 110: loss 8.5729, lr 0.00552, time 581.48ms, mfu 30.51%
iter 120: loss 8.3451, lr 0.00602, time 585.67ms, mfu 30.62%
iter 130: loss 8.2907, lr 0.00652, time 588.81ms, mfu 30.69%
iter 140: loss 8.4253, lr 0.00701, time 586.90ms, mfu 30.77%
iter 150: loss 8.4553, lr 0.00751, time 582.98ms, mfu 30.87%
iter 160: loss 8.2116, lr 0.00801, time 577.51ms, mfu 30.98%
iter 170: loss 8.1496, lr 0.00851, time 579.77ms, mfu 31.07%
iter 180: loss 8.1637, lr 0.00900, time 571.76ms, mfu 31.19%
iter 190: loss 8.0809, lr 0.00950, time 573.32ms, mfu 31.30%
step 200: train loss 8.0080, val loss 8.0112
saving checkpoint to out
merge_lora_weights 8.011184692382812 8.0 500
iter 200: loss 8.1027, lr 0.01000, time 4668.65ms, mfu 28.56%
iter 210: loss 7.9559, lr 0.01000, time 565.91ms, mfu 28.97%
iter 220: loss 7.8865, lr 0.01000, time 578.00ms, mfu 29.27%
iter 230: loss 7.9118, lr 0.01000, time 577.93ms, mfu 29.54%
iter 240: loss 7.7887, lr 0.01000, time 580.18ms, mfu 29.77%
iter 250: loss 7.7975, lr 0.01000, time 581.75ms, mfu 29.97%
iter 260: loss 7.6920, lr 0.01000, time 580.26ms, mfu 30.16%
iter 270: loss 7.6556, lr 0.01000, time 581.52ms, mfu 30.32%
iter 280: loss 7.6066, lr 0.01000, time 580.58ms, mfu 30.47%
iter 290: loss 7.6774, lr 0.01000, time 581.27ms, mfu 30.61%
step 300: train loss 7.7247, val loss 7.6872
saving checkpoint to out

=== SCALING OPERATION TRIGGERED (DDP SYNC) ===
Operation: merge_lora_weights
Trigger reason: Loss threshold
Current val loss: 7.6872, Trigger loss: 8.0000
Iterations since last op: 300, Max wait: 500
Executing operation: merge_lora_weights first burn with value: None
Performing architectural operation: merge_lora_weights
Merging LoRA weights into main weights...
LoRA weights merged and reset.

Detailed parameter count:
  total                  | Total:   52,149,504 | Trainable:    8,207,616
  token_embeddings       | Total:    2,451,456 | Trainable:    2,451,456
  position_embeddings    | Total:            0 | Trainable:            0
  attention_layers       | Total:    7,520,256 | Trainable:    2,211,840
  feed_forward_layers    | Total:    3,538,944 | Trainable:    3,538,944
  layer_norms            | Total:        4,608 | Trainable:        4,608
  final_layer_norm       | Total:          768 | Trainable:          768
  language_model_head    | Total:   38,633,472 | Trainable:            0
------------------------------------------------------------
Re-configuring optimizer after architectural change...
num decayed parameter tensors: 17, with 8,202,240 parameters
num non-decayed parameter tensors: 7, with 5,376 parameters
using fused AdamW: True
Transferring optimizer state for existing parameters...
Transferred optimizer state for 0 / 28 parameters
Re-compiling the model...

============================================================
             ARCHITECTURE CHANGE (at Iter 300)
============================================================
  n_layer                | 3
  n_head                 | 12
  n_embd                 | 768
  n_hidden               | 768
  block_size             | 1024
  vocab_size             | 50304
  dropout                | 0.0
  bias                   | False
  embedding_mode         | lora
  attn_lora_rank         | 48
  embedding_rank         | 48
  lora_alpha             | 1.0
============================================================

Architectural operation completed successfully.
=== SCALING OPERATION COMPLETE ===


=== SCALING OPERATION TRIGGERED (DDP SYNC) ===
Operation: change_lr
Trigger reason: Loss threshold
Current val loss: 7.6872, Trigger loss: 100.0000
Iterations since last op: 0, Max wait: 1
Executing operation: change_lr  with value: 0.7
LR multiplier: 10.0000 -> 7.0000
=== SCALING OPERATION COMPLETE ===


=== SCALING OPERATION TRIGGERED (DDP SYNC) ===
Operation: reset_lr_schedule
Trigger reason: Loss threshold
Current val loss: 7.6872, Trigger loss: 100.0000
Iterations since last op: 0, Max wait: 1
Executing operation: reset_lr_schedule  with value: None
LR schedule offset: 0 -> 300
=== SCALING OPERATION COMPLETE ===

merge_lora_weights 7.687236785888672 7.4 300
iter 300: loss 21.1542, lr 0.01000, time 4981.90ms, mfu 27.92%
iter 310: loss 10.9615, lr 0.00038, time 581.07ms, mfu 28.31%
iter 320: loss 8.4474, lr 0.00073, time 579.93ms, mfu 28.66%
iter 330: loss 7.9450, lr 0.00108, time 577.07ms, mfu 29.00%
iter 340: loss 7.8014, lr 0.00143, time 577.92ms, mfu 29.30%
iter 350: loss 7.8677, lr 0.00178, time 581.26ms, mfu 29.55%
iter 360: loss 7.7897, lr 0.00212, time 576.41ms, mfu 29.80%
iter 370: loss 7.4313, lr 0.00247, time 579.37ms, mfu 30.01%
iter 380: loss 7.3597, lr 0.00282, time 579.44ms, mfu 30.20%
iter 390: loss 7.3751, lr 0.00317, time 578.89ms, mfu 30.37%
step 400: train loss 7.5153, val loss 7.4398
saving checkpoint to out
merge_lora_weights 7.439838409423828 7.4 300
iter 400: loss 7.5333, lr 0.00352, time 4646.80ms, mfu 27.73%
iter 410: loss 7.5922, lr 0.00387, time 579.35ms, mfu 28.15%
iter 420: loss 7.4182, lr 0.00421, time 578.50ms, mfu 28.53%
iter 430: loss 7.3329, lr 0.00456, time 579.65ms, mfu 28.86%
iter 440: loss 7.5078, lr 0.00491, time 581.48ms, mfu 29.15%
iter 450: loss 7.3772, lr 0.00526, time 582.02ms, mfu 29.41%
iter 460: loss 7.2915, lr 0.00561, time 580.31ms, mfu 29.66%
iter 470: loss 7.5191, lr 0.00596, time 582.16ms, mfu 29.87%
iter 480: loss 7.4106, lr 0.00630, time 581.20ms, mfu 30.06%
iter 490: loss 7.3114, lr 0.00665, time 585.42ms, mfu 30.21%
step 500: train loss 7.3610, val loss 7.3414
saving checkpoint to out

=== SCALING OPERATION TRIGGERED (DDP SYNC) ===
Operation: merge_lora_weights
Trigger reason: Loss threshold
Current val loss: 7.3414, Trigger loss: 7.4000
Iterations since last op: 200, Max wait: 300
Executing operation: merge_lora_weights second burn with value: None
Performing architectural operation: merge_lora_weights
Merging LoRA weights into main weights...
LoRA weights merged and reset.

Detailed parameter count:
  total                  | Total:   52,149,504 | Trainable:    8,207,616
  token_embeddings       | Total:    2,451,456 | Trainable:    2,451,456
  position_embeddings    | Total:            0 | Trainable:            0
  attention_layers       | Total:    7,520,256 | Trainable:    2,211,840
  feed_forward_layers    | Total:    3,538,944 | Trainable:    3,538,944
  layer_norms            | Total:        4,608 | Trainable:        4,608
  final_layer_norm       | Total:          768 | Trainable:          768
  language_model_head    | Total:   38,633,472 | Trainable:            0
------------------------------------------------------------
Re-configuring optimizer after architectural change...
num decayed parameter tensors: 17, with 8,202,240 parameters
num non-decayed parameter tensors: 7, with 5,376 parameters
using fused AdamW: True
Transferring optimizer state for existing parameters...
Transferred optimizer state for 0 / 28 parameters
Re-compiling the model...

============================================================
             ARCHITECTURE CHANGE (at Iter 500)
============================================================
  n_layer                | 3
  n_head                 | 12
  n_embd                 | 768
  n_hidden               | 768
  block_size             | 1024
  vocab_size             | 50304
  dropout                | 0.0
  bias                   | False
  embedding_mode         | lora
  attn_lora_rank         | 48
  embedding_rank         | 48
  lora_alpha             | 1.0
============================================================

Architectural operation completed successfully.
=== SCALING OPERATION COMPLETE ===


=== SCALING OPERATION TRIGGERED (DDP SYNC) ===
Operation: change_lr
Trigger reason: Loss threshold
Current val loss: 7.3414, Trigger loss: 100.0000
Iterations since last op: 0, Max wait: 1
Executing operation: change_lr  with value: 0.2
LR multiplier: 7.0000 -> 1.4000
=== SCALING OPERATION COMPLETE ===


=== SCALING OPERATION TRIGGERED (DDP SYNC) ===
Operation: change_warmup_iters
Trigger reason: Loss threshold
Current val loss: 7.3414, Trigger loss: 100.0000
Iterations since last op: 0, Max wait: 1
Executing operation: change_warmup_iters  with value: 5
Warmup iters multiplier: 1.0000 -> 5.0000
=== SCALING OPERATION COMPLETE ===


=== SCALING OPERATION TRIGGERED (DDP SYNC) ===
Operation: reset_lr_schedule
Trigger reason: Loss threshold
Current val loss: 7.3414, Trigger loss: 100.0000
Iterations since last op: 0, Max wait: 1
Executing operation: reset_lr_schedule  with value: None
LR schedule offset: 300 -> 500
=== SCALING OPERATION COMPLETE ===


=== SCALING OPERATION TRIGGERED (DDP SYNC) ===
Operation: change_batch_size
Trigger reason: Loss threshold
Current val loss: 7.3414, Trigger loss: 100.0000
Iterations since last op: 0, Max wait: 1
Executing operation: change_batch_size  with value: 0.5
Batch size: 32 -> 16
=== SCALING OPERATION COMPLETE ===


=== SCALING OPERATION TRIGGERED (DDP SYNC) ===
Operation: change_grad_accum
Trigger reason: Loss threshold
Current val loss: 7.3414, Trigger loss: 100.0000
Iterations since last op: 0, Max wait: 1
Executing operation: change_grad_accum  with value: 2.0
Grad accum steps: 2 -> 4
=== SCALING OPERATION COMPLETE ===


=== SCALING OPERATION TRIGGERED (DDP SYNC) ===
Operation: change_lr
Trigger reason: Loss threshold
Current val loss: 7.3414, Trigger loss: 100.0000
Iterations since last op: 0, Max wait: 1
Executing operation: change_lr  with value: 0.7
LR multiplier: 1.4000 -> 0.9800
=== SCALING OPERATION COMPLETE ===


=== SCALING OPERATION TRIGGERED (DDP SYNC) ===
Operation: reset_lr_schedule
Trigger reason: Loss threshold
Current val loss: 7.3414, Trigger loss: 100.0000
Iterations since last op: 0, Max wait: 1
Executing operation: reset_lr_schedule  with value: None
LR schedule offset: 500 -> 500
=== SCALING OPERATION COMPLETE ===


=== SCALING OPERATION TRIGGERED (DDP SYNC) ===
Operation: stack_layers
Trigger reason: Loss threshold
Current val loss: 7.3414, Trigger loss: 100.0000
Iterations since last op: 0, Max wait: 1
Executing operation: stack_layers first resize with value: 2
Performing architectural operation: stack_layers
Stacking layers: current depth 3, creating 6 total layers.
Model now has 6 layers.

Detailed parameter count:
  total                  | Total:   63,213,312 | Trainable:   13,963,008
  token_embeddings       | Total:    2,451,456 | Trainable:    2,451,456
  position_embeddings    | Total:            0 | Trainable:            0
  attention_layers       | Total:   15,040,512 | Trainable:    4,423,680
  feed_forward_layers    | Total:    7,077,888 | Trainable:    7,077,888
  layer_norms            | Total:        9,216 | Trainable:        9,216
  final_layer_norm       | Total:          768 | Trainable:          768
  language_model_head    | Total:   38,633,472 | Trainable:            0
------------------------------------------------------------
Re-configuring optimizer after architectural change...
num decayed parameter tensors: 32, with 13,953,024 parameters
num non-decayed parameter tensors: 13, with 9,984 parameters
using fused AdamW: True
Transferring optimizer state for existing parameters...
Transferred optimizer state for 0 / 52 parameters
Re-compiling the model...

============================================================
             ARCHITECTURE CHANGE (at Iter 500)
============================================================
  n_layer                | 6
  n_head                 | 12
  n_embd                 | 768
  n_hidden               | 768
  block_size             | 1024
  vocab_size             | 50304
  dropout                | 0.0
  bias                   | False
  embedding_mode         | lora
  attn_lora_rank         | 48
  embedding_rank         | 48
  lora_alpha             | 1.0
============================================================

Architectural operation completed successfully.
Re-evaluating validation loss after operation...
New val loss after operation: 7.9932
iter 500: loss 7.9158, lr 0.00700, time 19846.16ms, mfu 27.31%
iter 510: loss 7.7779, lr 0.00001, time 755.38ms, mfu 27.70%
iter 520: loss 7.8495, lr 0.00002, time 765.67ms, mfu 28.02%
iter 530: loss 7.7018, lr 0.00003, time 774.98ms, mfu 28.26%
iter 540: loss 7.6274, lr 0.00004, time 778.23ms, mfu 28.47%
iter 550: loss 7.5106, lr 0.00005, time 779.88ms, mfu 28.65%
iter 560: loss 7.5657, lr 0.00006, time 780.40ms, mfu 28.81%
iter 570: loss 7.3314, lr 0.00007, time 774.37ms, mfu 28.98%
iter 580: loss 7.2228, lr 0.00008, time 773.64ms, mfu 29.13%
iter 590: loss 7.2813, lr 0.00009, time 766.23ms, mfu 29.30%
step 600: train loss 7.4012, val loss 7.3638
saving checkpoint to out
merge_lora_weights 7.363808631896973 7.1 700
iter 600: loss 7.3232, lr 0.00010, time 3659.34ms, mfu 27.01%
iter 610: loss 7.4310, lr 0.00011, time 758.07ms, mfu 27.43%
iter 620: loss 7.3343, lr 0.00012, time 759.58ms, mfu 27.79%
iter 630: loss 7.7113, lr 0.00013, time 759.69ms, mfu 28.12%
iter 640: loss 7.2469, lr 0.00014, time 761.49ms, mfu 28.41%
iter 650: loss 7.3674, lr 0.00015, time 762.28ms, mfu 28.67%
iter 660: loss 7.3579, lr 0.00016, time 765.52ms, mfu 28.88%
iter 670: loss 7.0801, lr 0.00017, time 767.81ms, mfu 29.07%
iter 680: loss 7.2547, lr 0.00018, time 766.62ms, mfu 29.24%
iter 690: loss 7.3817, lr 0.00019, time 771.67ms, mfu 29.38%
step 700: train loss 7.3435, val loss 7.3149
saving checkpoint to out
merge_lora_weights 7.314905643463135 7.1 700
iter 700: loss 7.3275, lr 0.00020, time 3608.12ms, mfu 27.09%
iter 710: loss 7.2522, lr 0.00021, time 764.86ms, mfu 27.47%
iter 720: loss 7.3899, lr 0.00022, time 761.11ms, mfu 27.83%
iter 730: loss 7.2668, lr 0.00023, time 763.50ms, mfu 28.14%
iter 740: loss 7.3277, lr 0.00024, time 761.91ms, mfu 28.42%
iter 750: loss 7.3479, lr 0.00025, time 762.59ms, mfu 28.68%
iter 760: loss 7.2912, lr 0.00026, time 760.79ms, mfu 28.91%
iter 770: loss 7.3521, lr 0.00027, time 760.56ms, mfu 29.12%
iter 780: loss 7.3812, lr 0.00028, time 763.32ms, mfu 29.30%
iter 790: loss 7.1724, lr 0.00028, time 759.90ms, mfu 29.48%
step 800: train loss 7.2351, val loss 7.2012
saving checkpoint to out
merge_lora_weights 7.2012128829956055 7.1 700
iter 800: loss 7.2722, lr 0.00029, time 3711.91ms, mfu 27.17%
iter 810: loss 7.1771, lr 0.00030, time 762.38ms, mfu 27.55%
iter 820: loss 7.2249, lr 0.00031, time 762.81ms, mfu 27.89%
iter 830: loss 7.2708, lr 0.00032, time 764.87ms, mfu 28.19%
iter 840: loss 7.2829, lr 0.00033, time 764.03ms, mfu 28.46%
iter 850: loss 7.3747, lr 0.00034, time 764.78ms, mfu 28.70%
iter 860: loss 7.3295, lr 0.00035, time 767.76ms, mfu 28.90%
iter 870: loss 7.1165, lr 0.00036, time 766.16ms, mfu 29.10%
iter 880: loss 7.1494, lr 0.00037, time 769.95ms, mfu 29.25%
iter 890: loss 7.3190, lr 0.00038, time 763.52ms, mfu 29.42%
step 900: train loss 7.1377, val loss 7.2288
saving checkpoint to out
merge_lora_weights 7.228762626647949 7.1 700
iter 900: loss 7.1413, lr 0.00039, time 3661.11ms, mfu 27.12%
iter 910: loss 7.2479, lr 0.00040, time 761.28ms, mfu 27.51%
iter 920: loss 7.1484, lr 0.00041, time 762.91ms, mfu 27.85%
iter 930: loss 7.2979, lr 0.00042, time 762.23ms, mfu 28.17%
iter 940: loss 7.1987, lr 0.00043, time 762.73ms, mfu 28.45%
iter 950: loss 7.2033, lr 0.00044, time 764.90ms, mfu 28.69%
iter 960: loss 7.2292, lr 0.00045, time 764.00ms, mfu 28.91%
iter 970: loss 7.0534, lr 0.00046, time 765.33ms, mfu 29.10%
iter 980: loss 7.3182, lr 0.00047, time 766.77ms, mfu 29.27%
iter 990: loss 7.0021, lr 0.00048, time 764.73ms, mfu 29.43%
step 1000: train loss 7.2359, val loss 7.1869
saving checkpoint to out
merge_lora_weights 7.186896324157715 7.1 700
iter 1000: loss 7.3104, lr 0.00049, time 3575.20ms, mfu 27.15%
iter 1010: loss 7.1668, lr 0.00050, time 768.37ms, mfu 27.51%
iter 1020: loss 7.3832, lr 0.00051, time 762.85ms, mfu 27.85%
iter 1030: loss 7.2030, lr 0.00052, time 763.36ms, mfu 28.16%
iter 1040: loss 6.9962, lr 0.00053, time 766.15ms, mfu 28.42%
iter 1050: loss 7.1855, lr 0.00054, time 766.38ms, mfu 28.66%
iter 1060: loss 7.1997, lr 0.00055, time 762.42ms, mfu 28.89%
iter 1070: loss 6.9772, lr 0.00056, time 765.98ms, mfu 29.09%
iter 1080: loss 7.0540, lr 0.00057, time 763.02ms, mfu 29.27%
iter 1090: loss 6.8370, lr 0.00058, time 764.28ms, mfu 29.43%
step 1100: train loss 7.0550, val loss 7.1511
saving checkpoint to out
merge_lora_weights 7.151075839996338 7.1 700
iter 1100: loss 7.1963, lr 0.00059, time 3681.59ms, mfu 27.13%
iter 1110: loss 6.9991, lr 0.00060, time 762.31ms, mfu 27.52%
iter 1120: loss 7.0242, lr 0.00061, time 761.59ms, mfu 27.86%
iter 1130: loss 7.1465, lr 0.00062, time 763.52ms, mfu 28.17%
iter 1140: loss 7.0357, lr 0.00063, time 767.84ms, mfu 28.43%
iter 1150: loss 6.9756, lr 0.00064, time 762.69ms, mfu 28.68%
iter 1160: loss 7.3091, lr 0.00065, time 764.03ms, mfu 28.90%
iter 1170: loss 6.8374, lr 0.00066, time 765.06ms, mfu 29.10%
iter 1180: loss 7.3248, lr 0.00067, time 763.36ms, mfu 29.28%
iter 1190: loss 7.0539, lr 0.00068, time 763.94ms, mfu 29.44%
step 1200: train loss 7.0885, val loss 7.0564
saving checkpoint to out

=== SCALING OPERATION TRIGGERED (DDP SYNC) ===
Operation: merge_lora_weights
Trigger reason: Loss threshold
Current val loss: 7.0564, Trigger loss: 7.1000
Iterations since last op: 700, Max wait: 700
Executing operation: merge_lora_weights third burn with value: None
Performing architectural operation: merge_lora_weights
Merging LoRA weights into main weights...
LoRA weights merged and reset.

Detailed parameter count:
  total                  | Total:   63,213,312 | Trainable:   13,963,008
  token_embeddings       | Total:    2,451,456 | Trainable:    2,451,456
  position_embeddings    | Total:            0 | Trainable:            0
  attention_layers       | Total:   15,040,512 | Trainable:    4,423,680
  feed_forward_layers    | Total:    7,077,888 | Trainable:    7,077,888
  layer_norms            | Total:        9,216 | Trainable:        9,216
  final_layer_norm       | Total:          768 | Trainable:          768
  language_model_head    | Total:   38,633,472 | Trainable:            0
------------------------------------------------------------
Re-configuring optimizer after architectural change...
num decayed parameter tensors: 32, with 13,953,024 parameters
num non-decayed parameter tensors: 13, with 9,984 parameters
using fused AdamW: True
Transferring optimizer state for existing parameters...
Transferred optimizer state for 0 / 52 parameters
Re-compiling the model...

============================================================
             ARCHITECTURE CHANGE (at Iter 1200)
============================================================
  n_layer                | 6
  n_head                 | 12
  n_embd                 | 768
  n_hidden               | 768
  block_size             | 1024
  vocab_size             | 50304
  dropout                | 0.0
  bias                   | False
  embedding_mode         | lora
  attn_lora_rank         | 48
  embedding_rank         | 48
  lora_alpha             | 1.0
============================================================

Architectural operation completed successfully.
=== SCALING OPERATION COMPLETE ===


=== SCALING OPERATION TRIGGERED (DDP SYNC) ===
Operation: change_batch_size
Trigger reason: Loss threshold
Current val loss: 7.0564, Trigger loss: 100.0000
Iterations since last op: 0, Max wait: 1
Executing operation: change_batch_size  with value: 0.5
Batch size: 16 -> 8
=== SCALING OPERATION COMPLETE ===


=== SCALING OPERATION TRIGGERED (DDP SYNC) ===
Operation: change_grad_accum
Trigger reason: Loss threshold
Current val loss: 7.0564, Trigger loss: 100.0000
Iterations since last op: 0, Max wait: 1
Executing operation: change_grad_accum  with value: 2.0
Grad accum steps: 4 -> 8
=== SCALING OPERATION COMPLETE ===


=== SCALING OPERATION TRIGGERED (DDP SYNC) ===
Operation: widen_mlp
Trigger reason: Loss threshold
Current val loss: 7.0564, Trigger loss: 100.0000
Iterations since last op: 0, Max wait: 1
Executing operation: widen_mlp  with value: 2
Performing architectural operation: widen_mlp
Widening MLP layers by a factor of 2.
MLP hidden dimension widened to 1536.

Detailed parameter count:
  total                  | Total:   70,291,200 | Trainable:   21,040,896
  token_embeddings       | Total:    2,451,456 | Trainable:    2,451,456
  position_embeddings    | Total:            0 | Trainable:            0
  attention_layers       | Total:   15,040,512 | Trainable:    4,423,680
  feed_forward_layers    | Total:   14,155,776 | Trainable:   14,155,776
  layer_norms            | Total:        9,216 | Trainable:        9,216
  final_layer_norm       | Total:          768 | Trainable:          768
  language_model_head    | Total:   38,633,472 | Trainable:            0
------------------------------------------------------------
Re-configuring optimizer after architectural change...
num decayed parameter tensors: 32, with 21,030,912 parameters
num non-decayed parameter tensors: 13, with 9,984 parameters
using fused AdamW: True
Transferring optimizer state for existing parameters...
Transferred optimizer state for 0 / 52 parameters
Re-compiling the model...

============================================================
             ARCHITECTURE CHANGE (at Iter 1200)
============================================================
  n_layer                | 6
  n_head                 | 12
  n_embd                 | 768
  n_hidden               | 1536
  block_size             | 1024
  vocab_size             | 50304
  dropout                | 0.0
  bias                   | False
  embedding_mode         | lora
  attn_lora_rank         | 48
  embedding_rank         | 48
  lora_alpha             | 1.0
============================================================

Architectural operation completed successfully.
Re-evaluating validation loss after operation...
New val loss after operation: 7.0378
iter 1200: loss 6.8651, lr 0.00069, time 17816.87ms, mfu 26.64%
iter 1210: loss 7.0417, lr 0.00070, time 819.64ms, mfu 27.14%
iter 1220: loss 7.1394, lr 0.00071, time 819.28ms, mfu 27.59%
iter 1230: loss 7.4039, lr 0.00072, time 834.59ms, mfu 27.93%
iter 1240: loss 7.1068, lr 0.00073, time 837.33ms, mfu 28.24%
iter 1250: loss 6.8783, lr 0.00074, time 844.51ms, mfu 28.48%
iter 1260: loss 7.2350, lr 0.00075, time 829.57ms, mfu 28.76%
iter 1270: loss 7.4688, lr 0.00075, time 829.92ms, mfu 29.00%
iter 1280: loss 7.0219, lr 0.00076, time 820.17ms, mfu 29.26%
iter 1290: loss 7.0670, lr 0.00077, time 816.73ms, mfu 29.51%
step 1300: train loss 7.0142, val loss 7.0641
saving checkpoint to out

=== SCALING OPERATION TRIGGERED (DDP SYNC) ===
Operation: change_lr
Trigger reason: Loss threshold
Current val loss: 7.0641, Trigger loss: 100.0000
Iterations since last op: 100, Max wait: 1
Executing operation: change_lr  with value: 0.7
LR multiplier: 0.9800 -> 0.6860
=== SCALING OPERATION COMPLETE ===


=== SCALING OPERATION TRIGGERED (DDP SYNC) ===
Operation: reset_lr_schedule
Trigger reason: Loss threshold
Current val loss: 7.0641, Trigger loss: 100.0000
Iterations since last op: 0, Max wait: 1
Executing operation: reset_lr_schedule  with value: None
LR schedule offset: 500 -> 1300
=== SCALING OPERATION COMPLETE ===


=== SCALING OPERATION TRIGGERED (DDP SYNC) ===
Operation: stack_layers
Trigger reason: Loss threshold
Current val loss: 7.0641, Trigger loss: 100.0000
Iterations since last op: 0, Max wait: 1
Executing operation: stack_layers third resize with value: 2
Performing architectural operation: stack_layers
Stacking layers: current depth 6, creating 12 total layers.
Model now has 12 layers.

Detailed parameter count:
  total                  | Total:   99,496,704 | Trainable:   39,629,568
  token_embeddings       | Total:    2,451,456 | Trainable:    2,451,456
  position_embeddings    | Total:            0 | Trainable:            0
  attention_layers       | Total:   30,081,024 | Trainable:    8,847,360
  feed_forward_layers    | Total:   28,311,552 | Trainable:   28,311,552
  layer_norms            | Total:       18,432 | Trainable:       18,432
  final_layer_norm       | Total:          768 | Trainable:          768
  language_model_head    | Total:   38,633,472 | Trainable:            0
------------------------------------------------------------
Re-configuring optimizer after architectural change...
num decayed parameter tensors: 62, with 39,610,368 parameters
num non-decayed parameter tensors: 25, with 19,200 parameters
using fused AdamW: True
Transferring optimizer state for existing parameters...
Transferred optimizer state for 0 / 100 parameters
Re-compiling the model...

============================================================
             ARCHITECTURE CHANGE (at Iter 1300)
============================================================
  n_layer                | 12
  n_head                 | 12
  n_embd                 | 768
  n_hidden               | 1536
  block_size             | 1024
  vocab_size             | 50304
  dropout                | 0.0
  bias                   | False
  embedding_mode         | lora
  attn_lora_rank         | 48
  embedding_rank         | 48
  lora_alpha             | 1.0
============================================================

Architectural operation completed successfully.
Re-evaluating validation loss after operation...
W0720 13:32:03.829000 46859 /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/_inductor/utils.py:1250] [0/6] Not enough SMs to use max_autotune_gemm mode
New val loss after operation: 7.3828
iter 1300: loss 7.6083, lr 0.00078, time 66608.60ms, mfu 26.61%
iter 1310: loss 7.4701, lr 0.00001, time 1278.46ms, mfu 26.96%
iter 1320: loss 7.3249, lr 0.00001, time 1298.35ms, mfu 27.23%
iter 1330: loss 7.2064, lr 0.00002, time 1319.67ms, mfu 27.42%
iter 1340: loss 7.0522, lr 0.00003, time 1323.04ms, mfu 27.59%
iter 1350: loss 7.0565, lr 0.00003, time 1315.48ms, mfu 27.75%
iter 1360: loss 7.2014, lr 0.00004, time 1298.49ms, mfu 27.94%
Traceback (most recent call last):
  File "/teamspace/studios/this_studio/nanoGPT/train.py", line 785, in <module>
    scaler.scale(loss).backward()
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/_tensor.py", line 648, in backward
    torch.autograd.backward(
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/autograd/__init__.py", line 353, in backward
    _engine_run_backward(
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/autograd/graph.py", line 824, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
KeyboardInterrupt
Traceback (most recent call last):
  File "/teamspace/studios/this_studio/nanoGPT/train.py", line 785, in <module>
    scaler.scale(loss).backward()
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/_tensor.py", line 648, in backward
    torch.autograd.backward(
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/autograd/__init__.py", line 353, in backward
    _engine_run_backward(
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/autograd/graph.py", line 824, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
KeyboardInterrupt
Exception ignored in atexit callback: <function _start_and_connect_service.<locals>.teardown_atexit at 0x7cd3113ad3f0>
Traceback (most recent call last):
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/wandb/sdk/lib/service/service_connection.py", line 54, in teardown_atexit
    conn.teardown(hooks.exit_code)
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/wandb/sdk/lib/service/service_connection.py", line 182, in teardown
    self._router.join()
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/wandb/sdk/interface/router.py", line 75, in join
    self._thread.join()
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/threading.py", line 1096, in join
    self._wait_for_tstate_lock()
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/threading.py", line 1116, in _wait_for_tstate_lock
    if lock.acquire(block, timeout):
KeyboardInterrupt:
Exception ignored in atexit callback: <function shutdown_compile_workers at 0x7cd31d9acee0>
Traceback (most recent call last):
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/_inductor/async_compile.py", line 113, in shutdown_compile_workers
    pool.shutdown()
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/_inductor/compile_worker/subproc_pool.py", line 239, in shutdown
    self.process.wait(300)
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/subprocess.py", line 1209, in wait
    return self._wait(timeout=timeout)
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/subprocess.py", line 1937, in _wait
    time.sleep(delay)
KeyboardInterrupt:
