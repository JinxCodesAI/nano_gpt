Calculating target architecture based on schedule...

============================================================
       TARGET MODEL ARCHITECTURE (at end of schedule)
============================================================
  n_layer                | 2
  n_head                 | 12
  n_embd                 | 768
  n_hidden               | 1536
  block_size             | 1024
  vocab_size             | 50304
  dropout                | 0.0
  bias                   | False
  embedding_mode         | standard
  attn_lora_rank         | 0
  embedding_rank         | 0
  lora_alpha             | 0.0
============================================================


============================================================
           INITIAL MODEL ARCHITECTURE (at Iter 0)
============================================================
  n_layer                | 1
  n_head                 | 12
  n_embd                 | 768
  n_hidden               | 768
  block_size             | 1024
  vocab_size             | 50304
  dropout                | 0.0
  bias                   | False
  embedding_mode         | lora
  attn_lora_rank         | 12
  embedding_rank         | 12
  lora_alpha             | 1.0
============================================================


Detailed parameter count:
  total                  | Total:   42,824,448 | Trainable:    2,421,504
  token_embeddings       | Total:      612,864 | Trainable:      612,864
  position_embeddings    | Total:            0 | Trainable:            0
  attention_layers       | Total:    2,396,160 | Trainable:      626,688
  feed_forward_layers    | Total:    1,179,648 | Trainable:    1,179,648
  layer_norms            | Total:        1,536 | Trainable:        1,536
  final_layer_norm       | Total:          768 | Trainable:          768
  language_model_head    | Total:   38,633,472 | Trainable:            0
------------------------------------------------------------
eval every:200
step 0: train loss 10.9695, val loss 10.9686
  MLP Rank Utilization (L0): 71.09% (546/768)
Attn LoRA    | 12/12           | 100.00%
--- Model Analysis ---
  Embedding Utilization (L0): 97.53% (749/768)
Embed LoRA   | 12/12           | 100.00%
W0720 20:52:31.915000 36692 /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/_inductor/utils.py:1250] [0/1] Not enough SMs to use max_autotune_gemm mode
  Average Attention Entropy:  5.8915
----------------------
merge_lora_weights 10.968631744384766 4.0 200
Traceback (most recent call last):
  File "/teamspace/studios/this_studio/nanoGPT/train.py", line 853, in <module>
    scaler.scale(loss).backward()
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/_tensor.py", line 648, in backward
    torch.autograd.backward(
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/autograd/__init__.py", line 353, in backward
    _engine_run_backward(
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/autograd/graph.py", line 824, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/autograd/function.py", line 307, in apply
    return user_fn(self, *args)
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 2111, in backward
    return impl_fn()
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 2097, in impl_fn
    out = CompiledFunction._backward_impl(ctx, all_args)
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 2217, in _backward_impl
    out = call_func_at_runtime_with_args(
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/_functorch/_aot_autograd/utils.py", line 126, in call_func_at_runtime_with_args
    out = normalize_as_list(f(args))
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py", line 838, in _fn
    return fn(*args, **kwargs)
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/_inductor/output_code.py", line 460, in __call__
    return self.current_callable(inputs)
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/_inductor/utils.py", line 2404, in run
    return model(new_inputs)
  File "/tmp/torchinductor_adamskrodzki/5v/c5vfqg2x37e5rudtf2wyyrnlsvgkeufq7ia4l4k7pbr6rgloiedh.py", line 984, in call
    buf1 = empty_strided_cuda((64, 1024, 50304), (51511296, 50304, 1), torch.bfloat16)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 6.14 GiB. GPU 0 has a total capacity of 21.96 GiB of which 5.38 GiB is free. Process 77592 has 16.58 GiB memory in use. Of the allocated memory 14.04 GiB is allocated by PyTorch, and 2.27 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "/teamspace/studios/this_studio/nanoGPT/train.py", line 853, in <module>
    scaler.scale(loss).backward()
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/_tensor.py", line 648, in backward
    torch.autograd.backward(
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/autograd/__init__.py", line 353, in backward
    _engine_run_backward(
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/autograd/graph.py", line 824, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/autograd/function.py", line 307, in apply
    return user_fn(self, *args)
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 2111, in backward
    return impl_fn()
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 2097, in impl_fn
    out = CompiledFunction._backward_impl(ctx, all_args)
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 2217, in _backward_impl
    out = call_func_at_runtime_with_args(
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/_functorch/_aot_autograd/utils.py", line 126, in call_func_at_runtime_with_args
    out = normalize_as_list(f(args))
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py", line 838, in _fn
    return fn(*args, **kwargs)
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/_inductor/output_code.py", line 460, in __call__
    return self.current_callable(inputs)
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/_inductor/utils.py", line 2404, in run
    return model(new_inputs)
  File "/tmp/torchinductor_adamskrodzki/5v/c5vfqg2x37e5rudtf2wyyrnlsvgkeufq7ia4l4k7pbr6rgloiedh.py", line 984, in call
    buf1 = empty_strided_cuda((64, 1024, 50304), (51511296, 50304, 1), torch.bfloat16)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 6.14 GiB. GPU 0 has a total capacity of 21.96 GiB of which 5.38 GiB is free. Process 77592 has 16.58 GiB memory in use. Of the allocated memory 14.04 GiB is allocated by PyTorch, and 2.27 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
