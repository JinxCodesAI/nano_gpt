2025-07-20 18:09:31,328 INFO    MainThread:305923 [wandb_setup.py:_flush():80] Current SDK version is 0.21.0
2025-07-20 18:09:31,329 INFO    MainThread:305923 [wandb_setup.py:_flush():80] Configure stats pid to 305923
2025-07-20 18:09:31,329 INFO    MainThread:305923 [wandb_setup.py:_flush():80] Loading settings from /teamspace/studios/this_studio/.config/wandb/settings
2025-07-20 18:09:31,329 INFO    MainThread:305923 [wandb_setup.py:_flush():80] Loading settings from /teamspace/studios/this_studio/nanoGPT/wandb/settings
2025-07-20 18:09:31,329 INFO    MainThread:305923 [wandb_setup.py:_flush():80] Loading settings from environment variables
2025-07-20 18:09:31,329 INFO    MainThread:305923 [wandb_init.py:setup_run_log_directory():703] Logging user logs to /teamspace/studios/this_studio/nanoGPT/wandb/run-20250720_180931-8ikvg5kb/logs/debug.log
2025-07-20 18:09:31,329 INFO    MainThread:305923 [wandb_init.py:setup_run_log_directory():704] Logging internal logs to /teamspace/studios/this_studio/nanoGPT/wandb/run-20250720_180931-8ikvg5kb/logs/debug-internal.log
2025-07-20 18:09:31,329 INFO    MainThread:305923 [wandb_init.py:init():830] calling init triggers
2025-07-20 18:09:31,329 INFO    MainThread:305923 [wandb_init.py:init():835] wandb.init called with sweep_config: {}
config: {'out_dir': 'out', 'eval_interval': 200, 'log_interval': 10, 'eval_iters': 10, 'eval_only': False, 'always_save_checkpoint': False, 'init_from': 'resume', 'log_dir': 'logs', 'file_logging': True, 'wandb_log': True, 'wandb_project': 'owt', 'wandb_run_name': 'gpt2', 'dataset': 'fineweb10B', 'gradient_accumulation_steps': 1, 'batch_size': 32, 'block_size': 1024, 'n_layer': 12, 'n_head': 12, 'n_embd': 768, 'dropout': 0.0, 'bias': False, 'use_rotary_embeddings': True, 'rotary_base': 10000.0, 'rotary_max_position_embeddings': 2048, 'learning_rate': 0.001, 'max_iters': 600000, 'weight_decay': 0.1, 'beta1': 0.9, 'beta2': 0.95, 'grad_clip': 1.0, 'decay_lr': True, 'warmup_iters': 200, 'lr_decay_iters': 600000, 'min_lr': 6e-05, 'backend': 'nccl', 'device': 'cuda', 'dtype': 'bfloat16', 'compile': True, 'embedding_mode': 'lora', 'attn_lora_rank': 48, 'embedding_rank': 48, 'lora_alpha': 1.0, 'attn_lora_rank_divisor': 16, 'vocab_lora_rank_divisor': 16, 'lora_alpha_multiplier': 1.0, 'n_layer_divisor': 4, 'n_hidden_divisor': 4, 'batch_size_multiplier': 1.0, 'grad_accum_multiplier': 1.0, 'lr_multiplier': 0.5, 'warmup_iters_multiplier': 1.0, 'eval_iters_multiplier': 1.0, 'eval_interval_multiplier': 1.0, '_wandb': {}}
2025-07-20 18:09:31,329 INFO    MainThread:305923 [wandb_init.py:init():871] starting backend
2025-07-20 18:09:31,537 INFO    MainThread:305923 [wandb_init.py:init():874] sending inform_init request
2025-07-20 18:09:31,541 INFO    MainThread:305923 [wandb_init.py:init():882] backend started and connected
2025-07-20 18:09:31,544 INFO    MainThread:305923 [wandb_init.py:init():953] updated telemetry
2025-07-20 18:09:31,558 INFO    MainThread:305923 [wandb_init.py:init():977] communicating run to backend with 90.0 second timeout
2025-07-20 18:09:31,816 INFO    MainThread:305923 [wandb_init.py:init():1029] starting run threads in backend
2025-07-20 18:09:32,028 INFO    MainThread:305923 [wandb_run.py:_console_start():2458] atexit reg
2025-07-20 18:09:32,028 INFO    MainThread:305923 [wandb_run.py:_redirect():2306] redirect: wrap_raw
2025-07-20 18:09:32,028 INFO    MainThread:305923 [wandb_run.py:_redirect():2375] Wrapping output streams.
2025-07-20 18:09:32,029 INFO    MainThread:305923 [wandb_run.py:_redirect():2398] Redirects installed.
2025-07-20 18:09:32,031 INFO    MainThread:305923 [wandb_init.py:init():1075] run started, returning control to user process
