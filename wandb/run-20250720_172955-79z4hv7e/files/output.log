Calculating target architecture based on schedule...

============================================================
       TARGET MODEL ARCHITECTURE (at end of schedule)
============================================================
  n_layer                | 6
  n_head                 | 12
  n_embd                 | 768
  n_hidden               | 1536
  block_size             | 1024
  vocab_size             | 50304
  dropout                | 0.0
  bias                   | False
  embedding_mode         | standard
  attn_lora_rank         | 0
  embedding_rank         | 0
  lora_alpha             | 0.0
============================================================


============================================================
           INITIAL MODEL ARCHITECTURE (at Iter 0)
============================================================
  n_layer                | 3
  n_head                 | 12
  n_embd                 | 768
  n_hidden               | 768
  block_size             | 1024
  vocab_size             | 50304
  dropout                | 0.0
  bias                   | False
  embedding_mode         | lora
  attn_lora_rank         | 48
  embedding_rank         | 48
  lora_alpha             | 1.0
============================================================


Detailed parameter count:
  total                  | Total:   52,149,504 | Trainable:    8,207,616
  token_embeddings       | Total:    2,451,456 | Trainable:    2,451,456
  position_embeddings    | Total:            0 | Trainable:            0
  attention_layers       | Total:    7,520,256 | Trainable:    2,211,840
  feed_forward_layers    | Total:    3,538,944 | Trainable:    3,538,944
  layer_norms            | Total:        4,608 | Trainable:        4,608
  final_layer_norm       | Total:          768 | Trainable:          768
  language_model_head    | Total:   38,633,472 | Trainable:            0
------------------------------------------------------------
eval every:100
step 900: train loss 7.3362, val loss 7.3387
  MLP Rank Utilization (L0): 69.27% (532/768)
Attn LoRA    | 5/48            | 10.42%
  MLP Rank Utilization (L1): 57.68% (443/768)
Attn LoRA    | 2/48            | 4.17%
  MLP Rank Utilization (L2): 56.38% (433/768)
Attn LoRA    | 2/48            | 4.17%
--- Model Analysis ---
  Embedding Utilization (L2): 97.53% (749/768)
  Average Attention Entropy:  4.4354
----------------------
Embed LoRA   | 3/48            | 6.25%
saving checkpoint to out
merge_lora_weights 7.338700771331787 4.0 10500
iter 900: loss 7.3965, lr 0.00050, time 12698.85ms, mfu -100.00%
iter 910: loss 7.2680, lr 0.00050, time 2293.35ms, mfu 32.23%
iter 920: loss 7.2387, lr 0.00050, time 2328.69ms, mfu 32.18%
iter 930: loss 7.3081, lr 0.00050, time 2320.47ms, mfu 32.15%
iter 940: loss 7.1350, lr 0.00050, time 2311.54ms, mfu 32.13%
iter 950: loss 7.3030, lr 0.00050, time 2298.08ms, mfu 32.14%
iter 960: loss 7.2377, lr 0.00050, time 2316.91ms, mfu 32.11%
iter 970: loss 7.1882, lr 0.00050, time 2320.43ms, mfu 32.09%
iter 980: loss 7.1222, lr 0.00050, time 2318.92ms, mfu 32.07%
iter 990: loss 7.2054, lr 0.00050, time 2317.31ms, mfu 32.05%
step 1000: train loss 7.2361, val loss 7.1771
  MLP Rank Utilization (L0): 69.53% (534/768)
Attn LoRA    | 5/48            | 10.42%
  MLP Rank Utilization (L1): 57.81% (444/768)
Attn LoRA    | 2/48            | 4.17%
  MLP Rank Utilization (L2): 56.51% (434/768)
Attn LoRA    | 2/48            | 4.17%
--- Model Analysis ---
  Embedding Utilization (L2): 97.53% (749/768)
  Average Attention Entropy:  4.4898
----------------------
Embed LoRA   | 3/48            | 6.25%
saving checkpoint to out
merge_lora_weights 7.177081108093262 4.0 10500
iter 1000: loss 7.2683, lr 0.00050, time 6679.33ms, mfu 29.95%
iter 1010: loss 7.2765, lr 0.00050, time 2315.78ms, mfu 30.15%
iter 1020: loss 7.0794, lr 0.00050, time 2324.50ms, mfu 30.31%
iter 1030: loss 7.1330, lr 0.00050, time 2314.59ms, mfu 30.48%
iter 1040: loss 7.2400, lr 0.00050, time 2313.78ms, mfu 30.62%
iter 1050: loss 7.1877, lr 0.00050, time 2315.29ms, mfu 30.75%
iter 1060: loss 7.1540, lr 0.00050, time 2317.98ms, mfu 30.87%
iter 1070: loss 7.1604, lr 0.00050, time 2317.50ms, mfu 30.97%
iter 1080: loss 7.1877, lr 0.00050, time 2315.04ms, mfu 31.07%
iter 1090: loss 7.1996, lr 0.00050, time 2315.88ms, mfu 31.15%
step 1100: train loss 7.1578, val loss 7.1548
  MLP Rank Utilization (L0): 69.79% (536/768)
Attn LoRA    | 5/48            | 10.42%
  MLP Rank Utilization (L1): 58.20% (447/768)
Attn LoRA    | 2/48            | 4.17%
  MLP Rank Utilization (L2): 56.77% (436/768)
Attn LoRA    | 2/48            | 4.17%
--- Model Analysis ---
  Embedding Utilization (L2): 97.53% (749/768)
  Average Attention Entropy:  4.4936
----------------------
Embed LoRA   | 3/48            | 6.25%
saving checkpoint to out
merge_lora_weights 7.154768466949463 4.0 10500
iter 1100: loss 7.1121, lr 0.00050, time 6527.01ms, mfu 29.17%
iter 1110: loss 7.3023, lr 0.00050, time 2317.86ms, mfu 29.44%
iter 1120: loss 7.1006, lr 0.00050, time 2319.75ms, mfu 29.68%
iter 1130: loss 7.1136, lr 0.00050, time 2318.03ms, mfu 29.90%
iter 1140: loss 7.1728, lr 0.00050, time 2317.43ms, mfu 30.10%
iter 1150: loss 7.1831, lr 0.00050, time 2318.17ms, mfu 30.28%
iter 1160: loss 7.1359, lr 0.00050, time 2316.80ms, mfu 30.45%
iter 1170: loss 7.2245, lr 0.00050, time 2317.10ms, mfu 30.59%
iter 1180: loss 7.0300, lr 0.00050, time 2316.00ms, mfu 30.72%
iter 1190: loss 7.0551, lr 0.00050, time 2315.33ms, mfu 30.84%
step 1200: train loss 7.1313, val loss 7.0554
  MLP Rank Utilization (L0): 69.79% (536/768)
Attn LoRA    | 5/48            | 10.42%
  MLP Rank Utilization (L1): 58.46% (449/768)
Attn LoRA    | 3/48            | 6.25%
  MLP Rank Utilization (L2): 57.16% (439/768)
Attn LoRA    | 2/48            | 4.17%
--- Model Analysis ---
  Embedding Utilization (L2): 97.53% (749/768)
  Average Attention Entropy:  4.4867
----------------------
Embed LoRA   | 3/48            | 6.25%
saving checkpoint to out
merge_lora_weights 7.055440425872803 4.0 10500
iter 1200: loss 7.0785, lr 0.00050, time 6686.75ms, mfu 28.87%
iter 1210: loss 6.9425, lr 0.00050, time 2320.68ms, mfu 29.16%
iter 1220: loss 7.2476, lr 0.00050, time 2327.74ms, mfu 29.42%
Traceback (most recent call last):
  File "/teamspace/studios/this_studio/nanoGPT/train.py", line 873, in <module>
    if ddp:
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/_tensor.py", line 648, in backward
    torch.autograd.backward(
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/autograd/__init__.py", line 353, in backward
    _engine_run_backward(
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/autograd/graph.py", line 824, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
KeyboardInterrupt
Traceback (most recent call last):
  File "/teamspace/studios/this_studio/nanoGPT/train.py", line 873, in <module>
    if ddp:
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/_tensor.py", line 648, in backward
    torch.autograd.backward(
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/autograd/__init__.py", line 353, in backward
    _engine_run_backward(
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/autograd/graph.py", line 824, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
KeyboardInterrupt
Exception ignored in atexit callback: <function _start_and_connect_service.<locals>.teardown_atexit at 0x7a395fa72050>
Traceback (most recent call last):
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/wandb/sdk/lib/service/service_connection.py", line 54, in teardown_atexit
    conn.teardown(hooks.exit_code)
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/wandb/sdk/lib/service/service_connection.py", line 182, in teardown
    self._router.join()
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/wandb/sdk/interface/router.py", line 75, in join
    self._thread.join()
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/threading.py", line 1096, in join
    self._wait_for_tstate_lock()
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/threading.py", line 1116, in _wait_for_tstate_lock
    if lock.acquire(block, timeout):
KeyboardInterrupt:
Exception ignored in atexit callback: <function shutdown_compile_workers at 0x7a39a8345e10>
Traceback (most recent call last):
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/_inductor/async_compile.py", line 113, in shutdown_compile_workers
    pool.shutdown()
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/_inductor/compile_worker/subproc_pool.py", line 239, in shutdown
    self.process.wait(300)
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/subprocess.py", line 1209, in wait
    return self._wait(timeout=timeout)
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/subprocess.py", line 1937, in _wait
    time.sleep(delay)
KeyboardInterrupt:
