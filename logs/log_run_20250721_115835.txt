================================================================================
Training run started at 2025-07-21 11:58:35
================================================================================
Configuration:
----------------------------------------
always_save_checkpoint: True
attn_lora_rank: 0
attn_lora_rank_divisor: 0
backend: nccl
batch_size: 1
batch_size_multiplier: 1.0
beta1: 0.9
beta2: 0.95
bias: False
block_size: 128
compile: False
dataset: shakespeare_char
decay_lr: True
device: cpu
dropout: 0.0
dtype: float16
embedding_mode: standard
embedding_rank: 0
eval_interval: 10
eval_interval_multiplier: 1.0
eval_iters: 5
eval_iters_multiplier: 1.0
eval_only: False
file_logging: True
grad_accum_multiplier: 1.0
grad_clip: 1.0
gradient_accumulation_steps: 1
init_from: scratch
learning_rate: 0.006
log_dir: logs
log_interval: 1
lora_alpha: 1.0
lora_alpha_multiplier: 1.0
lr_decay_iters: 600000
lr_multiplier: 1.0
max_iters: 600000
min_lr: 6e-05
n_embd: 16
n_head: 2
n_hidden_divisor: 1
n_layer: 2
n_layer_divisor: 1
num_train_shards: 1
out_dir: out_resume
rotary_base: 10000.0
rotary_max_position_embeddings: 2048
use_rotary_embeddings: False
vocab_lora_rank_divisor: 0
wandb_log: False
wandb_project: owt
wandb_run_name: gpt2
warmup_iters: 10
warmup_iters_multiplier: 1.0
weight_decay: 0.1
----------------------------------------
[2025-07-21 11:58:35] step 0: train loss 4.1732, val loss 4.1720
[2025-07-21 11:58:35] step 10: train loss 3.8767, val loss 3.8371
[2025-07-21 11:58:36] step 20: train loss 3.4285, val loss 3.4406
[2025-07-21 11:58:36] step 30: train loss 3.3843, val loss 3.3827
[2025-07-21 11:58:36] step 40: train loss 3.3693, val loss 3.3691
[2025-07-21 11:58:36] step 50: train loss 3.2230, val loss 3.2813
[2025-07-21 11:58:36] step 60: train loss 3.1936, val loss 3.1143
[2025-07-21 11:58:36] step 70: train loss 3.1215, val loss 3.1672
[2025-07-21 11:58:36] step 80: train loss 3.1294, val loss 3.1550
[2025-07-21 11:58:36] step 90: train loss 3.2015, val loss 3.1154
[2025-07-21 11:58:36] step 100: train loss 2.9804, val loss 3.1661
[2025-07-21 11:58:36] step 110: train loss 3.0785, val loss 3.0399
[2025-07-21 11:58:36] step 120: train loss 2.9687, val loss 2.9150
[2025-07-21 11:58:36] step 130: train loss 2.8501, val loss 2.9330
[2025-07-21 11:58:36] step 140: train loss 2.9607, val loss 2.9781
[2025-07-21 11:58:37] step 150: train loss 2.8998, val loss 2.9047
[2025-07-21 11:58:37] step 160: train loss 2.8425, val loss 2.9994
[2025-07-21 11:58:37] step 170: train loss 2.8779, val loss 2.8924
[2025-07-21 11:58:37] step 180: train loss 2.8579, val loss 2.8701
[2025-07-21 11:58:37] step 190: train loss 2.8893, val loss 2.8472
[2025-07-21 11:58:37] step 200: train loss 2.7819, val loss 2.8025
[2025-07-21 11:58:37] step 210: train loss 2.9246, val loss 2.9202
[2025-07-21 11:58:37] step 220: train loss 2.9208, val loss 2.7950
[2025-07-21 11:58:37] step 230: train loss 2.8204, val loss 2.8132
[2025-07-21 11:58:37] step 240: train loss 2.7430, val loss 2.8153
[2025-07-21 11:58:37] step 250: train loss 2.7468, val loss 2.7455
[2025-07-21 11:58:37] step 260: train loss 2.7468, val loss 2.7918
[2025-07-21 11:58:37] step 270: train loss 2.7192, val loss 2.6879
[2025-07-21 11:58:38] step 280: train loss 2.6763, val loss 2.7508
[2025-07-21 11:58:38] step 290: train loss 2.7765, val loss 2.7280
[2025-07-21 11:58:38] step 300: train loss 2.7476, val loss 2.6947
[2025-07-21 11:58:38] step 310: train loss 2.7096, val loss 2.8750
[2025-07-21 11:58:38] step 320: train loss 2.6882, val loss 2.7533
[2025-07-21 11:58:38] step 330: train loss 2.8256, val loss 2.7073
[2025-07-21 11:58:38] step 340: train loss 2.7081, val loss 2.7106
[2025-07-21 11:58:38] step 350: train loss 2.6827, val loss 2.5706
[2025-07-21 11:58:38] step 360: train loss 2.6602, val loss 2.6153
[2025-07-21 11:58:38] step 370: train loss 2.7734, val loss 2.7107
[2025-07-21 11:58:38] step 380: train loss 2.7105, val loss 2.7026
[2025-07-21 11:58:39] step 390: train loss 2.6394, val loss 2.6844
[2025-07-21 11:58:39] step 400: train loss 2.6215, val loss 2.6827
[2025-07-21 11:58:39] step 410: train loss 2.7227, val loss 2.5547
[2025-07-21 11:58:39] step 420: train loss 2.6461, val loss 2.7651
[2025-07-21 11:58:39] step 430: train loss 2.6102, val loss 2.6499
[2025-07-21 11:58:39] step 440: train loss 2.8139, val loss 2.6532
[2025-07-21 11:58:39] step 450: train loss 2.7063, val loss 2.7689
[2025-07-21 11:58:39] step 460: train loss 2.6264, val loss 2.7527
[2025-07-21 11:58:39] step 470: train loss 2.6110, val loss 2.6931
[2025-07-21 11:58:39] step 480: train loss 2.7630, val loss 2.6851
[2025-07-21 11:58:40] step 490: train loss 2.6407, val loss 2.6083
[2025-07-21 11:58:40] step 500: train loss 2.7223, val loss 2.6729
[2025-07-21 11:58:40] OPERATION_START: merge_lora_weights first burn | iter=500 | value=None | trigger=Timeout | val_loss=2.6729 | trigger_loss=1.5000 | max_wait=500
[2025-07-21 11:58:40] OPERATION_SUCCESS: merge_lora_weights | iter=500 | status=merged
[2025-07-21 11:58:40] OPERATION_START: change_lr  | iter=500 | value=0.7 | trigger=Loss threshold | val_loss=2.6729 | trigger_loss=100.0000 | max_wait=1
[2025-07-21 11:58:40] OPERATION_SUCCESS: change_lr | iter=500 | old=1.0 | new=0.7
[2025-07-21 11:58:40] step 510: train loss 2.6452, val loss 2.7236
[2025-07-21 11:58:40] step 520: train loss 2.6108, val loss 2.7443
[2025-07-21 11:58:40] step 530: train loss 2.7219, val loss 2.6989
[2025-07-21 11:58:40] step 540: train loss 2.5176, val loss 2.6675
[2025-07-21 11:58:40] step 550: train loss 2.7207, val loss 2.5632
[2025-07-21 11:58:40] step 560: train loss 2.5544, val loss 2.6716
[2025-07-21 11:58:40] step 570: train loss 2.5704, val loss 2.6361
[2025-07-21 11:58:40] step 580: train loss 2.6031, val loss 2.6875
[2025-07-21 11:58:40] step 590: train loss 2.5680, val loss 2.6351
[2025-07-21 11:58:41] step 600: train loss 2.5909, val loss 2.6073
[2025-07-21 11:58:41] step 610: train loss 2.6175, val loss 2.6384
[2025-07-21 11:58:41] step 620: train loss 2.6263, val loss 2.6135
[2025-07-21 11:58:41] step 630: train loss 2.5285, val loss 2.6418
[2025-07-21 11:58:41] step 640: train loss 2.6819, val loss 2.5590
[2025-07-21 11:58:41] step 650: train loss 2.6300, val loss 2.6359
[2025-07-21 11:58:41] step 660: train loss 2.5879, val loss 2.5400
[2025-07-21 11:58:41] step 670: train loss 2.6606, val loss 2.5298
[2025-07-21 11:58:41] step 680: train loss 2.5889, val loss 2.4950
[2025-07-21 11:58:41] step 690: train loss 2.6076, val loss 2.5924
[2025-07-21 11:58:41] step 700: train loss 2.5923, val loss 2.6487
[2025-07-21 11:58:41] step 710: train loss 2.5194, val loss 2.6028
[2025-07-21 11:58:42] step 720: train loss 2.6266, val loss 2.6247
[2025-07-21 11:58:42] step 730: train loss 2.6462, val loss 2.5478
[2025-07-21 11:58:42] step 740: train loss 2.6034, val loss 2.6017
[2025-07-21 11:58:42] step 750: train loss 2.6294, val loss 2.6210
[2025-07-21 11:58:42] step 760: train loss 2.5945, val loss 2.5157
[2025-07-21 11:58:42] step 770: train loss 2.5557, val loss 2.5833
[2025-07-21 11:58:42] step 780: train loss 2.5544, val loss 2.5900
[2025-07-21 11:58:42] step 790: train loss 2.5294, val loss 2.5904
[2025-07-21 11:58:42] step 800: train loss 2.4906, val loss 2.5671
[2025-07-21 11:58:42] step 810: train loss 2.6189, val loss 2.6521
[2025-07-21 11:58:42] step 820: train loss 2.6321, val loss 2.5809
[2025-07-21 11:58:43] step 830: train loss 2.4802, val loss 2.5436
[2025-07-21 11:58:43] step 840: train loss 2.6118, val loss 2.5535
