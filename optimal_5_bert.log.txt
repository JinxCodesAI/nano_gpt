Overriding config with .\config\shkspr_char_diff\optimal5.py:
out_dir = 'out'
init_from = 'scratch' # 'scratch' or 'resume'
wandb_log = True # disabled by default
wandb_project = 'experiments_diffusion'
wandb_run_name = 'shkspr_char_diff_moderate_first' # 'run' + str(time.time())
batch_size = 16
gradient_accumulation_steps = 8
# data
dataset = 'shakespeare_char'
use_paragraph_boundaries = False # if True, start samples at paragraph boundaries (double newlines)
# diffusion training config
training_type = 'unmasking'  # 'unmasking', 'remasking', or 'remasking_binary' - type of training
use_all_stages_for_training = True
weight_loss_by_mask_ratio = True
enable_entropy_penalty = False
uncertainty_factor = 0.1

# For unmasking: stage-based training with direct probability control

unmasking_stages = [
    {'type':'sticky','target_masked_ratio': 0.4, 'p1_probability': 0.15, 'p2_probability': 0.3, 'val_loss_stale_count': 6},
    {'type':'sticky','target_masked_ratio': 0.6, 'p1_probability': 0.1, 'p2_probability': 0.5, 'val_loss_stale_count': 8},
    {'type':'random','max_masked_ratio': 0.5, 'val_loss_stale_count': 10},
    {'type':'sticky','target_masked_ratio': 0.6, 'p1_probability': 0.3, 'p2_probability': 0.1, 'val_loss_stale_count': 8},
    {'type':'sticky','target_masked_ratio': 0.6, 'p1_probability': 0.1, 'p2_probability': 0.5, 'val_loss_stale_count': 8},
    {'type':'random','max_masked_ratio': 0.2, 'val_loss_stale_count': 10},
    {'type':'sticky','target_masked_ratio': 0.55, 'p1_probability': 0.1, 'p2_probability': 0.6, 'val_loss_stale_count': 10},
    {'type':'sticky','target_masked_ratio': 0.9, 'p1_probability': 0.1, 'p2_probability': 0.9, 'val_loss_stale_count': 20},
]


validation_stages = [
    {'type':'sticky','target_masked_ratio': 0.4, 'p1_probability': 0.15, 'p2_probability': 0.3, 'val_loss_stale_count': 6},
    {'type':'sticky','target_masked_ratio': 0.6, 'p1_probability': 0.1, 'p2_probability': 0.5, 'val_loss_stale_count': 8},
    {'type':'random','max_masked_ratio': 0.5, 'val_loss_stale_count': 10},
    {'type':'sticky','target_masked_ratio': 0.6, 'p1_probability': 0.3, 'p2_probability': 0.1, 'val_loss_stale_count': 8},
    {'type':'sticky','target_masked_ratio': 0.6, 'p1_probability': 0.1, 'p2_probability': 0.5, 'val_loss_stale_count': 8},
    {'type':'random','max_masked_ratio': 0.2, 'val_loss_stale_count': 10},
    {'type':'sticky','target_masked_ratio': 0.2, 'p1_probability': 0.3, 'p2_probability': 0.0, 'val_loss_stale_count': 2},
    {'type':'sticky','target_masked_ratio': 0.4, 'p1_probability': 0.3, 'p2_probability': 0.0, 'val_loss_stale_count': 4},
    {'type':'sticky','target_masked_ratio': 0.4, 'p1_probability': 0.15, 'p2_probability': 0.3, 'val_loss_stale_count': 6},
    {'type':'sticky','target_masked_ratio': 0.55, 'p1_probability': 0.1, 'p2_probability': 0.6, 'val_loss_stale_count': 10},
    {'type':'sticky','target_masked_ratio': 0.7, 'p1_probability': 0.2, 'p2_probability': 0.4, 'val_loss_stale_count': 15},
    {'type':'sticky','target_masked_ratio': 0.8, 'p1_probability': 0.2, 'p2_probability': 0.4, 'val_loss_stale_count': 20},
    {'type':'sticky','target_masked_ratio': 0.8, 'p1_probability': 0.1, 'p2_probability': 0.9, 'val_loss_stale_count': 20},
    {'type':'sticky','target_masked_ratio': 0.9, 'p1_probability': 0.1, 'p2_probability': 0.9, 'val_loss_stale_count': 20},
]

# adamw optimizer
learning_rate = 2e-4 # with baby networks can afford to go a bit higher
max_iters = 8000
warmup_iters = 2000 # how many steps to warm up for
lr_decay_iters = 15000 # make equal to max_iters usually
min_lr = 3e-5 # learning_rate / 10 usually
weight_decay=2e-2
dropout = 0.2 # for pretraining 0 is good, for finetuning try 0.1+

grad_clip = 0.0  # clip gradients at this value, or disable if == 0.0
# learning rate decay settings
decay_lr = True # it's just experiment, no need to decay

max_entropy_penalty = 3 # loss = loss * (1 + current_entropy_penalty * wrong_answers_entropy)

entropy_penalty_start_iter = 2500 # start increasing entropy penalty after this many iterations

================================================================================
SOURCE CODE:
================================================================================

--- model.py ---
"""
Full definition of a GPT Language Model, all of it in this single file.
References:
1) the official GPT-2 TensorFlow implementation released by OpenAI:
https://github.com/openai/gpt-2/blob/master/src/model.py
2) huggingface/transformers PyTorch implementation:
https://github.com/huggingface/transformers/blob/main/src/transformers/models/gpt2/modeling_gpt2.py
"""

import math
import inspect
from dataclasses import dataclass

import torch
import torch.nn as nn
from torch.nn import functional as F

class RotaryPositionalEmbedding(nn.Module):
    """
    Rotary Positional Embedding (RoPE) as described in:
    "RoFormer: Enhanced Transformer with Rotary Position Embedding"
    https://arxiv.org/abs/2104.09864
    """
    
    def __init__(self, dim, max_position_embeddings=2048, base=10000, device=None):
        super().__init__()
        self.dim = dim
        self.max_position_embeddings = max_position_embeddings
        self.base = base
        
        # Create frequency bands
        inv_freq = 1.0 / (self.base ** (torch.arange(0, self.dim, 2).float().to(device) / self.dim))
        self.register_buffer("inv_freq", inv_freq, persistent=False)
        
        # Build here to make `torch.jit.trace` work.
        self._set_cos_sin_cache(
            seq_len=max_position_embeddings, device=self.inv_freq.device, dtype=torch.get_default_dtype()
        )

    def _set_cos_sin_cache(self, seq_len, device, dtype):
        self.max_seq_len_cached = seq_len
        t = torch.arange(self.max_seq_len_cached, device=device, dtype=self.inv_freq.dtype)
        
        freqs = torch.einsum("i,j->ij", t, self.inv_freq)
        # Different from paper, but it uses a different permutation in order to obtain the same calculation
        emb = torch.cat((freqs, freqs), dim=-1)
        self.register_buffer("cos_cached", emb.cos().to(dtype), persistent=False)
        self.register_buffer("sin_cached", emb.sin().to(dtype), persistent=False)

    def forward(self, x, seq_len=None):
        # x: [bs, num_attention_heads, seq_len, head_size]
        if seq_len > self.max_seq_len_cached:
            self._set_cos_sin_cache(seq_len=seq_len, device=x.device, dtype=x.dtype)

        return (
            self.cos_cached[:seq_len].to(dtype=x.dtype),
            self.sin_cached[:seq_len].to(dtype=x.dtype),
        )

def rotate_half(x):
    """Rotates half the hidden dims of the input."""
    x1 = x[..., : x.shape[-1] // 2]
    x2 = x[..., x.shape[-1] // 2 :]
    return torch.cat((-x2, x1), dim=-1)

def apply_rotary_pos_emb(q, k, cos, sin, position_ids=None):
    """Applies Rotary Position Embedding to the query and key tensors."""
    cos = cos[position_ids].unsqueeze(1)  # [seq_len, 1, dim]
    sin = sin[position_ids].unsqueeze(1)  # [seq_len, 1, dim]
    q_embed = (q * cos) + (rotate_half(q) * sin)
    k_embed = (k * cos) + (rotate_half(k) * sin)
    return q_embed, k_embed

class LayerNorm(nn.Module):
    """ LayerNorm but with an optional bias. PyTorch doesn't support simply bias=False """

    def __init__(self, ndim, bias):
        super().__init__()
        self.weight = nn.Parameter(torch.ones(ndim))
        self.bias = nn.Parameter(torch.zeros(ndim)) if bias else None

    def forward(self, input):
        return F.layer_norm(input, self.weight.shape, self.weight, self.bias, 1e-5)

class CausalSelfAttention(nn.Module):
    """Causal self-attention with optional flash attention and RoPE"""

    def __init__(self, config):
        super().__init__()
        assert config.n_embd % config.n_head == 0
        # key, query, value projections for all heads, but in a batch
        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd, bias=config.bias)
        # output projection
        self.c_proj = nn.Linear(config.n_embd, config.n_embd, bias=config.bias)
        # regularization
        self.attn_dropout = nn.Dropout(config.dropout)
        self.resid_dropout = nn.Dropout(config.dropout)
        self.n_head = config.n_head
        self.n_embd = config.n_embd
        self.dropout = config.dropout
        
        # Rotary positional embeddings
        self.head_dim = config.n_embd // config.n_head
        use_rope = getattr(config, 'use_rope', True)
        if use_rope:
            self.rotary_emb = RotaryPositionalEmbedding(
                self.head_dim, 
                max_position_embeddings=config.block_size,
                device=None  # Will be set when model is moved to device
            )
        else:
            self.rotary_emb = None

        # flash attention make GPU go brrrrr but support is only in PyTorch >= 2.0
        self.flash = hasattr(torch.nn.functional, 'scaled_dot_product_attention')
        if not self.flash:
            print("WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0")
            # causal mask to ensure that attention is only applied to the left in the input sequence
            self.register_buffer("bias", torch.tril(torch.ones(config.block_size, config.block_size))
                                        .view(1, 1, config.block_size, config.block_size))

    def forward(self, x):
        B, T, C = x.size() # batch size, sequence length, embedding dimensionality (n_embd)

        # calculate query, key, values for all heads in batch and move head forward to be the batch dim
        q, k, v  = self.c_attn(x).split(self.n_embd, dim=2)
        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)
        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)
        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)

        # Apply rotary positional embeddings if available
        if self.rotary_emb is not None:
            cos, sin = self.rotary_emb(v, seq_len=T)
            position_ids = torch.arange(T, device=x.device).unsqueeze(0)
            q, k = apply_rotary_pos_emb(q, k, cos, sin, position_ids)

        # causal self-attention; Self-attend: (B, nh, T, hs) x (B, nh, hs, T) -> (B, nh, T, T)
        if self.flash:
            # efficient attention using Flash Attention CUDA kernels
            y = torch.nn.functional.scaled_dot_product_attention(q, k, v, attn_mask=None, dropout_p=self.dropout if self.training else 0, is_causal=True)
        else:
            # manual implementation of attention
            att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))
            att = att.masked_fill(self.bias[:,:,:T,:T] == 0, float('-inf'))
            att = F.softmax(att, dim=-1)
            att = self.attn_dropout(att)
            y = att @ v # (B, nh, T, T) x (B, nh, T, hs) -> (B, nh, T, hs)
        y = y.transpose(1, 2).contiguous().view(B, T, C) # re-assemble all head outputs side by side

        # output projection
        y = self.resid_dropout(self.c_proj(y))
        return y

class BidirectionalSelfAttention(nn.Module):
    """Bidirectional self-attention with optional flash attention and RoPE"""

    def __init__(self, config):
        super().__init__()
        assert config.n_embd % config.n_head == 0
        # key, query, value projections for all heads, but in a batch
        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd, bias=config.bias)
        # output projection
        self.c_proj = nn.Linear(config.n_embd, config.n_embd, bias=config.bias)
        # regularization
        self.attn_dropout = nn.Dropout(config.dropout)
        self.resid_dropout = nn.Dropout(config.dropout)
        self.n_head = config.n_head
        self.n_embd = config.n_embd
        self.dropout = config.dropout
        
        # Rotary positional embeddings
        self.head_dim = config.n_embd // config.n_head
        use_rope = getattr(config, 'use_rope', True)
        if use_rope:
            self.rotary_emb = RotaryPositionalEmbedding(
                self.head_dim, 
                max_position_embeddings=config.block_size,
                device=None  # Will be set when model is moved to device
            )
        else:
            self.rotary_emb = None

        # flash attention make GPU go brrrrr but support is only in PyTorch >= 2.0
        self.flash = hasattr(torch.nn.functional, 'scaled_dot_product_attention')
        if not self.flash:
            print("WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0")

    def forward(self, x):
        B, T, C = x.size() # batch size, sequence length, embedding dimensionality (n_embd)

        # calculate query, key, values for all heads in batch and move head forward to be the batch dim
        q, k, v  = self.c_attn(x).split(self.n_embd, dim=2)
        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)
        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)
        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)

        # Apply rotary positional embeddings if available
        if self.rotary_emb is not None:
            cos, sin = self.rotary_emb(v, seq_len=T)
            position_ids = torch.arange(T, device=x.device).unsqueeze(0)
            q, k = apply_rotary_pos_emb(q, k, cos, sin, position_ids)

        # bidirectional self-attention; Self-attend: (B, nh, T, hs) x (B, nh, hs, T) -> (B, nh, T, T)
        if self.flash:
            # efficient attention using Flash Attention CUDA kernels
            y = torch.nn.functional.scaled_dot_product_attention(q, k, v, attn_mask=None, dropout_p=self.dropout if self.training else 0, is_causal=False)
        else:
            # manual implementation of attention
            att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))
            # No masking for bidirectional attention
            att = F.softmax(att, dim=-1)
            att = self.attn_dropout(att)
            y = att @ v # (B, nh, T, T) x (B, nh, T, hs) -> (B, nh, T, hs)
        y = y.transpose(1, 2).contiguous().view(B, T, C) # re-assemble all head outputs side by side

        # output projection
        y = self.resid_dropout(self.c_proj(y))
        return y

class MLP(nn.Module):

    def __init__(self, config):
        super().__init__()
        self.c_fc    = nn.Linear(config.n_embd, 4 * config.n_embd, bias=config.bias)
        self.gelu    = nn.GELU()
        self.c_proj  = nn.Linear(4 * config.n_embd, config.n_embd, bias=config.bias)
        self.dropout = nn.Dropout(config.dropout)

    def forward(self, x):
        x = self.c_fc(x)
        x = self.gelu(x)
        x = self.c_proj(x)
        x = self.dropout(x)
        return x

class Block(nn.Module):

    def __init__(self, config):
        super().__init__()
        self.ln_1 = LayerNorm(config.n_embd, bias=config.bias)

        # Choose attention type based on config
        attention_type = getattr(config, 'attention_type', 'causal')
        if attention_type == 'bidirectional':
            self.attn = BidirectionalSelfAttention(config)
            print("Using bidirectional attention")
        else:
            self.attn = CausalSelfAttention(config)
            print("Using causal attention")

        self.ln_2 = LayerNorm(config.n_embd, bias=config.bias)
        self.mlp = MLP(config)

    def forward(self, x):
        x = x + self.attn(self.ln_1(x))
        x = x + self.mlp(self.ln_2(x))
        return x

@dataclass
class GPTConfig:
    block_size: int = 1024
    vocab_size: int = 50304 # GPT-2 vocab_size of 50257, padded up to nearest multiple of 64 for efficiency
    n_layer: int = 12
    n_head: int = 12
    n_embd: int = 768
    dropout: float = 0.0
    bias: bool = True # True: bias in Linears and LayerNorms, like GPT-2. False: a bit better and faster
    attention_type: str = 'causal' # 'causal' or 'bidirectional' - type of attention to use
    use_rope: bool = True # Use Rotary Position Embeddings instead of absolute position embeddings
    binary_classification: bool = False # True: use binary classification head (2 outputs), False: use language model head (vocab_size outputs)

class GPT(nn.Module):

    def __init__(self, config):
        super().__init__()
        assert config.vocab_size is not None
        assert config.block_size is not None
        self.config = config

        # Create transformer components - conditionally include position embeddings
        transformer_components = dict(
            wte = nn.Embedding(config.vocab_size, config.n_embd),
            drop = nn.Dropout(config.dropout),
            h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]),
            ln_f = LayerNorm(config.n_embd, bias=config.bias),
        )
        
        # Only add absolute position embeddings if not using RoPE
        if not getattr(config, 'use_rope', True):
            transformer_components['wpe'] = nn.Embedding(config.block_size, config.n_embd)
        
        self.transformer = nn.ModuleDict(transformer_components)
        
        # Choose between language model head and binary classification head
        if config.binary_classification:
            self.lm_head = nn.Linear(config.n_embd, 2, bias=False)  # Binary classifier: 2 outputs
        else:
            self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)
            # with weight tying when using torch.compile() some warnings get generated:
            # "UserWarning: functional_call was passed multiple values for tied weights.
            # This behavior is deprecated and will be an error in future versions"
            # not 100% sure what this is, so far seems to be harmless. TODO investigate
            self.transformer.wte.weight = self.lm_head.weight # https://paperswithcode.com/method/weight-tying

        # init all weights
        self.apply(self._init_weights)
        # apply special scaled init to the residual projections, per GPT-2 paper
        for pn, p in self.named_parameters():
            if pn.endswith('c_proj.weight'):
                torch.nn.init.normal_(p, mean=0.0, std=0.02/math.sqrt(2 * config.n_layer))
        
        # Special initialization for binary classification head (after general init)
        if config.binary_classification:
            # Initialize with much smaller weights to prevent gradient explosion
            torch.nn.init.normal_(self.lm_head.weight, mean=0.0, std=0.002)  # Very small std

        # report number of parameters
        print("number of parameters: %.2fM" % (self.get_num_params()/1e6,))

    def get_num_params(self, non_embedding=True):
        """
        Return the number of parameters in the model.
        For non-embedding count (default), the position embeddings get subtracted.
        The token embeddings would too, except due to the parameter sharing these
        params are actually used as weights in the final layer, so we include them.
        """
        n_params = sum(p.numel() for p in self.parameters())
        if non_embedding and hasattr(self.transformer, 'wpe'):
            n_params -= self.transformer.wpe.weight.numel()
        return n_params

    def _init_weights(self, module):
        if isinstance(module, nn.Linear):
            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)
            if module.bias is not None:
                torch.nn.init.zeros_(module.bias)
        elif isinstance(module, nn.Embedding):
            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)

    def forward(self, idx, targets=None):
        device = idx.device
        b, t = idx.size()
        assert t <= self.config.block_size, f"Cannot forward sequence of length {t}, block size is only {self.config.block_size}"

        # forward the GPT model itself
        tok_emb = self.transformer.wte(idx) # token embeddings of shape (b, t, n_embd)
        
        # Add positional embeddings only if not using RoPE
        if hasattr(self.transformer, 'wpe'):
            pos = torch.arange(0, t, dtype=torch.long, device=device) # shape (t)
            pos_emb = self.transformer.wpe(pos) # position embeddings of shape (t, n_embd)
            x = self.transformer.drop(tok_emb + pos_emb)
        else:
            # When using RoPE, no absolute position embeddings are needed
            x = self.transformer.drop(tok_emb)
            
        for block in self.transformer.h:
            x = block(x)
        x = self.transformer.ln_f(x)

        if targets is not None:
            # Training: always compute logits for all positions
            logits = self.lm_head(x)
            
            if self.config.binary_classification:
                # For binary classification, targets can be class indices or probability distributions
                if targets.dim() == 3:
                    # Probability distributions (batch_size, seq_len, num_classes)
                    # Cross-entropy can handle soft targets directly
                    loss = F.cross_entropy(logits.view(-1, 2), targets.view(-1, 2))
                else:
                    # Hard targets: 0 or 1 class indices
                    # Calculate dynamic class weights to handle class imbalance
                    flattened_targets = targets.view(-1)
                    valid_targets = flattened_targets[flattened_targets != -1]  # exclude ignore_index
                    
                    if len(valid_targets) > 0:
                        unique, counts = torch.unique(valid_targets, return_counts=True)
                        n_samples = len(valid_targets)
                        n_classes = 2
                        
                        # Create balanced class weights: n_samples / (n_classes * class_count)
                        class_weights = torch.zeros(2, device=targets.device, dtype=logits.dtype)
                        for cls, count in zip(unique, counts):
                            class_weights[cls] = n_samples / (n_classes * count)
                        
                        loss = F.cross_entropy(logits.view(-1, 2), flattened_targets, 
                                             weight=class_weights, ignore_index=-1)
                    else:
                        # Fallback if no valid targets (shouldn't happen in practice)
                        print(f"WARNING: No valid targets found for class weighting, using unweighted loss")
                        loss = F.cross_entropy(logits.view(-1, 2), flattened_targets, ignore_index=-1)
            else:
                # For language modeling, targets can be token IDs or probability distributions
                if targets.dim() == 3:
                    # Probability distributions (batch_size, seq_len, vocab_size)
                    # Cross-entropy can handle soft targets directly
                    loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1, logits.size(-1)))
                else:
                    # Token IDs (batch_size, seq_len)
                    loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-1)
        else:
            # Inference: behavior depends on attention type and classification mode
            if self.config.binary_classification:
                # For binary classification, always compute all positions
                logits = self.lm_head(x)
            else:
                attention_type = getattr(self.config, 'attention_type', 'causal')
                if attention_type == 'causal':
                    # For causal attention (autoregressive), optimize by only computing last position
                    logits = self.lm_head(x[:, [-1], :]) # note: using list [-1] to preserve the time dim
                else:
                    # For bidirectional attention (diffusion), we need logits for all positions
                    logits = self.lm_head(x)  # Compute logits for all positions
            loss = None

        return logits, loss

    def crop_block_size(self, block_size):
        # model surgery to decrease the block size if necessary
        # e.g. we may load the GPT2 pretrained model checkpoint (block size 1024)
        # but want to use a smaller block size for some smaller, simpler model
        assert block_size <= self.config.block_size
        self.config.block_size = block_size
        
        # Only crop position embeddings if they exist (not using RoPE)
        if hasattr(self.transformer, 'wpe'):
            self.transformer.wpe.weight = nn.Parameter(self.transformer.wpe.weight[:block_size])
            
        for block in self.transformer.h:
            # Only causal attention has bias buffer
            if hasattr(block.attn, 'bias'):
                block.attn.bias = block.attn.bias[:,:,:block_size,:block_size]
            # Update RoPE max position embeddings if using RoPE
            if hasattr(block.attn, 'rotary_emb') and block.attn.rotary_emb is not None:
                block.attn.rotary_emb.max_position_embeddings = block_size
                block.attn.rotary_emb._set_cos_sin_cache(
                    seq_len=block_size, 
                    device=block.attn.rotary_emb.inv_freq.device,
                    dtype=torch.get_default_dtype()
                )

    @classmethod
    def from_pretrained(cls, model_type, override_args=None):
        assert model_type in {'gpt2', 'gpt2-medium', 'gpt2-large', 'gpt2-xl'}
        override_args = override_args or {} # default to empty dict
        # only dropout can be overridden see more notes below
        assert all(k == 'dropout' for k in override_args)
        from transformers import GPT2LMHeadModel
        print("loading weights from pretrained gpt: %s" % model_type)

        # n_layer, n_head and n_embd are determined from model_type
        config_args = {
            'gpt2':         dict(n_layer=12, n_head=12, n_embd=768),  # 124M params
            'gpt2-medium':  dict(n_layer=24, n_head=16, n_embd=1024), # 350M params
            'gpt2-large':   dict(n_layer=36, n_head=20, n_embd=1280), # 774M params
            'gpt2-xl':      dict(n_layer=48, n_head=25, n_embd=1600), # 1558M params
        }[model_type]
        print("forcing vocab_size=50257, block_size=1024, bias=True")
        config_args['vocab_size'] = 50257 # always 50257 for GPT model checkpoints
        config_args['block_size'] = 1024 # always 1024 for GPT model checkpoints
        config_args['bias'] = True # always True for GPT model checkpoints
        # we can override the dropout rate, if desired
        if 'dropout' in override_args:
            print(f"overriding dropout rate to {override_args['dropout']}")
            config_args['dropout'] = override_args['dropout']
        # create a from-scratch initialized minGPT model
        config = GPTConfig(**config_args)
        model = GPT(config)
        sd = model.state_dict()
        sd_keys = sd.keys()
        sd_keys = [k for k in sd_keys if not k.endswith('.attn.bias')] # discard this mask / buffer, not a param

        # init a huggingface/transformers model
        model_hf = GPT2LMHeadModel.from_pretrained(model_type)
        sd_hf = model_hf.state_dict()

        # copy while ensuring all of the parameters are aligned and match in names and shapes
        sd_keys_hf = sd_hf.keys()
        sd_keys_hf = [k for k in sd_keys_hf if not k.endswith('.attn.masked_bias')] # ignore these, just a buffer
        sd_keys_hf = [k for k in sd_keys_hf if not k.endswith('.attn.bias')] # same, just the mask (buffer)
        transposed = ['attn.c_attn.weight', 'attn.c_proj.weight', 'mlp.c_fc.weight', 'mlp.c_proj.weight']
        # basically the openai checkpoints use a "Conv1D" module, but we only want to use a vanilla Linear
        # this means that we have to transpose these weights when we import them
        assert len(sd_keys_hf) == len(sd_keys), f"mismatched keys: {len(sd_keys_hf)} != {len(sd_keys)}"
        for k in sd_keys_hf:
            if any(k.endswith(w) for w in transposed):
                # special treatment for the Conv1D weights we need to transpose
                assert sd_hf[k].shape[::-1] == sd[k].shape
                with torch.no_grad():
                    sd[k].copy_(sd_hf[k].t())
            else:
                # vanilla copy over the other parameters
                assert sd_hf[k].shape == sd[k].shape
                with torch.no_grad():
                    sd[k].copy_(sd_hf[k])

        return model

    def configure_optimizers(self, weight_decay, learning_rate, betas, device_type):
        # start with all of the candidate parameters
        param_dict = {pn: p for pn, p in self.named_parameters()}
        # filter out those that do not require grad
        param_dict = {pn: p for pn, p in param_dict.items() if p.requires_grad}
        # create optim groups. Any parameters that is 2D will be weight decayed, otherwise no.
        # i.e. all weight tensors in matmuls + embeddings decay, all biases and layernorms don't.
        decay_params = [p for n, p in param_dict.items() if p.dim() >= 2]
        nodecay_params = [p for n, p in param_dict.items() if p.dim() < 2]
        optim_groups = [
            {'params': decay_params, 'weight_decay': weight_decay},
            {'params': nodecay_params, 'weight_decay': 0.0}
        ]
        num_decay_params = sum(p.numel() for p in decay_params)
        num_nodecay_params = sum(p.numel() for p in nodecay_params)
        print(f"num decayed parameter tensors: {len(decay_params)}, with {num_decay_params:,} parameters")
        print(f"num non-decayed parameter tensors: {len(nodecay_params)}, with {num_nodecay_params:,} parameters")
        # Create AdamW optimizer and use the fused version if it is available
        fused_available = 'fused' in inspect.signature(torch.optim.AdamW).parameters
        use_fused = fused_available and device_type == 'cuda'
        extra_args = dict(fused=True) if use_fused else dict()
        optimizer = torch.optim.AdamW(optim_groups, lr=learning_rate, betas=betas, **extra_args)
        print(f"using fused AdamW: {use_fused}")

        return optimizer

    def estimate_mfu(self, fwdbwd_per_iter, dt):
        """ estimate model flops utilization (MFU) in units of A100 bfloat16 peak FLOPS """
        # first estimate the number of flops we do per iteration.
        # see PaLM paper Appendix B as ref: https://arxiv.org/abs/2204.02311
        N = self.get_num_params()
        cfg = self.config
        L, H, Q, T = cfg.n_layer, cfg.n_head, cfg.n_embd//cfg.n_head, cfg.block_size
        flops_per_token = 6*N + 12*L*H*Q*T
        flops_per_fwdbwd = flops_per_token * T
        flops_per_iter = flops_per_fwdbwd * fwdbwd_per_iter
        # express our flops throughput as ratio of A100 bfloat16 peak flops
        flops_achieved = flops_per_iter * (1.0/dt) # per second
        flops_promised = 312e12 # A100 GPU bfloat16 peak flops is 312 TFLOPS
        mfu = flops_achieved / flops_promised
        return mfu

    @torch.no_grad()
    def generate(self, idx, max_new_tokens, temperature=1.0, top_k=None):
        """
        Take a conditioning sequence of indices idx (LongTensor of shape (b,t)) and complete
        the sequence max_new_tokens times, feeding the predictions back into the model each time.
        Most likely you'll want to make sure to be in model.eval() mode of operation for this.
        """
        for _ in range(max_new_tokens):
            # if the sequence context is growing too long we must crop it at block_size
            idx_cond = idx if idx.size(1) <= self.config.block_size else idx[:, -self.config.block_size:]
            # forward the model to get the logits for the index in the sequence
            logits, _ = self(idx_cond)
            # pluck the logits at the final step and scale by desired temperature
            logits = logits[:, -1, :] / temperature
            # optionally crop the logits to only the top k options
            if top_k is not None:
                v, _ = torch.topk(logits, min(top_k, logits.size(-1)))
                logits[logits < v[:, [-1]]] = -float('Inf')
            # apply softmax to convert logits to (normalized) probabilities
            probs = F.softmax(logits, dim=-1)
            # sample from the distribution
            idx_next = torch.multinomial(probs, num_samples=1)
            # append sampled index to the running sequence and continue
            idx = torch.cat((idx, idx_next), dim=1)

        return idx


--- train_run.py ---
"""
Main training script runner for diffusion training.
Uses train_utils.py for all function definitions.
"""

import os
import sys
import time
import math
import pickle
from contextlib import nullcontext
import threading
from queue import Queue

import numpy as np
import torch
from torch.nn.parallel import DistributedDataParallel as DDP
from torch.distributed import init_process_group, destroy_process_group

from model import GPTConfig, GPT
from utils import Timer, log_masking_stats
from training_utils import (
    get_batch, estimate_loss, get_lr, load_synthetic_model, 
    start_prefetch, stop_prefetch, TrainingContext, UnmaskingStage, update_stage_progress,
    create_unmasking_validation_set, UnmaskingStageType, StickyStageConfig, RandomStageConfig,
    calculate_wrong_answer_entropy, get_current_entropy_penalty, update_entropy_multiplier_ema,
    apply_label_smoothing
)

torch._dynamo.config.suppress_errors = True

# Global timer instance
timer = Timer()

def print_and_flush(msg):
    """Print message and immediately flush stdout for real-time logging"""
    print(msg)
    sys.stdout.flush()

# -----------------------------------------------------------------------------
# default config values 
# I/O
out_dir = 'out'
training_type = 'unmasking'  
eval_interval = 200
log_interval = 20
eval_iters = 20
eval_only = False # if True, script exits right after the first eval
always_save_checkpoint = True # if True, always save a checkpoint after each eval
init_from = 'resume' # 'scratch' or 'resume'
ckpt_filename = '34.5_58.4_UM.pt' # Specific checkpoint to load (if not latest)
# model
n_layer = 6
n_head = 6
n_embd = 384
dropout = 0.01 # for pretraining 0 is good, for finetuning try 0.1+
bias = False # do we use bias inside LayerNorm and Linear layers?
attention_type = 'bidirectional' # 'causal' or 'bidirectional' - type of attention to use (bidirectional recommended for diffusion)
use_rope = True # use Rotary Position Embeddings instead of absolute position embeddings
# wandb logging
wandb_log = True # disabled by default
wandb_project = 'diffusion'
wandb_run_name = '13k_UN_noise_0.2' # 'run' + str(time.time())
# data
dataset = 'shakespeare_char'
gradient_accumulation_steps = 1 # used to simulate larger batch sizes
batch_size = 16 # if gradient_accumulation_steps > 1, this is the micro-batch size
block_size = 1024
use_paragraph_boundaries = False # if True, start samples at paragraph boundaries (double newlines)
# unmasking training config
unmasking_stages = [] # override in config file
validation_stages = [] # override in config file
use_all_stages_for_training = False # if True, generate training batches from all stages like validation
weight_loss_by_mask_ratio = False # if True, weight loss by sqrt(1.0 / mask_ratio) to balance gradient magnitude across masking ratios
enable_entropy_penalty = False # if True, apply entropy penalty to incentivize uniform wrong answer distributions
max_entropy_penalty = 0.5 # maximum entropy penalty multiplier (penalizes concentrated wrong answers)
entropy_penalty_start_iter = 6000 # iteration to start applying entropy penalty
# label smoothing config
uncertainty_factor = 0.0 # if > 0, apply label smoothing: correct answer gets (1-u), wrong answers get u/(vocab_size-1)

# adamw optimizer
learning_rate = 1e-3 # with baby networks can afford to go a bit higher
max_iters = 50000
warmup_iters = 2000 # how many steps to warm up for
lr_decay_iters = 41000 # make equal to max_iters usually
min_lr = 1e-4 # learning_rate / 10 usually
beta1 = 0.9
beta2 = 0.99 # make a bit bigger because number of tokens per iter is small
weight_decay=1e-3

grad_clip = 1.0 # clip gradients at this value, or disable if == 0.0
# learning rate decay settings
decay_lr = True # whether to decay the learning rat
# DDP settings
backend = 'nccl' # 'nccl', 'gloo', etc.
# system
device = 'cuda' # examples: 'cpu', 'cuda', 'cuda:0', 'cuda:1' etc., or try 'mps' on macbooks
dtype = 'float16'
compile = True # use PyTorch 2.0 to compile the model to be faster
start_iter_num = 0

# -----------------------------------------------------------------------------
config_keys = [k for k,v in globals().items() if not k.startswith('_') and isinstance(v, (int, float, bool, str))]
exec(open('configurator.py').read()) # overrides from command line or config file

if len(unmasking_stages) == 0 or unmasking_stages is None:
    print_and_flush("No unmasking stages defined, exiting...")
    exit()

# Update wandb run name after configuration is loaded
if training_type == 'unmasking':
    wandb_run_name = f'{wandb_run_name}_unmasking'

config = {k: globals()[k] for k in config_keys} # will be useful for logging

# Print source code and global variables on startup
print("=" * 80)
print("SOURCE CODE:")
print("=" * 80)

import sys
import os

# Get all local Python files that are imported
local_files = set()
for module_name, module in sys.modules.items():
    if hasattr(module, '__file__') and module.__file__:
        file_path = module.__file__
        # Only include .py files in current directory (not packages/libraries)
        if file_path.endswith('.py') and os.path.dirname(file_path) == os.getcwd():
            local_files.add(os.path.basename(file_path))

# Always include the main script
local_files.add('train_run.py')

# Convert to sorted list for consistent output
local_files = sorted(local_files)

for filename in local_files:
    print(f"\n--- {filename} ---")
    try:
        with open(filename, 'r') as f:
            print(f.read())
    except FileNotFoundError:
        print(f"File {filename} not found")

print("\n" + "=" * 80)
print("GLOBAL VARIABLES:")
print("=" * 80)
for name, value in sorted(globals().items()):
    if not name.startswith('_') and not callable(value):
        print(f"{name} = {value}")

print("\n" + "=" * 80)
# -----------------------------------------------------------------------------

# various inits, derived attributes, I/O setup
ddp = int(os.environ.get('RANK', -1)) != -1 # is this a ddp run?
if ddp:
    init_process_group(backend=backend)
    ddp_rank = int(os.environ['RANK'])
    ddp_local_rank = int(os.environ['LOCAL_RANK'])
    ddp_world_size = int(os.environ['WORLD_SIZE'])
    device = f'cuda:{ddp_local_rank}'
    torch.cuda.set_device(device)
    master_process = ddp_rank == 0 # this process will do logging, checkpointing etc.
    seed_offset = ddp_rank # each process gets a different seed
    # world_size number of processes will be training simultaneously, so we can scale
    # down the desired gradient accumulation iterations per process proportionally
    assert gradient_accumulation_steps % ddp_world_size == 0
    gradient_accumulation_steps //= ddp_world_size
else:
    # if not ddp, we are running on a single gpu, and one process
    master_process = True
    seed_offset = 0
    ddp_world_size = 1
tokens_per_iter = gradient_accumulation_steps * ddp_world_size * batch_size * block_size
print_and_flush(f"tokens per iteration will be: {tokens_per_iter:,}")

if master_process:
    os.makedirs(out_dir, exist_ok=True)
torch.manual_seed(1337 + seed_offset)
torch.backends.cuda.matmul.allow_tf32 = True # allow tf32 on matmul
torch.backends.cudnn.allow_tf32 = True # allow tf32 on cudnn
device_type = 'cuda' if 'cuda' in device else 'cpu' # for later use in torch.autocast
# note: float16 data type will automatically use a GradScaler
ptdtype = {'float32': torch.float32, 'bfloat16': torch.bfloat16, 'float16': torch.float16}[dtype]
ctx = nullcontext() if device_type == 'cpu' else torch.amp.autocast(device_type=device_type, dtype=ptdtype)

# poor man's data loader
data_dir = os.path.join('data', dataset)

# init these up here, can override if init_from='resume' (i.e. from a checkpoint)
iter_num = 0
best_val_loss = 1e9
checkpoint_training_context = None  # For restoring training context state

# attempt to derive vocab_size from the dataset
meta_path = os.path.join(data_dir, 'meta.pkl')
meta_vocab_size = None
if os.path.exists(meta_path):
    with open(meta_path, 'rb') as f:
        meta = pickle.load(f)
    meta_vocab_size = meta['vocab_size']
    print_and_flush(f"found vocab_size = {meta_vocab_size} (inside {meta_path})")
    
    # Set special token ID for unmasking training
    mask_token_id = meta_vocab_size
    extended_vocab_size = meta_vocab_size + 15  # Reserve 15 special tokens for future finetuning
    print_and_flush(f"mask_token_id = {mask_token_id}, extended_vocab_size = {extended_vocab_size}")
else:
    mask_token_id = 65
    extended_vocab_size = 65 + 15  # Reserve 15 special tokens

# Create training context with all parameters
# Convert unmasking_stages dict to UnmaskingStage objects
unmasking_stage_objects = None
if training_type == 'unmasking':
    unmasking_stage_objects = []
    for stage in unmasking_stages:
        stage_type = stage['type']
        if stage_type == 'sticky':
            config = StickyStageConfig(
                target_masked_ratio=stage['target_masked_ratio'],
                p1_probability=stage['p1_probability'],
                p2_probability=stage['p2_probability'],
                val_loss_stale_count=stage['val_loss_stale_count']
            )
        elif stage_type == 'random':
            config = RandomStageConfig(
                max_masked_ratio=stage['max_masked_ratio'],
                val_loss_stale_count=stage['val_loss_stale_count']
            )
        else:
            raise ValueError(f"Unknown stage type: {stage_type}")
        
        unmasking_stage_objects.append(UnmaskingStage(config))

# Convert validation_stages dict to UnmaskingStage objects (if different from training stages)
validation_stage_objects = None
if training_type == 'unmasking' and len(validation_stages) > 0:
    validation_stage_objects = []
    for stage in validation_stages:
        stage_type = stage['type']
        if stage_type == 'sticky':
            config = StickyStageConfig(
                target_masked_ratio=stage['target_masked_ratio'],
                p1_probability=stage['p1_probability'],
                p2_probability=stage['p2_probability'],
                val_loss_stale_count=stage['val_loss_stale_count']
            )
        elif stage_type == 'random':
            config = RandomStageConfig(
                max_masked_ratio=stage['max_masked_ratio'],
                val_loss_stale_count=stage['val_loss_stale_count']
            )
        else:
            raise ValueError(f"Unknown stage type: {stage_type}")
        
        validation_stage_objects.append(UnmaskingStage(config))

training_ctx = TrainingContext(
    training_type=training_type,
    batch_size=batch_size,
    block_size=block_size,
    max_iters=max_iters,
    device=device,
    device_type=device_type,
    seed_offset=seed_offset,
    data_dir=data_dir,
    meta_vocab_size=meta_vocab_size,
    mask_token_id=mask_token_id,
    extended_vocab_size=extended_vocab_size,
    iter_num=iter_num,
    unmasking_stages=unmasking_stage_objects,
    validation_stages=validation_stage_objects,
    eval_iters=eval_iters,
    warmup_iters=warmup_iters,
    lr_decay_iters=lr_decay_iters,
    learning_rate=learning_rate,
    min_lr=min_lr,
    use_paragraph_boundaries=use_paragraph_boundaries,
    use_all_stages_for_training=use_all_stages_for_training,
    weight_loss_by_mask_ratio=weight_loss_by_mask_ratio,
    enable_entropy_penalty=enable_entropy_penalty,
    max_entropy_penalty=max_entropy_penalty,
    entropy_penalty_start_iter=entropy_penalty_start_iter,
    uncertainty_factor=uncertainty_factor
)

# Apply restored training context state if resuming from checkpoint
print(f"DEBUG: init_from='{init_from}', checkpoint_training_context={checkpoint_training_context}")
if init_from == 'resume' and checkpoint_training_context is not None:
    print("Applying restored training context state...")
    training_ctx.current_stage = checkpoint_training_context.get('current_stage', 0)
    training_ctx.val_loss_stale_count = checkpoint_training_context.get('val_loss_stale_count', 0)
    training_ctx.best_val_loss_this_stage = checkpoint_training_context.get('best_val_loss_for_stage', float('inf'))
    training_ctx.entropy_multiplier_ema = checkpoint_training_context.get('entropy_multiplier_ema', 1.0)
    print(f"Training context restored: stage={training_ctx.current_stage}, stale_count={training_ctx.val_loss_stale_count}, entropy_ema={training_ctx.entropy_multiplier_ema:.4f}")
else:
    print(f"DEBUG: NOT applying training context. init_from='{init_from}', checkpoint_training_context={checkpoint_training_context is not None}")

# model init
model_args = dict(n_layer=n_layer, n_head=n_head, n_embd=n_embd, block_size=block_size,
                  bias=bias, vocab_size=None, dropout=dropout, attention_type=attention_type, use_rope=use_rope) # start with model_args from command line
if init_from == 'scratch':
    # init a new model from scratch
    print_and_flush("Initializing a new model from scratch")
    # determine the vocab size we'll use for from-scratch training
    model_args['vocab_size'] = extended_vocab_size if meta_vocab_size is not None else 65 + 15
    gptconf = GPTConfig(**model_args)
    model = GPT(gptconf)
elif init_from == 'resume':
    print_and_flush(f"Resuming unmasking training from {out_dir}")
    # resume training from a checkpoint.
    # Find the latest unmasking checkpoint file
    if ckpt_filename is None:
        import glob
        ckpt_pattern = os.path.join(out_dir, 'ckpt_*unmasking*.pt')
        ckpt_files = glob.glob(ckpt_pattern)
        if not ckpt_files:
            # Fallback to old naming convention
            ckpt_path = os.path.join(out_dir, 'ckpt.pt')
            if not os.path.exists(ckpt_path):
                raise FileNotFoundError(f"No unmasking checkpoint files found in {out_dir}")
        else:
            # Extract iteration numbers and find the latest
            def extract_iter_num(filename):
                basename = os.path.basename(filename)
                # Extract number from ckpt_unmasking_XXX.pt
                parts = basename.split('_')
                for part in parts:
                    if part.replace('.pt', '').isdigit():
                        return int(part.replace('.pt', ''))
                return 0

            latest_ckpt = max(ckpt_files, key=extract_iter_num)
            ckpt_path = latest_ckpt
        print_and_flush(f"Loading latest checkpoint: {os.path.basename(ckpt_path)}")
    else:
        ckpt_path = os.path.join(out_dir, ckpt_filename)
        if not os.path.exists(ckpt_path):
            raise FileNotFoundError(f"Checkpoint file {ckpt_path} not found")

    checkpoint = torch.load(ckpt_path, map_location=device, weights_only=False)
    checkpoint_model_args = checkpoint['model_args']
    training_ctx.extended_vocab_size = checkpoint_model_args['vocab_size']
    print_and_flush(f"Checkpoint vocab size: {training_ctx.extended_vocab_size}")
    # force these config attributes to be equal otherwise we can't even resume training
    # the rest of the attributes (e.g. dropout) can stay as desired from command line
    for k in ['n_layer', 'n_head', 'n_embd', 'block_size', 'bias', 'vocab_size']:
        model_args[k] = checkpoint_model_args[k]
    # Also restore use_rope setting if it exists in checkpoint
    if 'use_rope' in checkpoint_model_args:
        model_args['use_rope'] = checkpoint_model_args['use_rope']
    # create the model
    gptconf = GPTConfig(**model_args)
    model = GPT(gptconf)
    state_dict = checkpoint['model']
    # fix the keys of the state dictionary :(
    # honestly no idea how checkpoints sometimes get this prefix, have to debug more
    unwanted_prefix = '_orig_mod.'
    for k,v in list(state_dict.items()):
        if k.startswith(unwanted_prefix):
            state_dict[k[len(unwanted_prefix):]] = state_dict.pop(k)
    model.load_state_dict(state_dict)
    iter_num = checkpoint['iter_num']
    start_iter_num = iter_num
    best_val_loss = checkpoint['best_val_loss']
    
    # Restore training context state if available
    if 'training_context' in checkpoint:
        ctx_state = checkpoint['training_context']
        print_and_flush(f"Restoring training context state:")
        print_and_flush(f"  Stage: {ctx_state.get('current_stage', 0)}")
        print_and_flush(f"  Val loss stale count: {ctx_state.get('val_loss_stale_count', 0)}")
        print_and_flush(f"  Best val loss for stage: {ctx_state.get('best_val_loss_for_stage', float('inf'))}")
        
        # These will be set on the training_ctx after it's created
        checkpoint_training_context = ctx_state
    else:
        checkpoint_training_context = None

# crop down the model block size if desired, using model surgery
if block_size < model.config.block_size:
    model.crop_block_size(block_size)
    model_args['block_size'] = block_size # so that the checkpoint will have the right value
model.to(device)

# No synthetic model loading needed for unmasking training

# initialize a GradScaler. If enabled=False scaler is a no-op
scaler = torch.cuda.amp.GradScaler(enabled=(dtype == 'float16'))

# optimizer
optimizer = model.configure_optimizers(weight_decay, learning_rate, (beta1, beta2), device_type)
if init_from == 'resume':
    optimizer.load_state_dict(checkpoint['optimizer'])
checkpoint = None # free up memory

# compile the model
if compile:
    print_and_flush("compiling the model... (takes a ~minute)")
    unoptimized_model = model
    model = torch.compile(model) # requires PyTorch 2.0

# wrap model into DDP container
if ddp:
    model = DDP(model, device_ids=[ddp_local_rank])

# logging
if wandb_log and master_process and not eval_only:
    import wandb
    wandb.init(project=wandb_project, name=wandb_run_name, config=config)

# Function to reload model and optimizer from checkpoint during training
def reload_from_checkpoint():
    """Reload model and optimizer from the latest checkpoint"""
    global model, optimizer, iter_num, best_val_loss, training_ctx, raw_model
    
    print(f"\n*** RELOADING FROM CHECKPOINT ***")
    
    # Find the latest unmasking checkpoint file
    import glob
    ckpt_pattern = os.path.join(out_dir, 'ckpt_*unmasking*.pt')
    ckpt_files = glob.glob(ckpt_pattern)
    
    if not ckpt_files:
        print("No unmasking checkpoint files found for recovery - cannot continue")
        return False
    
    # Extract iteration numbers and find the latest
    def extract_iter_num(filename):
        basename = os.path.basename(filename)
        # Extract number from ckpt_unmasking_XXX.pt
        parts = basename.split('_')
        for part in parts:
            if part.replace('.pt', '').isdigit():
                return int(part.replace('.pt', ''))
        return 0
    
    latest_ckpt = max(ckpt_files, key=extract_iter_num)
    ckpt_path = latest_ckpt
    print(f"Reloading from checkpoint: {os.path.basename(ckpt_path)}")
    
    # Load checkpoint
    checkpoint = torch.load(ckpt_path, map_location=device, weights_only=False)
    
    # Reload model state - handle compiled vs non-compiled model mismatches
    model_state = checkpoint['model']
    
    # Check if current model expects _orig_mod prefix but checkpoint doesn't have it
    current_keys = set(raw_model.state_dict().keys())
    checkpoint_keys = set(model_state.keys())
    
    # Determine if we need to add or remove _orig_mod prefix
    if any(k.startswith('_orig_mod.') for k in current_keys) and not any(k.startswith('_orig_mod.') for k in checkpoint_keys):
        # Current model is compiled (has _orig_mod prefix), but checkpoint doesn't - add prefix
        print("Adding _orig_mod prefix to checkpoint keys for compiled model")
        new_state = {}
        for k, v in model_state.items():
            new_state[f'_orig_mod.{k}'] = v
        model_state = new_state
    elif not any(k.startswith('_orig_mod.') for k in current_keys) and any(k.startswith('_orig_mod.') for k in checkpoint_keys):
        # Current model is not compiled, but checkpoint has _orig_mod prefix - remove prefix
        print("Removing _orig_mod prefix from checkpoint keys for non-compiled model")
        unwanted_prefix = '_orig_mod.'
        for k, v in list(model_state.items()):
            if k.startswith(unwanted_prefix):
                model_state[k[len(unwanted_prefix):]] = model_state.pop(k)
    
    raw_model.load_state_dict(model_state)
    
    # Reload optimizer state
    optimizer.load_state_dict(checkpoint['optimizer'])
    
    # Update iteration and loss tracking
    # Step back iteration to avoid immediately hitting the same problematic iteration
    iter_num = checkpoint['iter_num'] - 1
    best_val_loss = checkpoint['best_val_loss']
    
    # Restore training context state if available
    if 'training_context' in checkpoint:
        ctx_state = checkpoint['training_context']
        training_ctx.current_stage = ctx_state.get('current_stage', 0)
        training_ctx.val_loss_stale_count = ctx_state.get('val_loss_stale_count', 0)
        training_ctx.best_val_loss_this_stage = ctx_state.get('best_val_loss_for_stage', float('inf'))
        training_ctx.entropy_multiplier_ema = ctx_state.get('entropy_multiplier_ema', 1.0)
        print(f"Training context restored: stage={training_ctx.current_stage}, entropy_ema={training_ctx.entropy_multiplier_ema:.4f}")
    
    
    print(f"Model and optimizer reloaded from iteration {iter_num}")
    print("*** CHECKPOINT RELOAD COMPLETE ***\n")
    return True

# training loop
X, Y, mask = get_batch('train', training_ctx) # fetch the very first batch
t0 = time.time()
local_iter_num = 0 # number of iterations in the lifetime of this process
raw_model = model.module if ddp else model # unwrap DDP container if needed
running_mfu = -1.0

# Show initial stage configuration for unmasking training
if training_ctx.training_type == 'unmasking':
    stage_config = training_ctx.get_current_stage_config()
    print(f"\n*** STAGE-BASED UNMASKING TRAINING INITIALIZED ***")
    print(f"Starting at Stage {training_ctx.current_stage}:")
    stage_type = stage_config.get_stage_type()
    print(f"  Stage type: {stage_type.value}")
    if stage_type == UnmaskingStageType.STICKY:
        config = stage_config.config
        print(f"  Target masked ratio: {config.target_masked_ratio}")
        print(f"  P1 probability: {config.p1_probability}")
        print(f"  P2 probability: {config.p2_probability}")
    elif stage_type == UnmaskingStageType.RANDOM:
        config = stage_config.config
        print(f"  Max masked ratio: {config.max_masked_ratio}")
    print(f"  Val loss stale count limit: {stage_config.get_val_loss_stale_count()}")
    print(f"Total stages configured: {len(training_ctx.unmasking_stages)}")
    print("*** STAGE INITIALIZATION COMPLETE ***\n")
    
    # Pre-create validation set with equal representation from all stages
    print("Pre-creating validation set...")
    create_unmasking_validation_set(training_ctx)
    
    # Training batches will be generated fresh each time from all stages when flag is enabled
    if training_ctx.use_all_stages_for_training:
        print("Training will generate fresh batches from all stages each iteration")

print_and_flush("Starting training loop...")
just_recovered = False
while True:

    # determine and set the learning rate for this iteration
    lr = get_lr(iter_num, training_ctx) if decay_lr else learning_rate
    for param_group in optimizer.param_groups:
        param_group['lr'] = lr

    # evaluate the loss on train/val sets and write checkpoints
    if (iter_num % eval_interval == 0 and master_process and not just_recovered) or eval_only:
        print_and_flush(f"\n--- Starting validation at iteration {iter_num} ---")
        with timer.time_function('validation'):
            # Update training context with current iteration
            training_ctx.iter_num = iter_num
            losses = estimate_loss(model, ctx, timer, training_ctx)

        # VALIDATION INSTABILITY DETECTION
        train_loss_finite = math.isfinite(losses['train'])
        val_loss_finite = math.isfinite(losses['val'])
        if not train_loss_finite or not val_loss_finite:
            print_and_flush(f"\n*** VALIDATION INSTABILITY at iter {iter_num} ***")
            print_and_flush(f"Train loss: {losses['train']} ({'finite' if train_loss_finite else 'NaN/Inf'})")
            print_and_flush(f"Val loss: {losses['val']} ({'finite' if val_loss_finite else 'NaN/Inf'})")
            print_and_flush("NaN detected in validation - model has become unstable")
            print_and_flush("*** TERMINATING TRAINING ***")
            break
        
        # Print basic losses
        print_and_flush(f"--- Validation complete ---")
        print_and_flush(f"step {iter_num}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}, lr {lr:.6f}")
        
        # Print entropy penalty information if enabled
        if training_ctx.enable_entropy_penalty:
            current_entropy_penalty = get_current_entropy_penalty(iter_num, training_ctx)
            print_and_flush(f"entropy penalty: {current_entropy_penalty:.4f}, multiplier EMA: {training_ctx.entropy_multiplier_ema:.4f}")
        
        # Print stage information for unmasking training
        if 'current_stage' in losses:
            stage_config = training_ctx.get_current_stage_config()
            stage_type = stage_config.get_stage_type()
            stage_info = f"Stage {losses['current_stage']} ({stage_type.value}): "
            if stage_type == UnmaskingStageType.STICKY:
                config = stage_config.config
                stage_info += f"target_ratio={config.target_masked_ratio:.1f}, p1={config.p1_probability:.1f}, p2={config.p2_probability:.1f}"
            elif stage_type == UnmaskingStageType.RANDOM:
                config = stage_config.config
                stage_info += f"max_ratio={config.max_masked_ratio:.1f}"
            stage_info += f", stale_count={losses.get('val_loss_stale_count', 0)}"
            print_and_flush(stage_info)

        # Print model vs random statistics if available
        if 'val_model_vs_random' in losses:
            print(f"  val model vs random: {losses['val_model_vs_random']:.2f}x better")
            print(f"  val avg correct prob: {losses['val_avg_correct_prob']:.4f} (random: {1.0/training_ctx.extended_vocab_size:.4f})")
            if 'val_signal_to_noise' in losses:
                print(f"  val signal to noise: {losses['val_signal_to_noise']:.2f} (median: {losses.get('val_signal_to_noise_median', 0.0):.2f})")
            if 'val_most_likely_accuracy' in losses:
                print(f"  Most likely guess correct P %: {losses['val_most_likely_accuracy']:.1f}%")
        
        # Update stage progress for unmasking training
        stage_advanced = update_stage_progress(training_ctx, losses['val'])
        if stage_advanced:
            print(f"Advanced to stage {training_ctx.current_stage} - validation set remains consistent across all stages")
        
        print()  # Add blank line for readability

        if wandb_log and master_process and not eval_only:
            log_dict = {
                "iter": iter_num,
                "train/loss": losses['train'],
                "val/loss": losses['val'],
                "lr": lr,
                "model vs random": losses.get('val_model_vs_random', 0.0),
                "signal to noise": losses.get('val_signal_to_noise', 0.0),
                "signal to noise median": losses.get('val_signal_to_noise_median', 0.0),
                "mfu": running_mfu*100, # convert to percentage
                "masked_token_ratio": losses.get('train_masked_token_ratio', 0.0),
                "min_masked_token_ratio": losses.get('train_min_masked_token_ratio', 0.0),
                "max_masked_token_ratio": losses.get('train_max_masked_token_ratio', 0.0),
            }
            
            # Add entropy penalty to validation wandb logging if enabled
            if training_ctx.enable_entropy_penalty:
                current_entropy_penalty = get_current_entropy_penalty(iter_num, training_ctx)
                log_dict["entropy_penalty"] = current_entropy_penalty
                log_dict["entropy_multiplier_ema"] = training_ctx.entropy_multiplier_ema
            
            # Add per-stage validation losses for unmasking training
            for stage_idx in range(len(training_ctx.validation_stages or [])):
                stage_loss_key = f'val_stage_{stage_idx}_loss'
                stage_samples_key = f'val_stage_{stage_idx}_samples'
                if stage_loss_key in losses:
                    log_dict[f'val/stage_{stage_idx}_loss'] = losses[stage_loss_key]
                    log_dict[f'val/stage_{stage_idx}_samples'] = losses[stage_samples_key]

            wandb.log(log_dict)
        if losses['val'] < best_val_loss or always_save_checkpoint:
            best_val_loss = losses['val']
            if iter_num > 0:
                checkpoint = {
                    'model': raw_model.state_dict(),
                    'optimizer': optimizer.state_dict(),
                    'model_args': model_args,
                    'iter_num': iter_num,
                    'best_val_loss': best_val_loss,
                    'config': config,
                }
                
                # Save training context state for proper resumption
                checkpoint['training_context'] = {
                    'current_stage': training_ctx.current_stage,
                    'val_loss_stale_count': training_ctx.val_loss_stale_count,
                    'best_val_loss_for_stage': training_ctx.best_val_loss_this_stage,
                    'entropy_multiplier_ema': training_ctx.entropy_multiplier_ema
                }
                ckpt_filename = f'ckpt_unmasking_{iter_num}.pt'
                    
                if start_iter_num != iter_num:
                    print(f"saving checkpoint to {out_dir}/{ckpt_filename}")
                    torch.save(checkpoint, os.path.join(out_dir, ckpt_filename))
    if eval_only:
        break

    # forward backward update, with optional gradient accumulation to simulate larger batch size
    # and using the GradScaler if data type is float16
    with timer.time_function('gradient_accumulation_loop'):
        for micro_step in range(gradient_accumulation_steps):
            if ddp:
                # in DDP training we only need to sync gradients at the last micro step.
                # the official way to do this is with model.no_sync() context manager, but
                # I really dislike that this bloats the code and forces us to repeat code
                # looking at the source of that context manager, it just toggles this variable
                model.require_backward_grad_sync = (micro_step == gradient_accumulation_steps - 1)
            with ctx:
                with timer.time_function('forward_pass'):
                    # Combined forward pass and loss computation for efficiency
                    logits, loss = model(X, Y)
                
                with timer.time_function('instability_detection'):
                    # TRAINING INSTABILITY DETECTION
                    if not torch.isfinite(logits).all():
                        print(f"\n*** INSTABILITY DETECTED at iter {iter_num} ***")
                        print(f"Logits contain NaN/Inf: {torch.isnan(logits).sum().item()} NaN, {torch.isinf(logits).sum().item()} Inf")
                        print(f"Logits stats: min={logits.min().item():.6f}, max={logits.max().item():.6f}, mean={logits.mean().item():.6f}")
                        print("*** ATTEMPTING RECOVERY FROM CHECKPOINT ***")
                        if reload_from_checkpoint():
                            # Reset local state and restart iteration completely
                            local_iter_num = 0
                            running_mfu = -1.0
                            training_ctx.iter_num = iter_num
                            # Generate new batch to avoid same problematic data
                            X, Y, mask = get_batch('train', training_ctx)
                            # Reset scaler state and start fresh iteration
                            scaler = torch.cuda.amp.GradScaler(enabled=(dtype == 'float16'))
                            optimizer.zero_grad(set_to_none=True)
                            just_recovered = True
                            t0 = time.time()
                            continue
                        else:
                            print("*** RECOVERY FAILED - TERMINATING TRAINING ***")
                            break
                    
                    if not torch.isfinite(loss):
                        print(f"\n*** LOSS INSTABILITY at iter {iter_num} ***")
                        print(f"Loss is {loss.item()}: {'NaN' if torch.isnan(loss) else 'Inf'}")
                        print("*** ATTEMPTING RECOVERY FROM CHECKPOINT ***")
                        if reload_from_checkpoint():
                            # Reset local state and restart iteration completely
                            local_iter_num = 0
                            running_mfu = -1.0
                            training_ctx.iter_num = iter_num
                            # Generate new batch to avoid same problematic data
                            X, Y, mask = get_batch('train', training_ctx)
                            # Reset scaler state and start fresh iteration
                            scaler = torch.cuda.amp.GradScaler(enabled=(dtype == 'float16'))
                            optimizer.zero_grad(set_to_none=True)
                            just_recovered = True
                            t0 = time.time()
                            continue
                        else:
                            print("*** RECOVERY FAILED - TERMINATING TRAINING ***")
                            break
                
                # Apply masking for unmasking training only (most efficient path)
                if training_ctx.training_type == 'unmasking' and mask.any():
                    # Fast path: reshape once and use boolean indexing
                    # Cross-entropy handles both hard targets (indices) and soft targets (probabilities)
                    logits_reshaped = logits.view(-1, logits.size(-1))
                    mask_reshaped = mask.view(-1)
                    
                    if Y.dim() == 3:
                        # Soft targets (probability distributions)
                        targets_reshaped = Y.view(-1, Y.size(-1))
                        loss = torch.nn.functional.cross_entropy(
                            logits_reshaped[mask_reshaped], 
                            targets_reshaped[mask_reshaped], 
                            reduction='mean'
                        )
                    else:
                        # Hard targets (token indices)
                        targets_reshaped = Y.view(-1)
                        loss = torch.nn.functional.cross_entropy(
                            logits_reshaped[mask_reshaped], 
                            targets_reshaped[mask_reshaped], 
                            reduction='mean'
                        )
                    
                    # Apply mask ratio weighting if enabled
                    if training_ctx.weight_loss_by_mask_ratio:
                        mask_ratio = mask.float().mean().item()
                        if mask_ratio > 0:
                            weight = (1.0 / mask_ratio) ** 0.5  # sqrt(1.0 / mask_ratio)
                            loss = loss * weight
                else:
                    if training_ctx.training_type == 'unmasking':
                        loss = torch.tensor(0.0, device=loss.device, requires_grad=True)
                
                with timer.time_function('loss_processing'):
                    # Apply entropy penalty if enabled (works for all training types)
                    if training_ctx.enable_entropy_penalty:
                        current_entropy_penalty = get_current_entropy_penalty(iter_num, training_ctx)
                        if current_entropy_penalty > 0:
                            # Calculate entropy of wrong answer distributions
                            wrong_answer_entropy = calculate_wrong_answer_entropy(logits, Y, training_ctx.extended_vocab_size)
                            
                            # Calculate max possible entropy for wrong answers: log(vocab_size - 1)
                            max_wrong_entropy = math.log(training_ctx.extended_vocab_size - 1)
                            
                            # Penalty for LOW entropy (concentrated wrong answers)
                            # When entropy is low (bad) -> high penalty
                            # When entropy is high (good) -> low penalty  
                            entropy_penalty_factor = (max_wrong_entropy - wrong_answer_entropy) / max_wrong_entropy
                            entropy_multiplier = 1.0 + current_entropy_penalty * entropy_penalty_factor
                            loss = loss * entropy_multiplier
                            
                            # Update EMA of entropy multiplier
                            update_entropy_multiplier_ema(training_ctx, entropy_multiplier)
                        else:
                            # No penalty applied, multiplier is 1.0
                            update_entropy_multiplier_ema(training_ctx, 1.0)

                # For remasking variants, model's internal loss is already correct
                
                # UNIVERSAL: Check final loss after any training-type-specific processing
                if not torch.isfinite(loss):
                    print(f"\n*** FINAL LOSS INSTABILITY at iter {iter_num} ***")
                    print(f"Final loss is {loss.item()}: {'NaN' if torch.isnan(loss) else 'Inf'}")
                    print(f"Training type: {training_ctx.training_type}")
                    if hasattr(mask, 'float'):  # Check if mask exists
                        print(f"Mask ratio: {mask.float().mean().item():.4f}")
                    print("*** ATTEMPTING RECOVERY FROM CHECKPOINT ***")
                    if reload_from_checkpoint():
                        # Reset local state and restart iteration completely
                        local_iter_num = 0
                        running_mfu = -1.0
                        training_ctx.iter_num = iter_num
                        # Generate new batch to avoid same problematic data
                        X, Y, mask = get_batch('train', training_ctx)
                        # Reset scaler state and start fresh iteration
                        scaler = torch.cuda.amp.GradScaler(enabled=(dtype == 'float16'))
                        optimizer.zero_grad(set_to_none=True)
                        just_recovered = True
                        t0 = time.time()
                        continue
                    else:
                        print("*** RECOVERY FAILED - TERMINATING TRAINING ***")
                        break
                        
                loss = loss / gradient_accumulation_steps
        # immediately async prefetch next batch while model is doing the forward pass on the GPU
        with timer.time_function('data_generation'):
            # Update training context with current iteration for the next batch
            training_ctx.iter_num = iter_num
            X, Y, mask = get_batch('train', training_ctx)
            # backward pass, with gradient scaling if training in fp16
            with timer.time_function('backward_pass'):
                scaler.scale(loss).backward()
    
    # GRADIENT PROCESSING AND CLIPPING
    with timer.time_function('gradient_processing'):
        if grad_clip != 0.0:
            scaler.unscale_(optimizer)
            # Monitor gradient norms before clipping
            grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)
        
            # Check for true instability (NaN/Inf gradients)
            if not torch.isfinite(grad_norm):
                # At iteration 0 with lr=0, infinite gradients indicate model/loss issues
                if iter_num == 0:
                    print(f"\n*** INITIALIZATION PROBLEM at iter {iter_num} ***")
                    print(f"Gradient norm is {grad_norm.item()}: {'NaN' if torch.isnan(grad_norm) else 'Inf'}")
                    print(f"Learning rate: {lr:.6f}")
                    print("This suggests model initialization or loss computation issues")
                    
                    # Check a few key statistics
                    print("\nModel parameter stats:")
                    for name, param in list(model.named_parameters())[:3]:  # First 3 params
                        print(f"  {name}: mean={param.data.mean().item():.6f}, std={param.data.std().item():.6f}")
                        if param.grad is not None:
                            print(f"    grad: mean={param.grad.data.mean().item():.6f}, std={param.grad.data.std().item():.6f}")
                else:
                    print(f"\n*** GRADIENT INSTABILITY at iter {iter_num} ***")
                    print(f"Gradient norm is {grad_norm.item()}: {'NaN' if torch.isnan(grad_norm) else 'Inf'}")
                
                # Check individual parameter gradients
                nan_params = 0
                inf_params = 0
                for name, param in model.named_parameters():
                    if param.grad is not None:
                        if torch.isnan(param.grad).any():
                            nan_params += 1
                        if torch.isinf(param.grad).any():
                            inf_params += 1
                print(f"Parameters with NaN gradients: {nan_params}, with Inf gradients: {inf_params}")
                print("*** ATTEMPTING RECOVERY FROM CHECKPOINT ***")
                if reload_from_checkpoint():
                    # Reset local state and restart iteration completely
                    local_iter_num = 0
                    running_mfu = -1.0
                    training_ctx.iter_num = iter_num
                    # Generate new batch to avoid same problematic data
                    X, Y, mask = get_batch('train', training_ctx)
                    # Reset scaler state and start fresh iteration
                    scaler = torch.cuda.amp.GradScaler(enabled=(dtype == 'float16'))
                    optimizer.zero_grad(set_to_none=True)
                    just_recovered = True
                    t0 = time.time()
                    continue
                else:
                    print("*** RECOVERY FAILED - TERMINATING TRAINING ***")
                    break
            
            # Only warn about large gradients after initial iterations (when lr > 0)
            if iter_num > 10 and grad_norm > grad_clip * 10:
                print(f"WARNING: Large gradient norm at iter {iter_num}: {grad_norm.item():.4f} (clip threshold: {grad_clip})")
        else:
            # Still check gradient norms even without clipping
            total_norm = 0.0
            nan_grads = False
            inf_grads = False
            
            for param in model.parameters():
                if param.grad is not None:
                    param_norm = param.grad.data.norm(2)
                    if torch.isnan(param_norm):
                        nan_grads = True
                    if torch.isinf(param_norm):
                        inf_grads = True
                    total_norm += param_norm.item() ** 2
            
            total_norm = total_norm ** (1. / 2)
            
            if nan_grads or inf_grads:
                print(f"\n*** GRADIENT INSTABILITY at iter {iter_num} (no clipping) ***")
                print(f"NaN gradients: {nan_grads}, Inf gradients: {inf_grads}")
                print(f"Total gradient norm: {total_norm:.6f}")
                print("*** ATTEMPTING RECOVERY FROM CHECKPOINT ***")
                if reload_from_checkpoint():
                    # Reset local state and restart iteration completely
                    local_iter_num = 0
                    running_mfu = -1.0
                    training_ctx.iter_num = iter_num
                    # Generate new batch to avoid same problematic data
                    X, Y, mask = get_batch('train', training_ctx)
                    # Reset scaler state and start fresh iteration
                    scaler = torch.cuda.amp.GradScaler(enabled=(dtype == 'float16'))
                    optimizer.zero_grad(set_to_none=True)
                    just_recovered = True
                    t0 = time.time()
                    continue
                else:
                    print("*** RECOVERY FAILED - TERMINATING TRAINING ***")
                    break
    
    # OPTIMIZER OPERATIONS  
    with timer.time_function('optimizer_operations'):
        # step the optimizer and scaler if training in fp16
        scaler.step(optimizer)
        scaler.update()
    
    # PARAMETER STABILITY DETECTION
    with timer.time_function('parameter_stability_check'):
        nan_params = 0
        inf_params = 0
        param_names_with_issues = []
        
        for name, param in model.named_parameters():
            if param.data is not None:
                if torch.isnan(param.data).any():
                    nan_params += 1
                    param_names_with_issues.append(f"{name}(NaN)")
                if torch.isinf(param.data).any():
                    inf_params += 1
                    param_names_with_issues.append(f"{name}(Inf)")
        
        if nan_params > 0 or inf_params > 0:
            print(f"\n*** PARAMETER INSTABILITY at iter {iter_num} ***")
            print(f"Parameters with NaN values: {nan_params}, with Inf values: {inf_params}")
            print(f"Affected parameters: {param_names_with_issues[:10]}")  # Show first 10
            if len(param_names_with_issues) > 10:
                print(f"... and {len(param_names_with_issues) - 10} more")
            print("*** ATTEMPTING RECOVERY FROM CHECKPOINT ***")
            if reload_from_checkpoint():
                # Reset local state and restart iteration completely
                local_iter_num = 0
                running_mfu = -1.0
                training_ctx.iter_num = iter_num
                # Generate new batch to avoid same problematic data
                X, Y, mask = get_batch('train', training_ctx)
                # Reset scaler state and start fresh iteration
                scaler = torch.cuda.amp.GradScaler(enabled=(dtype == 'float16'))
                optimizer.zero_grad(set_to_none=True)
                just_recovered = True
                t0 = time.time()
                continue
            else:
                print("*** RECOVERY FAILED - TERMINATING TRAINING ***")
                break
    
    # CLEANUP OPERATIONS
    with timer.time_function('cleanup_operations'):
        # flush the gradients as soon as we can, no need for this memory anymore
        optimizer.zero_grad(set_to_none=True)

    # timing and logging
    t1 = time.time()
    dt = t1 - t0
    t0 = t1
    if iter_num % log_interval == 0 and master_process:
        # GPU SYNCHRONIZATION OPERATIONS
        with timer.time_function('gpu_synchronization'):
            # get loss as float. note: this is a CPU-GPU sync point
            # scale up to undo the division above, approximating the true total loss (exact would have been a sum)
            lossf = loss.item() * gradient_accumulation_steps
            if local_iter_num >= 5: # let the training loop settle a bit
                mfu = raw_model.estimate_mfu(batch_size * gradient_accumulation_steps, dt)
                running_mfu = mfu if running_mfu == -1.0 else 0.9*running_mfu + 0.1*mfu

        # Enhanced logging with detailed timing - use recent measurements only
        data_time = timer.get_recent_average('data_generation') * 1000
        forward_time = timer.get_recent_average('forward_pass') * 1000
        loss_time = timer.get_recent_average('loss_computation') * 1000
        backward_time = timer.get_recent_average('backward_pass') * 1000
        grad_accum_time = timer.get_recent_average('gradient_accumulation_loop') * 1000
        grad_proc_time = timer.get_recent_average('gradient_processing') * 1000
        optimizer_time = timer.get_recent_average('optimizer_operations') * 1000
        param_check_time = timer.get_recent_average('parameter_stability_check') * 1000
        cleanup_time = timer.get_recent_average('cleanup_operations') * 1000
        gpu_sync_time = timer.get_recent_average('gpu_synchronization') * 1000
        loss_proc_time = timer.get_recent_average('loss_processing') * 1000
        instability_time = timer.get_recent_average('instability_detection') * 1000

        # Calculate total of measured components (avoid double-counting nested timers)
        # grad_accum_time already contains ALL nested operations: data, forward, backward, loss_proc, instability
        measured_total = grad_accum_time + grad_proc_time + optimizer_time + param_check_time + cleanup_time + gpu_sync_time
        total_time = dt * 1000
        unaccounted_time = total_time - measured_total

        print(f"iter {iter_num}: loss {lossf:.4f}, time {dt*1000:.2f}ms, mfu {running_mfu*100:.2f}%")
        print(f"  data: {data_time:.1f}ms, grad_accum: {grad_accum_time:.1f}ms (fw: {forward_time:.1f}ms, bw: {backward_time:.1f}ms)")
        print(f"  grad_proc: {grad_proc_time:.1f}ms, optimizer: {optimizer_time:.1f}ms, param_check: {param_check_time:.1f}ms")
        print(f"  loss_proc: {loss_proc_time:.1f}ms, instability: {instability_time:.1f}ms")
        print(f"  cleanup: {cleanup_time:.1f}ms, gpu_sync: {gpu_sync_time:.1f}ms")
        print(f"  measured: {measured_total:.1f}ms, unaccounted: {unaccounted_time:.1f}ms ({unaccounted_time/total_time*100:.1f}%)")
        
        # Add entropy penalty logging if enabled
        if training_ctx.enable_entropy_penalty:
            current_entropy_penalty = get_current_entropy_penalty(iter_num, training_ctx)
            if current_entropy_penalty > 0:
                print(f"  entropy_penalty: {current_entropy_penalty:.4f}, multiplier_ema: {training_ctx.entropy_multiplier_ema:.4f} (max: {training_ctx.max_entropy_penalty})")

        # Validation timing (when applicable)
        if iter_num % eval_interval == 0:
            val_time = timer.get_average('validation') * 1000
            val_data_time = timer.get_average('validation_data_generation') * 1000
            val_forward_time = timer.get_average('validation_forward_pass') * 1000
            val_loss_time = timer.get_average('validation_loss_computation') * 1000
            print(f"  validation: {val_time:.2f}ms (data: {val_data_time:.2f}ms, forward: {val_forward_time:.2f}ms, loss: {val_loss_time:.2f}ms)")

        # Add masking statistics logging for unmasking
        stage_config = training_ctx.get_current_stage_config()
        if stage_config and iter_num % (log_interval * 10) == 0:
            mask_ratio = mask.float().mean().item()
            stage_type = stage_config.get_stage_type()
            stage_info = f"Masking: stage={training_ctx.current_stage} ({stage_type.value}), actual_ratio={mask_ratio:.3f}"
            if stage_type == UnmaskingStageType.STICKY:
                config = stage_config.config
                stage_info += f", target={config.target_masked_ratio:.1f}, p1={config.p1_probability:.1f}, p2={config.p2_probability:.1f}"
            elif stage_type == UnmaskingStageType.RANDOM:
                config = stage_config.config
                stage_info += f", max={config.max_masked_ratio:.1f}"
            print(stage_info)
        
        if wandb_log and master_process and not eval_only:
            log_dict = {
                "iter": iter_num,
                "train/loss": lossf,
                "lr": lr,
                "mfu": running_mfu*100 # convert to percentage
            }
            
            # Add entropy penalty to wandb logging if enabled
            if training_ctx.enable_entropy_penalty:
                current_entropy_penalty = get_current_entropy_penalty(iter_num, training_ctx)
                log_dict["entropy_penalty"] = current_entropy_penalty
                log_dict["entropy_multiplier_ema"] = training_ctx.entropy_multiplier_ema

            wandb.log(log_dict)
    iter_num += 1
    local_iter_num += 1
    just_recovered = False  # Reset recovery flag after successful iteration

    # termination conditions
    if iter_num > max_iters:
        break

if ddp:
    destroy_process_group()

# Cleanup prefetch thread
stop_prefetch()

--- utils.py ---
"""
Utility functions for diffusion training including timing and logging
"""
import time
import torch
from collections import defaultdict


class Timer:
    """Timer class for performance monitoring with context manager support"""
    
    def __init__(self):
        self.times = defaultdict(list)
    
    def time_function(self, name):
        """Context manager for timing function calls"""
        class TimerContext:
            def __init__(self, timer, name):
                self.timer = timer
                self.name = name
                self.start_time = None
            
            def __enter__(self):
                self.start_time = time.time()
                return self
            
            def __exit__(self, *args):
                elapsed = time.time() - self.start_time
                self.timer.times[self.name].append(elapsed)
        
        return TimerContext(self, name)
    
    def get_average(self, name, last_n=100):
        """Get average time for last N calls"""
        if name not in self.times or not self.times[name]:
            return 0.0
        return sum(self.times[name][-last_n:]) / min(len(self.times[name]), last_n)
    
    def get_recent_average(self, name, last_n=10):
        """Get average time for very recent calls only"""
        if name not in self.times or not self.times[name]:
            return 0.0
        return sum(self.times[name][-last_n:]) / min(len(self.times[name]), last_n)
    
    def get_last_time(self, name):
        """Get the most recent time measurement"""
        if name not in self.times or not self.times[name]:
            return 0.0
        return self.times[name][-1]




def analyze_clustering(mask):
    """Analyze clustering properties of mask patterns"""
    batch_size, seq_len = mask.shape
    cluster_sizes = []

    for batch_idx in range(batch_size):
        mask_seq = mask[batch_idx].cpu().numpy()

        # Find connected components (clusters)
        in_cluster = False
        current_cluster_size = 0

        for pos in range(seq_len):
            if mask_seq[pos]:  # Masked position
                if not in_cluster:
                    in_cluster = True
                    current_cluster_size = 1
                else:
                    current_cluster_size += 1
            else:  # Unmasked position
                if in_cluster:
                    cluster_sizes.append(current_cluster_size)
                    in_cluster = False
                    current_cluster_size = 0

        # Handle cluster at end of sequence
        if in_cluster:
            cluster_sizes.append(current_cluster_size)

    if cluster_sizes:
        avg_cluster_size = sum(cluster_sizes) / len(cluster_sizes)
        max_cluster_size = max(cluster_sizes)
        num_clusters = len(cluster_sizes)
        return {
            'avg_cluster_size': avg_cluster_size,
            'max_cluster_size': max_cluster_size,
            'num_clusters_per_batch': num_clusters / batch_size
        }
    else:
        return {
            'avg_cluster_size': 0,
            'max_cluster_size': 0,
            'num_clusters_per_batch': 0
        }


def analyze_masking_patterns_with_transition(mask, iter_num, sticky_transition_start, sticky_transition_end):
    """Analyze masking patterns during independent->sticky transition"""
    mask_ratio = mask.float().mean().item()

    # Calculate current transition state
    if iter_num < sticky_transition_start:
        transition_state = "independent"
        sticky_ratio = 0.0
    elif iter_num >= sticky_transition_end:
        transition_state = "sticky"
        sticky_ratio = 1.0
    else:
        progress = (iter_num - sticky_transition_start) / (sticky_transition_end - sticky_transition_start)
        sticky_ratio = progress
        transition_state = f"transition ({sticky_ratio:.2f})"

    # Analyze clustering (more relevant during sticky phase)
    if sticky_ratio > 0.1:  # Only analyze clusters when some sticky masking present
        cluster_stats = analyze_clustering(mask)
        return {
            'mask_ratio': mask_ratio,
            'transition_state': transition_state,
            'sticky_ratio': sticky_ratio,
            **cluster_stats
        }
    else:
        return {
            'mask_ratio': mask_ratio,
            'transition_state': transition_state,
            'sticky_ratio': sticky_ratio
        }


def log_masking_stats(mask, iter_num, log_interval, sticky_transition_start=None, sticky_transition_end=None):
    """Log statistics about masking patterns"""
    if sticky_transition_start is not None and sticky_transition_end is not None:
        # Enhanced logging with transition tracking
        masking_stats = analyze_masking_patterns_with_transition(mask, iter_num, sticky_transition_start, sticky_transition_end)
        if iter_num % (log_interval * 10) == 0:  # Less frequent detailed stats
            print(f"Masking: {masking_stats}")
    else:
        # Original simple logging
        mask_ratio = mask.float().mean().item()
        batch_size, seq_len = mask.shape

        # Count consecutive masked regions
        mask_np = mask.cpu().numpy()
        consecutive_regions = []
        for batch_idx in range(batch_size):
            regions = []
            current_length = 0
            for pos in range(seq_len):
                if mask_np[batch_idx, pos]:
                    current_length += 1
                else:
                    if current_length > 0:
                        regions.append(current_length)
                        current_length = 0
            if current_length > 0:
                regions.append(current_length)
            consecutive_regions.extend(regions)

        avg_region_length = sum(consecutive_regions) / len(consecutive_regions) if consecutive_regions else 0

        if iter_num % (log_interval * 10) == 0:  # Less frequent detailed stats
            print(f"Masking stats: {mask_ratio:.3f} ratio, {avg_region_length:.1f} avg region length")


================================================================================
GLOBAL VARIABLES:
================================================================================
always_save_checkpoint = True
arg = .\config\shkspr_char_diff\optimal5.py
attention_type = bidirectional
backend = nccl
batch_size = 16
beta1 = 0.9
beta2 = 0.99
bias = False
block_size = 1024
ckpt_filename = 34.5_58.4_UM.pt
compile = True
config = {'out_dir': 'out', 'training_type': 'unmasking', 'eval_interval': 200, 'log_interval': 20, 'eval_iters': 20, 'eval_only': False, 'always_save_checkpoint': True, 'init_from': 'scratch', 'ckpt_filename': '34.5_58.4_UM.pt', 'n_layer': 6, 'n_head': 6, 'n_embd': 384, 'dropout': 0.2, 'bias': False, 'attention_type': 'bidirectional', 'use_rope': True, 'wandb_log': True, 'wandb_project': 'experiments_diffusion', 'wandb_run_name': 'shkspr_char_diff_moderate_first_unmasking', 'dataset': 'shakespeare_char', 'gradient_accumulation_steps': 8, 'batch_size': 16, 'block_size': 1024, 'use_paragraph_boundaries': False, 'use_all_stages_for_training': True, 'weight_loss_by_mask_ratio': True, 'enable_entropy_penalty': False, 'max_entropy_penalty': 3, 'entropy_penalty_start_iter': 2500, 'uncertainty_factor': 0.1, 'learning_rate': 0.0002, 'max_iters': 8000, 'warmup_iters': 2000, 'lr_decay_iters': 15000, 'min_lr': 3e-05, 'beta1': 0.9, 'beta2': 0.99, 'weight_decay': 0.02, 'grad_clip': 0.0, 'decay_lr': True, 'backend': 'nccl', 'device': 'cuda', 'dtype': 'float16', 'compile': True, 'start_iter_num': 0}
config_file = .\config\shkspr_char_diff\optimal5.py
config_keys = ['out_dir', 'training_type', 'eval_interval', 'log_interval', 'eval_iters', 'eval_only', 'always_save_checkpoint', 'init_from', 'ckpt_filename', 'n_layer', 'n_head', 'n_embd', 'dropout', 'bias', 'attention_type', 'use_rope', 'wandb_log', 'wandb_project', 'wandb_run_name', 'dataset', 'gradient_accumulation_steps', 'batch_size', 'block_size', 'use_paragraph_boundaries', 'use_all_stages_for_training', 'weight_loss_by_mask_ratio', 'enable_entropy_penalty', 'max_entropy_penalty', 'entropy_penalty_start_iter', 'uncertainty_factor', 'learning_rate', 'max_iters', 'warmup_iters', 'lr_decay_iters', 'min_lr', 'beta1', 'beta2', 'weight_decay', 'grad_clip', 'decay_lr', 'backend', 'device', 'dtype', 'compile', 'start_iter_num']
dataset = shakespeare_char
decay_lr = True
device = cuda
dropout = 0.2
dtype = float16
enable_entropy_penalty = False
entropy_penalty_start_iter = 2500
eval_interval = 200
eval_iters = 20
eval_only = False
f = <_io.TextIOWrapper name='utils.py' mode='r' encoding='cp1250'>
file_path = C:\Users\Adam\AppData\Local\Programs\Python\Python310\lib\site-packages\torch\_dynamo\__init__.py
filename = utils.py
grad_clip = 0.0
gradient_accumulation_steps = 8
init_from = scratch
learning_rate = 0.0002
local_files = ['model.py', 'train_run.py', 'utils.py']
log_interval = 20
lr_decay_iters = 15000
math = <module 'math' (built-in)>
max_entropy_penalty = 3
max_iters = 8000
min_lr = 3e-05
module = <module 'torch._dynamo' from 'C:\\Users\\Adam\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\_dynamo\\__init__.py'>
module_name = torch._dynamo
n_embd = 384
n_head = 6
n_layer = 6
np = <module 'numpy' from 'C:\\Users\\Adam\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\numpy\\__init__.py'>
os = <module 'os' from 'C:\\Users\\Adam\\AppData\\Local\\Programs\\Python\\Python310\\lib\\os.py'>
out_dir = out
pickle = <module 'pickle' from 'C:\\Users\\Adam\\AppData\\Local\\Programs\\Python\\Python310\\lib\\pickle.py'>
start_iter_num = 0
sys = <module 'sys' (built-in)>
threading = <module 'threading' from 'C:\\Users\\Adam\\AppData\\Local\\Programs\\Python\\Python310\\lib\\threading.py'>
time = <module 'time' (built-in)>
timer = <utils.Timer object at 0x0000025C442E9DE0>
torch = <module 'torch' from 'C:\\Users\\Adam\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\__init__.py'>
training_type = unmasking
uncertainty_factor = 0.1
unmasking_stages = [{'type': 'sticky', 'target_masked_ratio': 0.4, 'p1_probability': 0.15, 'p2_probability': 0.3, 'val_loss_stale_count': 6}, {'type': 'sticky', 'target_masked_ratio': 0.6, 'p1_probability': 0.1, 'p2_probability': 0.5, 'val_loss_stale_count': 8}, {'type': 'random', 'max_masked_ratio': 0.5, 'val_loss_stale_count': 10}, {'type': 'sticky', 'target_masked_ratio': 0.6, 'p1_probability': 0.3, 'p2_probability': 0.1, 'val_loss_stale_count': 8}, {'type': 'sticky', 'target_masked_ratio': 0.6, 'p1_probability': 0.1, 'p2_probability': 0.5, 'val_loss_stale_count': 8}, {'type': 'random', 'max_masked_ratio': 0.2, 'val_loss_stale_count': 10}, {'type': 'sticky', 'target_masked_ratio': 0.55, 'p1_probability': 0.1, 'p2_probability': 0.6, 'val_loss_stale_count': 10}, {'type': 'sticky', 'target_masked_ratio': 0.9, 'p1_probability': 0.1, 'p2_probability': 0.9, 'val_loss_stale_count': 20}]
use_all_stages_for_training = True
use_paragraph_boundaries = False
use_rope = True
validation_stages = [{'type': 'sticky', 'target_masked_ratio': 0.4, 'p1_probability': 0.15, 'p2_probability': 0.3, 'val_loss_stale_count': 6}, {'type': 'sticky', 'target_masked_ratio': 0.6, 'p1_probability': 0.1, 'p2_probability': 0.5, 'val_loss_stale_count': 8}, {'type': 'random', 'max_masked_ratio': 0.5, 'val_loss_stale_count': 10}, {'type': 'sticky', 'target_masked_ratio': 0.6, 'p1_probability': 0.3, 'p2_probability': 0.1, 'val_loss_stale_count': 8}, {'type': 'sticky', 'target_masked_ratio': 0.6, 'p1_probability': 0.1, 'p2_probability': 0.5, 'val_loss_stale_count': 8}, {'type': 'random', 'max_masked_ratio': 0.2, 'val_loss_stale_count': 10}, {'type': 'sticky', 'target_masked_ratio': 0.2, 'p1_probability': 0.3, 'p2_probability': 0.0, 'val_loss_stale_count': 2}, {'type': 'sticky', 'target_masked_ratio': 0.4, 'p1_probability': 0.3, 'p2_probability': 0.0, 'val_loss_stale_count': 4}, {'type': 'sticky', 'target_masked_ratio': 0.4, 'p1_probability': 0.15, 'p2_probability': 0.3, 'val_loss_stale_count': 6}, {'type': 'sticky', 'target_masked_ratio': 0.55, 'p1_probability': 0.1, 'p2_probability': 0.6, 'val_loss_stale_count': 10}, {'type': 'sticky', 'target_masked_ratio': 0.7, 'p1_probability': 0.2, 'p2_probability': 0.4, 'val_loss_stale_count': 15}, {'type': 'sticky', 'target_masked_ratio': 0.8, 'p1_probability': 0.2, 'p2_probability': 0.4, 'val_loss_stale_count': 20}, {'type': 'sticky', 'target_masked_ratio': 0.8, 'p1_probability': 0.1, 'p2_probability': 0.9, 'val_loss_stale_count': 20}, {'type': 'sticky', 'target_masked_ratio': 0.9, 'p1_probability': 0.1, 'p2_probability': 0.9, 'val_loss_stale_count': 20}]
wandb_log = True
wandb_project = experiments_diffusion
wandb_run_name = shkspr_char_diff_moderate_first_unmasking
warmup_iters = 2000
weight_decay = 0.02
weight_loss_by_mask_ratio = True

================================================================================
tokens per iteration will be: 131,072
found vocab_size = 65 (inside data\shakespeare_char\meta.pkl)
mask_token_id = 65, extended_vocab_size = 80
DEBUG: init_from='scratch', checkpoint_training_context=None
DEBUG: NOT applying training context. init_from='scratch', checkpoint_training_context=False
Initializing a new model from scratch
Using bidirectional attention
Using bidirectional attention
Using bidirectional attention
Using bidirectional attention
Using bidirectional attention
Using bidirectional attention
number of parameters: 10.65M
num decayed parameter tensors: 25, with 10,647,552 parameters
num non-decayed parameter tensors: 13, with 4,992 parameters
using fused AdamW: True
compiling the model... (takes a ~minute)

*** STAGE-BASED UNMASKING TRAINING INITIALIZED ***
Starting at Stage 0:
  Stage type: sticky
  Target masked ratio: 0.4
  P1 probability: 0.15
  P2 probability: 0.3
  Val loss stale count limit: 6
Total stages configured: 8
*** STAGE INITIALIZATION COMPLETE ***

Pre-creating validation set...
Creating validation set with samples from all stages...
  Stage 0 (sticky): 23 samples (target_ratio=0.4, p1=0.1, p2=0.3)
  Stage 1 (sticky): 23 samples (target_ratio=0.6, p1=0.1, p2=0.5)
  Stage 2 (random): 23 samples (max_ratio=0.5)
  Stage 3 (sticky): 23 samples (target_ratio=0.6, p1=0.3, p2=0.1)
  Stage 4 (sticky): 23 samples (target_ratio=0.6, p1=0.1, p2=0.5)
  Stage 5 (random): 23 samples (max_ratio=0.2)
  Stage 6 (sticky): 23 samples (target_ratio=0.2, p1=0.3, p2=0.0)
  Stage 7 (sticky): 23 samples (target_ratio=0.4, p1=0.3, p2=0.0)
  Stage 8 (sticky): 23 samples (target_ratio=0.4, p1=0.1, p2=0.3)
  Stage 9 (sticky): 23 samples (target_ratio=0.6, p1=0.1, p2=0.6)
  Stage 10 (sticky): 23 samples (target_ratio=0.7, p1=0.2, p2=0.4)
  Stage 11 (sticky): 23 samples (target_ratio=0.8, p1=0.2, p2=0.4)
  Stage 12 (sticky): 22 samples (target_ratio=0.8, p1=0.1, p2=0.9)
  Stage 13 (sticky): 22 samples (target_ratio=0.9, p1=0.1, p2=0.9)
Validation set created: 28 batches, 320 total samples
Training will generate fresh batches from all stages each iteration
Starting training loop...

--- Starting validation at iteration 0 ---
Using validation set with samples from all 14 stages
  Validation complete: 20 batches processed (320 samples), avg loss = 7.7381
  Per-stage validation losses:
    Stage 0 (sticky): 6.9721 (23 samples) - ratio=0.4
    Stage 1 (sticky): 5.7179 (16 samples) - ratio=0.6
    Stage 2 (random): 7.3675 (23 samples) - max_ratio=0.5
    Stage 3 (sticky): 8.2115 (7 samples) - ratio=0.6
    Stage 4 (sticky): 5.7115 (16 samples) - ratio=0.6
    Stage 5 (random): 5.7218 (23 samples) - max_ratio=0.2
    Stage 6 (sticky): 5.7149 (7 samples) - ratio=0.2
    Stage 7 (sticky): 14.8302 (16 samples) - ratio=0.4
    Stage 8 (sticky): 12.4162 (23 samples) - ratio=0.4
    Stage 9 (sticky): 9.8029 (7 samples) - ratio=0.6
    Stage 10 (sticky): 6.9619 (23 samples) - ratio=0.7
    Stage 11 (sticky): 6.9726 (16 samples) - ratio=0.8
    Stage 12 (sticky): 6.9734 (7 samples) - ratio=0.8
    Stage 13 (sticky): 5.9743 (23 samples) - ratio=0.9
--- Validation complete ---
step 0: train loss 6.2713, val loss 7.7381, lr 0.000000
Stage 0 (sticky): target_ratio=0.4, p1=0.1, p2=0.3, stale_count=0
  val model vs random: 1.10x better
  val avg correct prob: 0.0138 (random: 0.0125)
  val signal to noise: 0.45 (median: 0.37)
  Most likely guess correct P %: 5.9%
  Stage 0: New best val loss 7.7381, reset stale count to 0

iter 0: loss 6.3102, time 52807.97ms, mfu -100.00%
  data: 172.0ms, grad_accum: 858.6ms (fw: 13.3ms, bw: 124.1ms)
  grad_proc: 15.0ms, optimizer: 5.0ms, param_check: 14.2ms
  loss_proc: 0.0ms, instability: 70.5ms
  cleanup: 0.0ms, gpu_sync: 1.0ms
  measured: 893.7ms, unaccounted: 51914.2ms (98.3%)
  validation: 48559.06ms (data: 26.09ms, forward: 0.00ms, loss: 1183.05ms)
Masking: stage=0 (sticky), actual_ratio=0.508, target=0.4, p1=0.1, p2=0.3
iter 20: loss 5.6154, time 703.81ms, mfu 5.51%
  data: 172.9ms, grad_accum: 668.3ms (fw: 12.8ms, bw: 123.7ms)
  grad_proc: 15.6ms, optimizer: 1.5ms, param_check: 15.8ms
  loss_proc: 0.0ms, instability: 47.2ms
  cleanup: 0.2ms, gpu_sync: 1.0ms
  measured: 702.4ms, unaccounted: 1.4ms (0.2%)
iter 40: loss 5.1518, time 759.74ms, mfu 5.46%
  data: 210.8ms, grad_accum: 749.0ms (fw: 18.6ms, bw: 131.1ms)
  grad_proc: 20.1ms, optimizer: 2.0ms, param_check: 22.7ms
  loss_proc: 0.0ms, instability: 44.7ms
  cleanup: 0.5ms, gpu_sync: 0.7ms
  measured: 795.0ms, unaccounted: -35.3ms (-4.6%)
iter 60: loss 5.0004, time 705.29ms, mfu 5.47%
  data: 178.1ms, grad_accum: 684.9ms (fw: 12.8ms, bw: 128.1ms)
  grad_proc: 17.4ms, optimizer: 1.5ms, param_check: 17.1ms
  loss_proc: 0.0ms, instability: 47.8ms
  cleanup: 0.3ms, gpu_sync: 0.6ms
  measured: 721.7ms, unaccounted: -16.4ms (-2.3%)
iter 80: loss 5.0531, time 781.98ms, mfu 5.42%
  data: 182.6ms, grad_accum: 702.1ms (fw: 12.9ms, bw: 129.4ms)
  grad_proc: 16.1ms, optimizer: 1.7ms, param_check: 17.8ms
  loss_proc: 0.0ms, instability: 53.2ms
  cleanup: 0.3ms, gpu_sync: 0.7ms
  measured: 738.6ms, unaccounted: 43.4ms (5.5%)
iter 100: loss 4.7945, time 705.22ms, mfu 5.42%
  data: 174.2ms, grad_accum: 687.5ms (fw: 12.3ms, bw: 125.7ms)
  grad_proc: 16.8ms, optimizer: 1.5ms, param_check: 17.0ms
  loss_proc: 0.0ms, instability: 48.6ms
  cleanup: 0.2ms, gpu_sync: 0.7ms
  measured: 723.6ms, unaccounted: -18.3ms (-2.6%)
iter 120: loss 4.9655, time 723.19ms, mfu 5.42%
  data: 177.5ms, grad_accum: 698.5ms (fw: 12.2ms, bw: 127.9ms)
  grad_proc: 16.2ms, optimizer: 1.6ms, param_check: 16.7ms
  loss_proc: 0.0ms, instability: 49.4ms
  cleanup: 0.5ms, gpu_sync: 0.6ms
  measured: 734.2ms, unaccounted: -11.0ms (-1.5%)
iter 140: loss 4.8352, time 747.60ms, mfu 5.39%
  data: 179.4ms, grad_accum: 727.4ms (fw: 12.4ms, bw: 131.3ms)
  grad_proc: 15.8ms, optimizer: 1.5ms, param_check: 17.1ms
  loss_proc: 0.0ms, instability: 53.1ms
  cleanup: 0.5ms, gpu_sync: 0.7ms
  measured: 763.0ms, unaccounted: -15.4ms (-2.1%)
iter 160: loss 4.7060, time 778.38ms, mfu 5.35%
  data: 187.0ms, grad_accum: 768.2ms (fw: 12.5ms, bw: 138.5ms)
  grad_proc: 15.4ms, optimizer: 1.5ms, param_check: 16.5ms
  loss_proc: 0.1ms, instability: 55.6ms
  cleanup: 0.1ms, gpu_sync: 0.6ms
  measured: 802.4ms, unaccounted: -24.0ms (-3.1%)
iter 180: loss 4.7791, time 875.68ms, mfu 5.26%
  data: 200.8ms, grad_accum: 823.6ms (fw: 14.6ms, bw: 143.7ms)
  grad_proc: 17.5ms, optimizer: 1.8ms, param_check: 18.2ms
  loss_proc: 0.0ms, instability: 64.2ms
  cleanup: 0.3ms, gpu_sync: 0.5ms
  measured: 861.9ms, unaccounted: 13.8ms (1.6%)

--- Starting validation at iteration 200 ---
Using validation set with samples from all 14 stages
  Validation complete: 20 batches processed (320 samples), avg loss = 5.9718
  Per-stage validation losses:
    Stage 0 (sticky): 5.3518 (23 samples) - ratio=0.4
    Stage 1 (sticky): 4.4087 (16 samples) - ratio=0.6
    Stage 2 (random): 5.6852 (23 samples) - max_ratio=0.5
    Stage 3 (sticky): 6.3667 (7 samples) - ratio=0.6
    Stage 4 (sticky): 4.4061 (16 samples) - ratio=0.6
    Stage 5 (random): 4.3963 (23 samples) - max_ratio=0.2
    Stage 6 (sticky): 4.3684 (7 samples) - ratio=0.2
    Stage 7 (sticky): 11.6952 (16 samples) - ratio=0.4
    Stage 8 (sticky): 9.5094 (23 samples) - ratio=0.4
    Stage 9 (sticky): 7.6248 (7 samples) - ratio=0.6
    Stage 10 (sticky): 5.4059 (23 samples) - ratio=0.7
    Stage 11 (sticky): 5.3324 (16 samples) - ratio=0.8
    Stage 12 (sticky): 5.4348 (7 samples) - ratio=0.8
    Stage 13 (sticky): 4.5508 (23 samples) - ratio=0.9
--- Validation complete ---
step 200: train loss 4.7332, val loss 5.9718, lr 0.000020
Stage 0 (sticky): target_ratio=0.4, p1=0.1, p2=0.3, stale_count=0
  val model vs random: 8.66x better
  val avg correct prob: 0.1083 (random: 0.0125)
  val signal to noise: 4.24 (median: 0.27)
  Most likely guess correct P %: 21.8%
  Stage 0: New best val loss 5.9718, reset stale count to 0

saving checkpoint to out/ckpt_unmasking_200.pt
iter 200: loss 4.8130, time 4586.78ms, mfu 4.82%
  data: 206.9ms, grad_accum: 840.8ms (fw: 14.4ms, bw: 151.9ms)
  grad_proc: 16.9ms, optimizer: 1.7ms, param_check: 18.0ms
  loss_proc: 0.0ms, instability: 60.7ms
  cleanup: 0.4ms, gpu_sync: 0.4ms
  measured: 878.2ms, unaccounted: 3708.6ms (80.9%)
  validation: 26003.25ms (data: 27.19ms, forward: 0.00ms, loss: 617.63ms)
Masking: stage=0 (sticky), actual_ratio=0.493, target=0.4, p1=0.1, p2=0.3
iter 220: loss 4.6685, time 861.50ms, mfu 4.79%
  data: 193.8ms, grad_accum: 805.4ms (fw: 12.2ms, bw: 146.3ms)
  grad_proc: 16.6ms, optimizer: 1.5ms, param_check: 17.8ms
  loss_proc: 0.0ms, instability: 64.1ms
  cleanup: 0.3ms, gpu_sync: 0.4ms
  measured: 842.0ms, unaccounted: 19.5ms (2.3%)
iter 240: loss 4.6508, time 856.07ms, mfu 4.76%
  data: 200.1ms, grad_accum: 839.1ms (fw: 12.1ms, bw: 148.9ms)
  grad_proc: 18.0ms, optimizer: 1.7ms, param_check: 17.8ms
  loss_proc: 0.0ms, instability: 64.0ms
  cleanup: 0.4ms, gpu_sync: 0.4ms
  measured: 877.4ms, unaccounted: -21.3ms (-2.5%)
iter 260: loss 4.8014, time 985.06ms, mfu 4.68%
  data: 246.9ms, grad_accum: 955.3ms (fw: 17.8ms, bw: 162.7ms)
  grad_proc: 20.3ms, optimizer: 2.6ms, param_check: 36.5ms
  loss_proc: 0.0ms, instability: 69.5ms
  cleanup: 0.4ms, gpu_sync: 0.5ms
  measured: 1015.6ms, unaccounted: -30.6ms (-3.1%)
iter 280: loss 4.7475, time 1031.69ms, mfu 4.59%
  data: 208.5ms, grad_accum: 876.0ms (fw: 13.5ms, bw: 155.0ms)
  grad_proc: 16.4ms, optimizer: 1.5ms, param_check: 18.1ms
  loss_proc: 0.0ms, instability: 79.5ms
  cleanup: 0.4ms, gpu_sync: 0.5ms
  measured: 912.8ms, unaccounted: 118.8ms (11.5%)
iter 300: loss 4.7940, time 779.42ms, mfu 4.62%
  data: 197.0ms, grad_accum: 812.9ms (fw: 12.2ms, bw: 148.0ms)
  grad_proc: 16.2ms, optimizer: 1.6ms, param_check: 17.9ms
  loss_proc: 0.0ms, instability: 57.8ms
  cleanup: 0.2ms, gpu_sync: 0.4ms
  measured: 849.3ms, unaccounted: -69.9ms (-9.0%)
iter 320: loss 4.8350, time 860.88ms, mfu 4.61%
  data: 203.3ms, grad_accum: 857.3ms (fw: 12.4ms, bw: 154.4ms)
  grad_proc: 17.0ms, optimizer: 1.6ms, param_check: 18.3ms
  loss_proc: 0.0ms, instability: 61.9ms
  cleanup: 0.2ms, gpu_sync: 0.4ms
  measured: 894.7ms, unaccounted: -33.9ms (-3.9%)
iter 340: loss 4.8251, time 867.04ms, mfu 4.60%
  data: 201.7ms, grad_accum: 854.8ms (fw: 12.5ms, bw: 152.8ms)
  grad_proc: 16.6ms, optimizer: 1.4ms, param_check: 18.0ms
  loss_proc: 0.0ms, instability: 63.8ms
  cleanup: 0.0ms, gpu_sync: 0.4ms
  measured: 891.2ms, unaccounted: -24.2ms (-2.8%)
iter 360: loss 4.7580, time 888.35ms, mfu 4.57%
  data: 201.6ms, grad_accum: 846.1ms (fw: 12.0ms, bw: 151.6ms)
  grad_proc: 17.5ms, optimizer: 1.6ms, param_check: 18.7ms
  loss_proc: 0.0ms, instability: 66.4ms
  cleanup: 0.3ms, gpu_sync: 0.5ms
  measured: 884.7ms, unaccounted: 3.6ms (0.4%)
iter 380: loss 4.7600, time 967.86ms, mfu 4.52%
  data: 203.2ms, grad_accum: 862.1ms (fw: 13.3ms, bw: 154.8ms)
  grad_proc: 16.5ms, optimizer: 1.5ms, param_check: 18.2ms
  loss_proc: 0.0ms, instability: 70.9ms
  cleanup: 0.3ms, gpu_sync: 0.5ms
  measured: 899.0ms, unaccounted: 68.9ms (7.1%)

--- Starting validation at iteration 400 ---
Using validation set with samples from all 14 stages
  Validation complete: 20 batches processed (320 samples), avg loss = 5.9504
  Per-stage validation losses:
    Stage 0 (sticky): 5.3351 (23 samples) - ratio=0.4
    Stage 1 (sticky): 4.3988 (16 samples) - ratio=0.6
    Stage 2 (random): 5.6692 (23 samples) - max_ratio=0.5
    Stage 3 (sticky): 6.3644 (7 samples) - ratio=0.6
    Stage 4 (sticky): 4.3934 (16 samples) - ratio=0.6
    Stage 5 (random): 4.3816 (23 samples) - max_ratio=0.2
    Stage 6 (sticky): 4.3523 (7 samples) - ratio=0.2
    Stage 7 (sticky): 11.6335 (16 samples) - ratio=0.4
    Stage 8 (sticky): 9.4645 (23 samples) - ratio=0.4
    Stage 9 (sticky): 7.6164 (7 samples) - ratio=0.6
    Stage 10 (sticky): 5.3794 (23 samples) - ratio=0.7
    Stage 11 (sticky): 5.3117 (16 samples) - ratio=0.8
    Stage 12 (sticky): 5.4113 (7 samples) - ratio=0.8
    Stage 13 (sticky): 4.5338 (23 samples) - ratio=0.9
--- Validation complete ---
step 400: train loss 4.7170, val loss 5.9504, lr 0.000040
Stage 0 (sticky): target_ratio=0.4, p1=0.1, p2=0.3, stale_count=0
  val model vs random: 9.35x better
  val avg correct prob: 0.1169 (random: 0.0125)
  val signal to noise: 4.70 (median: 0.30)
  Most likely guess correct P %: 22.1%
  Stage 0: New best val loss 5.9504, reset stale count to 0

saving checkpoint to out/ckpt_unmasking_400.pt
iter 400: loss 4.6096, time 4652.41ms, mfu 4.15%
  data: 201.6ms, grad_accum: 852.7ms (fw: 12.5ms, bw: 150.5ms)
  grad_proc: 16.8ms, optimizer: 1.3ms, param_check: 17.7ms
  loss_proc: 0.1ms, instability: 77.4ms
  cleanup: 0.2ms, gpu_sync: 0.5ms
  measured: 889.2ms, unaccounted: 3763.2ms (80.9%)
  validation: 18483.46ms (data: 21.40ms, forward: 0.00ms, loss: 50.87ms)
Masking: stage=0 (sticky), actual_ratio=0.499, target=0.4, p1=0.1, p2=0.3
iter 420: loss 4.7763, time 872.84ms, mfu 4.18%
  data: 207.4ms, grad_accum: 877.9ms (fw: 12.3ms, bw: 157.7ms)
  grad_proc: 16.3ms, optimizer: 1.4ms, param_check: 18.5ms
  loss_proc: 0.0ms, instability: 64.8ms
  cleanup: 0.4ms, gpu_sync: 0.4ms
  measured: 914.9ms, unaccounted: -42.1ms (-4.8%)
iter 440: loss 4.7426, time 930.82ms, mfu 4.18%
  data: 209.2ms, grad_accum: 886.9ms (fw: 12.4ms, bw: 160.2ms)
  grad_proc: 18.1ms, optimizer: 1.7ms, param_check: 19.7ms
  loss_proc: 0.0ms, instability: 70.6ms
  cleanup: 0.4ms, gpu_sync: 0.5ms
  measured: 927.4ms, unaccounted: 3.4ms (0.4%)
iter 460: loss 4.6630, time 891.66ms, mfu 4.19%
  data: 199.2ms, grad_accum: 834.7ms (fw: 13.4ms, bw: 149.2ms)
  grad_proc: 18.8ms, optimizer: 1.7ms, param_check: 18.1ms
  loss_proc: 0.0ms, instability: 63.4ms
  cleanup: 0.1ms, gpu_sync: 0.5ms
  measured: 874.0ms, unaccounted: 17.7ms (2.0%)
iter 480: loss 4.6534, time 907.85ms, mfu 4.20%
  data: 208.0ms, grad_accum: 874.7ms (fw: 12.5ms, bw: 154.8ms)
  grad_proc: 17.1ms, optimizer: 1.5ms, param_check: 18.0ms
  loss_proc: 0.0ms, instability: 72.2ms
  cleanup: 0.5ms, gpu_sync: 0.4ms
  measured: 912.1ms, unaccounted: -4.3ms (-0.5%)
iter 500: loss 4.8138, time 930.94ms, mfu 4.20%
  data: 206.0ms, grad_accum: 874.7ms (fw: 12.6ms, bw: 155.0ms)
  grad_proc: 17.3ms, optimizer: 1.4ms, param_check: 18.7ms
  loss_proc: 0.0ms, instability: 70.2ms
  cleanup: 0.2ms, gpu_sync: 0.4ms
  measured: 912.7ms, unaccounted: 18.2ms (2.0%)
iter 520: loss 4.6438, time 924.71ms, mfu 4.20%
  data: 206.0ms, grad_accum: 878.2ms (fw: 12.4ms, bw: 156.3ms)
  grad_proc: 17.6ms, optimizer: 1.5ms, param_check: 18.6ms
  loss_proc: 0.0ms, instability: 70.0ms
  cleanup: 0.2ms, gpu_sync: 0.5ms
  measured: 916.5ms, unaccounted: 8.2ms (0.9%)
iter 540: loss 4.6126, time 870.14ms, mfu 4.22%
  data: 200.9ms, grad_accum: 853.9ms (fw: 12.3ms, bw: 151.3ms)
  grad_proc: 16.6ms, optimizer: 1.6ms, param_check: 18.5ms
  loss_proc: 0.1ms, instability: 62.5ms
  cleanup: 0.3ms, gpu_sync: 0.4ms
  measured: 891.4ms, unaccounted: -21.3ms (-2.4%)
iter 560: loss 4.5548, time 910.34ms, mfu 4.22%
  data: 218.3ms, grad_accum: 918.8ms (fw: 12.3ms, bw: 167.3ms)
  grad_proc: 17.2ms, optimizer: 1.5ms, param_check: 19.3ms
  loss_proc: 0.0ms, instability: 68.9ms
  cleanup: 0.4ms, gpu_sync: 0.3ms
  measured: 957.5ms, unaccounted: -47.1ms (-5.2%)
iter 580: loss 4.7469, time 931.16ms, mfu 4.22%
  data: 212.9ms, grad_accum: 911.8ms (fw: 14.0ms, bw: 164.1ms)
  grad_proc: 17.5ms, optimizer: 1.6ms, param_check: 18.3ms
  loss_proc: 0.0ms, instability: 69.7ms
  cleanup: 0.3ms, gpu_sync: 0.3ms
  measured: 949.7ms, unaccounted: -18.5ms (-2.0%)

--- Starting validation at iteration 600 ---
Using validation set with samples from all 14 stages
  Validation complete: 20 batches processed (320 samples), avg loss = 5.6113
  Per-stage validation losses:
    Stage 0 (sticky): 5.0681 (23 samples) - ratio=0.4
    Stage 1 (sticky): 4.2625 (16 samples) - ratio=0.6
    Stage 2 (random): 5.4097 (23 samples) - max_ratio=0.5
    Stage 3 (sticky): 5.9070 (7 samples) - ratio=0.6
    Stage 4 (sticky): 4.1904 (16 samples) - ratio=0.6
    Stage 5 (random): 4.2066 (23 samples) - max_ratio=0.2
    Stage 6 (sticky): 4.2103 (7 samples) - ratio=0.2
    Stage 7 (sticky): 10.6812 (16 samples) - ratio=0.4
    Stage 8 (sticky): 8.7775 (23 samples) - ratio=0.4
    Stage 9 (sticky): 7.0847 (7 samples) - ratio=0.6
    Stage 10 (sticky): 5.0205 (23 samples) - ratio=0.7
    Stage 11 (sticky): 5.0346 (16 samples) - ratio=0.8
    Stage 12 (sticky): 5.1115 (7 samples) - ratio=0.8
    Stage 13 (sticky): 4.3896 (23 samples) - ratio=0.9
--- Validation complete ---
step 600: train loss 4.6051, val loss 5.6113, lr 0.000060
Stage 0 (sticky): target_ratio=0.4, p1=0.1, p2=0.3, stale_count=0
  val model vs random: 9.96x better
  val avg correct prob: 0.1245 (random: 0.0125)
  val signal to noise: 4.23 (median: 0.34)
  Most likely guess correct P %: 24.9%
  Stage 0: New best val loss 5.6113, reset stale count to 0

saving checkpoint to out/ckpt_unmasking_600.pt
iter 600: loss 4.5424, time 4452.95ms, mfu 3.88%
  data: 204.8ms, grad_accum: 854.1ms (fw: 12.4ms, bw: 155.4ms)
  grad_proc: 18.5ms, optimizer: 1.7ms, param_check: 19.3ms
  loss_proc: 0.0ms, instability: 70.0ms
  cleanup: 0.4ms, gpu_sync: 0.3ms
  measured: 894.4ms, unaccounted: 3558.6ms (79.9%)
  validation: 14693.15ms (data: 19.61ms, forward: 0.00ms, loss: 53.83ms)
Masking: stage=0 (sticky), actual_ratio=0.484, target=0.4, p1=0.1, p2=0.3
iter 620: loss 4.6123, time 934.18ms, mfu 3.91%
  data: 215.4ms, grad_accum: 920.4ms (fw: 12.4ms, bw: 166.3ms)
  grad_proc: 17.4ms, optimizer: 1.6ms, param_check: 19.0ms
  loss_proc: 0.0ms, instability: 70.3ms
  cleanup: 0.4ms, gpu_sync: 0.4ms
  measured: 959.1ms, unaccounted: -25.0ms (-2.7%)
iter 640: loss 4.3832, time 914.83ms, mfu 3.94%
  data: 211.9ms, grad_accum: 908.8ms (fw: 13.1ms, bw: 161.7ms)
  grad_proc: 17.9ms, optimizer: 1.4ms, param_check: 20.2ms
  loss_proc: 0.0ms, instability: 71.7ms
  cleanup: 0.3ms, gpu_sync: 0.4ms
  measured: 949.0ms, unaccounted: -34.1ms (-3.7%)
iter 660: loss 4.3994, time 925.63ms, mfu 3.97%
  data: 202.7ms, grad_accum: 862.9ms (fw: 12.8ms, bw: 152.8ms)
  grad_proc: 16.1ms, optimizer: 1.8ms, param_check: 17.4ms
  loss_proc: 0.0ms, instability: 69.8ms
  cleanup: 0.5ms, gpu_sync: 0.3ms
  measured: 898.9ms, unaccounted: 26.7ms (2.9%)
iter 680: loss 4.3312, time 1004.90ms, mfu 3.96%
  data: 209.3ms, grad_accum: 897.3ms (fw: 13.1ms, bw: 158.6ms)
  grad_proc: 17.0ms, optimizer: 1.2ms, param_check: 18.3ms
  loss_proc: 0.0ms, instability: 76.2ms
  cleanup: 0.3ms, gpu_sync: 0.4ms
  measured: 934.5ms, unaccounted: 70.4ms (7.0%)
iter 700: loss 4.2164, time 890.69ms, mfu 4.00%
  data: 209.8ms, grad_accum: 892.4ms (fw: 13.0ms, bw: 160.2ms)
  grad_proc: 18.1ms, optimizer: 1.5ms, param_check: 18.6ms
  loss_proc: 0.0ms, instability: 65.8ms
  cleanup: 0.4ms, gpu_sync: 0.6ms
  measured: 931.5ms, unaccounted: -40.8ms (-4.6%)
iter 720: loss 4.3678, time 922.21ms, mfu 4.02%
  data: 216.3ms, grad_accum: 902.2ms (fw: 12.5ms, bw: 165.6ms)
  grad_proc: 18.7ms, optimizer: 1.2ms, param_check: 19.4ms
  loss_proc: 0.0ms, instability: 69.5ms
  cleanup: 0.7ms, gpu_sync: 0.6ms
  measured: 942.7ms, unaccounted: -20.5ms (-2.2%)
iter 740: loss 4.1655, time 926.16ms, mfu 4.03%
  data: 217.3ms, grad_accum: 926.0ms (fw: 12.3ms, bw: 167.4ms)
  grad_proc: 17.3ms, optimizer: 1.7ms, param_check: 18.5ms
  loss_proc: 0.0ms, instability: 70.6ms
  cleanup: 0.2ms, gpu_sync: 0.6ms
  measured: 964.3ms, unaccounted: -38.2ms (-4.1%)
iter 760: loss 4.2200, time 922.78ms, mfu 4.05%
  data: 209.7ms, grad_accum: 887.8ms (fw: 12.8ms, bw: 159.1ms)
  grad_proc: 15.2ms, optimizer: 1.5ms, param_check: 18.1ms
  loss_proc: 0.0ms, instability: 69.8ms
  cleanup: 0.1ms, gpu_sync: 0.7ms
  measured: 923.4ms, unaccounted: -0.6ms (-0.1%)
iter 780: loss 4.2333, time 1015.29ms, mfu 4.03%
  data: 215.5ms, grad_accum: 908.8ms (fw: 13.0ms, bw: 165.0ms)
  grad_proc: 17.8ms, optimizer: 1.3ms, param_check: 18.8ms
  loss_proc: 0.0ms, instability: 80.3ms
  cleanup: 0.3ms, gpu_sync: 0.7ms
  measured: 947.6ms, unaccounted: 67.7ms (6.7%)

--- Starting validation at iteration 800 ---
Using validation set with samples from all 14 stages
  Validation complete: 20 batches processed (320 samples), avg loss = 4.5687
  Per-stage validation losses:
    Stage 0 (sticky): 4.1609 (23 samples) - ratio=0.4
    Stage 1 (sticky): 3.7782 (16 samples) - ratio=0.6
    Stage 2 (random): 4.5488 (23 samples) - max_ratio=0.5
    Stage 3 (sticky): 4.6440 (7 samples) - ratio=0.6
    Stage 4 (sticky): 3.5305 (16 samples) - ratio=0.6
    Stage 5 (random): 3.6155 (23 samples) - max_ratio=0.2
    Stage 6 (sticky): 3.6981 (7 samples) - ratio=0.2
    Stage 7 (sticky): 8.1603 (16 samples) - ratio=0.4
    Stage 8 (sticky): 6.7364 (23 samples) - ratio=0.4
    Stage 9 (sticky): 5.5021 (7 samples) - ratio=0.6
    Stage 10 (sticky): 3.9436 (23 samples) - ratio=0.7
    Stage 11 (sticky): 4.1382 (16 samples) - ratio=0.8
    Stage 12 (sticky): 4.2135 (7 samples) - ratio=0.8
    Stage 13 (sticky): 3.8499 (23 samples) - ratio=0.9
--- Validation complete ---
step 800: train loss 4.0786, val loss 4.5687, lr 0.000080
Stage 0 (sticky): target_ratio=0.4, p1=0.1, p2=0.3, stale_count=0
  val model vs random: 17.89x better
  val avg correct prob: 0.2236 (random: 0.0125)
  val signal to noise: 7.06 (median: 0.52)
  Most likely guess correct P %: 36.8%
  Stage 0: New best val loss 4.5687, reset stale count to 0

saving checkpoint to out/ckpt_unmasking_800.pt
iter 800: loss 4.1874, time 4515.95ms, mfu 3.71%
  data: 220.8ms, grad_accum: 912.7ms (fw: 12.5ms, bw: 166.1ms)
  grad_proc: 16.2ms, optimizer: 1.3ms, param_check: 19.4ms
  loss_proc: 0.0ms, instability: 72.7ms
  cleanup: 0.3ms, gpu_sync: 0.7ms
  measured: 950.5ms, unaccounted: 3565.4ms (79.0%)
  validation: 12434.11ms (data: 20.42ms, forward: 0.00ms, loss: 54.11ms)
Masking: stage=0 (sticky), actual_ratio=0.515, target=0.4, p1=0.1, p2=0.3
iter 820: loss 4.0186, time 943.47ms, mfu 3.75%
  data: 217.4ms, grad_accum: 919.9ms (fw: 13.8ms, bw: 165.7ms)
  grad_proc: 17.5ms, optimizer: 1.7ms, param_check: 18.5ms
  loss_proc: 0.1ms, instability: 71.4ms
  cleanup: 0.1ms, gpu_sync: 0.6ms
  measured: 958.3ms, unaccounted: -14.8ms (-1.6%)
iter 840: loss 3.9697, time 941.08ms, mfu 3.79%
  data: 220.8ms, grad_accum: 915.7ms (fw: 12.6ms, bw: 167.5ms)
  grad_proc: 17.9ms, optimizer: 1.7ms, param_check: 20.7ms
  loss_proc: 0.0ms, instability: 69.2ms
  cleanup: 0.1ms, gpu_sync: 0.6ms
  measured: 956.7ms, unaccounted: -15.6ms (-1.7%)
iter 860: loss 4.0691, time 1045.50ms, mfu 3.78%
  data: 235.0ms, grad_accum: 991.9ms (fw: 12.9ms, bw: 180.8ms)
  grad_proc: 18.9ms, optimizer: 1.7ms, param_check: 21.6ms
  loss_proc: 0.0ms, instability: 80.8ms
  cleanup: 0.3ms, gpu_sync: 0.6ms
  measured: 1034.9ms, unaccounted: 10.6ms (1.0%)
iter 880: loss 4.1217, time 1004.94ms, mfu 3.79%
  data: 235.2ms, grad_accum: 1002.1ms (fw: 13.2ms, bw: 180.7ms)
  grad_proc: 19.6ms, optimizer: 1.4ms, param_check: 20.3ms
  loss_proc: 0.0ms, instability: 80.9ms
  cleanup: 0.5ms, gpu_sync: 0.5ms
  measured: 1044.3ms, unaccounted: -39.3ms (-3.9%)
iter 900: loss 4.0214, time 1051.76ms, mfu 3.78%
  data: 232.5ms, grad_accum: 978.3ms (fw: 12.6ms, bw: 176.2ms)
  grad_proc: 19.1ms, optimizer: 1.8ms, param_check: 20.2ms
  loss_proc: 0.0ms, instability: 82.3ms
  cleanup: 0.2ms, gpu_sync: 0.4ms
  measured: 1020.1ms, unaccounted: 31.7ms (3.0%)
iter 920: loss 4.0150, time 1015.76ms, mfu 3.78%
  data: 232.6ms, grad_accum: 978.1ms (fw: 14.3ms, bw: 177.3ms)
  grad_proc: 18.6ms, optimizer: 1.7ms, param_check: 19.4ms
  loss_proc: 0.0ms, instability: 74.0ms
  cleanup: 0.5ms, gpu_sync: 0.4ms
  measured: 1018.7ms, unaccounted: -3.0ms (-0.3%)
iter 940: loss 4.0585, time 960.55ms, mfu 3.80%
  data: 235.9ms, grad_accum: 972.1ms (fw: 16.7ms, bw: 180.0ms)
  grad_proc: 18.9ms, optimizer: 1.5ms, param_check: 22.0ms
  loss_proc: 0.0ms, instability: 67.6ms
  cleanup: 0.4ms, gpu_sync: 0.3ms
  measured: 1015.2ms, unaccounted: -54.6ms (-5.7%)
iter 960: loss 4.0278, time 1204.83ms, mfu 3.75%
  data: 249.2ms, grad_accum: 1043.0ms (fw: 15.1ms, bw: 190.8ms)
  grad_proc: 18.5ms, optimizer: 1.7ms, param_check: 21.1ms
  loss_proc: 0.0ms, instability: 93.7ms
  cleanup: 0.1ms, gpu_sync: 0.3ms
  measured: 1084.7ms, unaccounted: 120.1ms (10.0%)
iter 980: loss 3.9846, time 934.65ms, mfu 3.79%
  data: 239.7ms, grad_accum: 999.9ms (fw: 12.9ms, bw: 185.5ms)
  grad_proc: 17.5ms, optimizer: 1.5ms, param_check: 20.0ms
  loss_proc: 0.0ms, instability: 68.6ms
  cleanup: 0.0ms, gpu_sync: 0.3ms
  measured: 1039.2ms, unaccounted: -104.6ms (-11.2%)

--- Starting validation at iteration 1000 ---
Using validation set with samples from all 14 stages
  Validation complete: 20 batches processed (320 samples), avg loss = 4.2483
  Per-stage validation losses:
    Stage 0 (sticky): 3.8516 (23 samples) - ratio=0.4
    Stage 1 (sticky): 3.5917 (16 samples) - ratio=0.6
    Stage 2 (random): 4.2675 (23 samples) - max_ratio=0.5
    Stage 3 (sticky): 4.2278 (7 samples) - ratio=0.6
    Stage 4 (sticky): 3.3321 (16 samples) - ratio=0.6
    Stage 5 (random): 3.4284 (23 samples) - max_ratio=0.2
    Stage 6 (sticky): 3.5103 (7 samples) - ratio=0.2
    Stage 7 (sticky): 7.4435 (16 samples) - ratio=0.4
    Stage 8 (sticky): 6.1788 (23 samples) - ratio=0.4
    Stage 9 (sticky): 5.0172 (7 samples) - ratio=0.6
    Stage 10 (sticky): 3.6336 (23 samples) - ratio=0.7
    Stage 11 (sticky): 3.8671 (16 samples) - ratio=0.8
    Stage 12 (sticky): 3.9271 (7 samples) - ratio=0.8
    Stage 13 (sticky): 3.6644 (23 samples) - ratio=0.9
--- Validation complete ---
step 1000: train loss 3.8838, val loss 4.2483, lr 0.000100
Stage 0 (sticky): target_ratio=0.4, p1=0.1, p2=0.3, stale_count=0
  val model vs random: 20.62x better
  val avg correct prob: 0.2577 (random: 0.0125)
  val signal to noise: 8.43 (median: 0.62)
  Most likely guess correct P %: 40.7%
  Stage 0: New best val loss 4.2483, reset stale count to 0

saving checkpoint to out/ckpt_unmasking_1000.pt
iter 1000: loss 3.8656, time 5191.07ms, mfu 3.48%
  data: 237.5ms, grad_accum: 988.4ms (fw: 13.8ms, bw: 184.5ms)
  grad_proc: 19.3ms, optimizer: 1.5ms, param_check: 19.8ms
  loss_proc: 0.0ms, instability: 80.9ms
  cleanup: 0.1ms, gpu_sync: 0.3ms
  measured: 1029.3ms, unaccounted: 4161.7ms (80.2%)
  validation: 11014.13ms (data: 21.63ms, forward: 0.00ms, loss: 58.00ms)
Masking: stage=0 (sticky), actual_ratio=0.500, target=0.4, p1=0.1, p2=0.3
iter 1020: loss 3.8274, time 1028.58ms, mfu 3.51%
  data: 232.9ms, grad_accum: 987.8ms (fw: 14.1ms, bw: 180.3ms)
  grad_proc: 17.7ms, optimizer: 1.6ms, param_check: 19.8ms
  loss_proc: 0.0ms, instability: 80.0ms
  cleanup: 0.5ms, gpu_sync: 0.3ms
  measured: 1027.6ms, unaccounted: 1.0ms (0.1%)
iter 1040: loss 4.0286, time 1020.42ms, mfu 3.54%
  data: 238.6ms, grad_accum: 990.8ms (fw: 14.2ms, bw: 184.8ms)
  grad_proc: 17.4ms, optimizer: 1.3ms, param_check: 19.2ms
  loss_proc: 0.0ms, instability: 79.6ms
  cleanup: 0.4ms, gpu_sync: 0.2ms
  measured: 1029.4ms, unaccounted: -9.0ms (-0.9%)
iter 1060: loss 3.9533, time 930.84ms, mfu 3.60%
  data: 218.6ms, grad_accum: 924.6ms (fw: 12.9ms, bw: 170.9ms)
  grad_proc: 15.5ms, optimizer: 1.5ms, param_check: 18.5ms
  loss_proc: 0.0ms, instability: 69.7ms
  cleanup: 0.2ms, gpu_sync: 0.3ms
  measured: 960.6ms, unaccounted: -29.8ms (-3.2%)
iter 1080: loss 3.7843, time 1002.11ms, mfu 3.63%
  data: 231.7ms, grad_accum: 959.9ms (fw: 12.4ms, bw: 176.9ms)
  grad_proc: 17.4ms, optimizer: 1.8ms, param_check: 19.2ms
  loss_proc: 0.0ms, instability: 77.7ms
  cleanup: 0.3ms, gpu_sync: 0.4ms
  measured: 998.9ms, unaccounted: 3.2ms (0.3%)
iter 1100: loss 3.8339, time 963.11ms, mfu 3.67%
  data: 224.0ms, grad_accum: 931.8ms (fw: 12.8ms, bw: 172.7ms)
  grad_proc: 16.8ms, optimizer: 1.5ms, param_check: 18.8ms
  loss_proc: 0.0ms, instability: 73.3ms
  cleanup: 0.3ms, gpu_sync: 0.3ms
  measured: 969.6ms, unaccounted: -6.5ms (-0.7%)
iter 1120: loss 3.9294, time 985.74ms, mfu 3.69%
  data: 220.2ms, grad_accum: 900.2ms (fw: 12.8ms, bw: 168.4ms)
  grad_proc: 16.6ms, optimizer: 1.2ms, param_check: 19.0ms
  loss_proc: 0.0ms, instability: 74.8ms
  cleanup: 0.4ms, gpu_sync: 0.3ms
  measured: 937.7ms, unaccounted: 48.1ms (4.9%)
iter 1140: loss 3.8187, time 994.00ms, mfu 3.71%
  data: 231.3ms, grad_accum: 944.3ms (fw: 13.4ms, bw: 177.5ms)
  grad_proc: 19.6ms, optimizer: 1.4ms, param_check: 20.8ms
  loss_proc: 0.0ms, instability: 74.0ms
  cleanup: 0.2ms, gpu_sync: 0.4ms
  measured: 986.7ms, unaccounted: 7.3ms (0.7%)
iter 1160: loss 3.8719, time 962.70ms, mfu 3.75%
  data: 228.8ms, grad_accum: 937.0ms (fw: 12.7ms, bw: 176.4ms)
  grad_proc: 16.1ms, optimizer: 1.8ms, param_check: 18.3ms
  loss_proc: 0.0ms, instability: 73.7ms
  cleanup: 0.3ms, gpu_sync: 0.3ms
  measured: 973.8ms, unaccounted: -11.1ms (-1.2%)
iter 1180: loss 3.7983, time 1020.09ms, mfu 3.75%
  data: 270.6ms, grad_accum: 1002.3ms (fw: 25.0ms, bw: 178.0ms)
  grad_proc: 19.6ms, optimizer: 2.2ms, param_check: 40.3ms
  loss_proc: 0.0ms, instability: 61.0ms
  cleanup: 0.5ms, gpu_sync: 0.3ms
  measured: 1065.2ms, unaccounted: -45.1ms (-4.4%)

--- Starting validation at iteration 1200 ---
Using validation set with samples from all 14 stages
  Validation complete: 20 batches processed (320 samples), avg loss = 4.1273
  Per-stage validation losses:
    Stage 0 (sticky): 3.7634 (23 samples) - ratio=0.4
    Stage 1 (sticky): 3.5614 (16 samples) - ratio=0.6
    Stage 2 (random): 4.1844 (23 samples) - max_ratio=0.5
    Stage 3 (sticky): 4.0262 (7 samples) - ratio=0.6
    Stage 4 (sticky): 3.2620 (16 samples) - ratio=0.6
    Stage 5 (random): 3.3757 (23 samples) - max_ratio=0.2
    Stage 6 (sticky): 3.4523 (7 samples) - ratio=0.2
    Stage 7 (sticky): 7.1651 (16 samples) - ratio=0.4
    Stage 8 (sticky): 5.8870 (23 samples) - ratio=0.4
    Stage 9 (sticky): 4.8171 (7 samples) - ratio=0.6
    Stage 10 (sticky): 3.4997 (23 samples) - ratio=0.7
    Stage 11 (sticky): 3.7616 (16 samples) - ratio=0.8
    Stage 12 (sticky): 3.8438 (7 samples) - ratio=0.8
    Stage 13 (sticky): 3.6182 (23 samples) - ratio=0.9
--- Validation complete ---
step 1200: train loss 3.7836, val loss 4.1273, lr 0.000120
Stage 0 (sticky): target_ratio=0.4, p1=0.1, p2=0.3, stale_count=0
  val model vs random: 22.97x better
  val avg correct prob: 0.2872 (random: 0.0125)
  val signal to noise: 10.68 (median: 0.66)
  Most likely guess correct P %: 42.4%
  Stage 0: New best val loss 4.1273, reset stale count to 0

saving checkpoint to out/ckpt_unmasking_1200.pt
iter 1200: loss 3.8436, time 4886.48ms, mfu 3.45%
  data: 228.5ms, grad_accum: 937.7ms (fw: 12.9ms, bw: 175.5ms)
  grad_proc: 17.6ms, optimizer: 1.7ms, param_check: 20.3ms
  loss_proc: 0.0ms, instability: 72.9ms
  cleanup: 0.4ms, gpu_sync: 0.3ms
  measured: 977.9ms, unaccounted: 3908.6ms (80.0%)
  validation: 9970.71ms (data: 21.13ms, forward: 0.00ms, loss: 59.81ms)
Masking: stage=0 (sticky), actual_ratio=0.497, target=0.4, p1=0.1, p2=0.3
iter 1220: loss 3.8461, time 1010.48ms, mfu 3.49%
  data: 246.5ms, grad_accum: 988.6ms (fw: 14.2ms, bw: 181.1ms)
  grad_proc: 16.9ms, optimizer: 1.6ms, param_check: 19.8ms
  loss_proc: 0.0ms, instability: 77.5ms
  cleanup: 0.4ms, gpu_sync: 0.3ms
  measured: 1027.6ms, unaccounted: -17.1ms (-1.7%)
iter 1240: loss 3.8457, time 1006.66ms, mfu 3.53%
  data: 236.4ms, grad_accum: 990.8ms (fw: 13.5ms, bw: 184.9ms)
  grad_proc: 18.7ms, optimizer: 1.7ms, param_check: 19.1ms
  loss_proc: 0.0ms, instability: 77.8ms
  cleanup: 0.3ms, gpu_sync: 0.4ms
  measured: 1030.9ms, unaccounted: -24.3ms (-2.4%)
iter 1260: loss 3.9091, time 1034.66ms, mfu 3.55%
  data: 239.9ms, grad_accum: 990.2ms (fw: 12.4ms, bw: 184.5ms)
  grad_proc: 17.6ms, optimizer: 1.3ms, param_check: 20.1ms
  loss_proc: 0.0ms, instability: 78.7ms
  cleanup: 0.4ms, gpu_sync: 0.4ms
  measured: 1030.0ms, unaccounted: 4.7ms (0.5%)
iter 1280: loss 3.7975, time 980.72ms, mfu 3.59%
  data: 229.5ms, grad_accum: 945.7ms (fw: 13.4ms, bw: 174.3ms)
  grad_proc: 19.6ms, optimizer: 1.4ms, param_check: 20.5ms
  loss_proc: 0.0ms, instability: 74.7ms
  cleanup: 0.4ms, gpu_sync: 0.4ms
  measured: 988.0ms, unaccounted: -7.3ms (-0.7%)
iter 1300: loss 3.7903, time 900.66ms, mfu 3.66%
  data: 220.0ms, grad_accum: 900.8ms (fw: 13.7ms, bw: 169.2ms)
  grad_proc: 15.7ms, optimizer: 1.3ms, param_check: 18.4ms
  loss_proc: 0.0ms, instability: 64.4ms
  cleanup: 0.4ms, gpu_sync: 0.4ms
  measured: 936.9ms, unaccounted: -36.2ms (-4.0%)
iter 1320: loss 3.8964, time 917.79ms, mfu 3.72%
  data: 218.2ms, grad_accum: 896.2ms (fw: 12.3ms, bw: 171.0ms)
  grad_proc: 16.1ms, optimizer: 1.7ms, param_check: 18.4ms
  loss_proc: 0.0ms, instability: 69.5ms
  cleanup: 0.2ms, gpu_sync: 0.4ms
  measured: 933.0ms, unaccounted: -15.2ms (-1.7%)
iter 1340: loss 3.7610, time 955.70ms, mfu 3.75%
  data: 213.7ms, grad_accum: 867.8ms (fw: 12.9ms, bw: 163.8ms)
  grad_proc: 18.7ms, optimizer: 1.7ms, param_check: 19.5ms
  loss_proc: 0.0ms, instability: 71.8ms
  cleanup: 0.3ms, gpu_sync: 0.3ms
  measured: 908.3ms, unaccounted: 47.4ms (5.0%)
iter 1360: loss 3.7778, time 945.34ms, mfu 3.79%
  data: 219.6ms, grad_accum: 892.8ms (fw: 12.5ms, bw: 169.0ms)
  grad_proc: 17.4ms, optimizer: 1.4ms, param_check: 19.3ms
  loss_proc: 0.0ms, instability: 71.2ms
  cleanup: 0.3ms, gpu_sync: 0.4ms
  measured: 931.6ms, unaccounted: 13.7ms (1.5%)
iter 1380: loss 3.7559, time 936.52ms, mfu 3.82%
  data: 216.8ms, grad_accum: 893.5ms (fw: 13.0ms, bw: 169.2ms)
  grad_proc: 17.4ms, optimizer: 1.5ms, param_check: 19.2ms
  loss_proc: 0.0ms, instability: 69.1ms
  cleanup: 0.3ms, gpu_sync: 0.4ms
  measured: 932.3ms, unaccounted: 4.3ms (0.5%)

--- Starting validation at iteration 1400 ---
Using validation set with samples from all 14 stages
  Validation complete: 20 batches processed (320 samples), avg loss = 3.9895
  Per-stage validation losses:
    Stage 0 (sticky): 3.6234 (23 samples) - ratio=0.4
    Stage 1 (sticky): 3.4900 (16 samples) - ratio=0.6
    Stage 2 (random): 4.0623 (23 samples) - max_ratio=0.5
    Stage 3 (sticky): 3.8925 (7 samples) - ratio=0.6
    Stage 4 (sticky): 3.1794 (16 samples) - ratio=0.6
    Stage 5 (random): 3.3069 (23 samples) - max_ratio=0.2
    Stage 6 (sticky): 3.3749 (7 samples) - ratio=0.2
    Stage 7 (sticky): 6.8703 (16 samples) - ratio=0.4
    Stage 8 (sticky): 5.6266 (23 samples) - ratio=0.4
    Stage 9 (sticky): 4.6000 (7 samples) - ratio=0.6
    Stage 10 (sticky): 3.3754 (23 samples) - ratio=0.7
    Stage 11 (sticky): 3.6203 (16 samples) - ratio=0.8
    Stage 12 (sticky): 3.7248 (7 samples) - ratio=0.8
    Stage 13 (sticky): 3.5244 (23 samples) - ratio=0.9
--- Validation complete ---
step 1400: train loss 3.7185, val loss 3.9895, lr 0.000140
Stage 0 (sticky): target_ratio=0.4, p1=0.1, p2=0.3, stale_count=0
  val model vs random: 24.19x better
  val avg correct prob: 0.3024 (random: 0.0125)
  val signal to noise: 10.89 (median: 0.73)
  Most likely guess correct P %: 44.4%
  Stage 0: New best val loss 3.9895, reset stale count to 0

saving checkpoint to out/ckpt_unmasking_1400.pt
iter 1400: loss 3.8347, time 4513.63ms, mfu 3.52%
  data: 219.9ms, grad_accum: 901.4ms (fw: 12.4ms, bw: 169.9ms)
  grad_proc: 18.6ms, optimizer: 1.4ms, param_check: 19.6ms
  loss_proc: 0.0ms, instability: 70.2ms
  cleanup: 0.4ms, gpu_sync: 0.4ms
  measured: 941.8ms, unaccounted: 3571.8ms (79.1%)
  validation: 9146.66ms (data: 19.99ms, forward: 0.00ms, loss: 58.67ms)
Masking: stage=0 (sticky), actual_ratio=0.487, target=0.4, p1=0.1, p2=0.3
iter 1420: loss 3.7850, time 938.05ms, mfu 3.59%
  data: 218.1ms, grad_accum: 891.5ms (fw: 13.1ms, bw: 165.2ms)
  grad_proc: 17.2ms, optimizer: 1.6ms, param_check: 19.5ms
  loss_proc: 0.0ms, instability: 71.5ms
  cleanup: 0.4ms, gpu_sync: 0.4ms
  measured: 930.5ms, unaccounted: 7.5ms (0.8%)
iter 1440: loss 3.7711, time 1098.56ms, mfu 3.58%
  data: 221.9ms, grad_accum: 897.8ms (fw: 14.9ms, bw: 168.9ms)
  grad_proc: 17.1ms, optimizer: 1.5ms, param_check: 19.5ms
  loss_proc: 0.0ms, instability: 78.6ms
  cleanup: 0.3ms, gpu_sync: 0.4ms
  measured: 936.5ms, unaccounted: 162.0ms (14.7%)
iter 1460: loss 3.7831, time 960.59ms, mfu 3.62%
  data: 217.6ms, grad_accum: 910.4ms (fw: 12.4ms, bw: 169.2ms)
  grad_proc: 15.9ms, optimizer: 1.9ms, param_check: 18.1ms
  loss_proc: 0.0ms, instability: 72.8ms
  cleanup: 0.1ms, gpu_sync: 0.4ms
  measured: 946.8ms, unaccounted: 13.8ms (1.4%)
iter 1480: loss 3.7468, time 907.04ms, mfu 3.69%
  data: 217.8ms, grad_accum: 900.5ms (fw: 12.3ms, bw: 169.0ms)
  grad_proc: 16.8ms, optimizer: 1.4ms, param_check: 19.0ms
  loss_proc: 0.0ms, instability: 68.0ms
  cleanup: 0.4ms, gpu_sync: 0.3ms
  measured: 938.3ms, unaccounted: -31.3ms (-3.5%)
iter 1500: loss 3.8162, time 947.87ms, mfu 3.73%
  data: 215.8ms, grad_accum: 891.6ms (fw: 12.7ms, bw: 167.8ms)
  grad_proc: 17.0ms, optimizer: 1.3ms, param_check: 18.8ms
  loss_proc: 0.0ms, instability: 71.5ms
  cleanup: 0.2ms, gpu_sync: 0.3ms
  measured: 929.2ms, unaccounted: 18.7ms (2.0%)
iter 1520: loss 3.7736, time 936.79ms, mfu 3.77%
  data: 219.7ms, grad_accum: 903.3ms (fw: 12.6ms, bw: 170.4ms)
  grad_proc: 17.8ms, optimizer: 1.4ms, param_check: 18.6ms
  loss_proc: 0.0ms, instability: 69.9ms
  cleanup: 0.4ms, gpu_sync: 0.3ms
  measured: 941.9ms, unaccounted: -5.1ms (-0.5%)
iter 1540: loss 3.8066, time 930.72ms, mfu 3.81%
  data: 217.7ms, grad_accum: 888.8ms (fw: 13.8ms, bw: 168.2ms)
  grad_proc: 15.3ms, optimizer: 1.6ms, param_check: 18.5ms
  loss_proc: 0.0ms, instability: 68.9ms
  cleanup: 0.6ms, gpu_sync: 0.3ms
  measured: 925.1ms, unaccounted: 5.6ms (0.6%)
iter 1560: loss 3.7614, time 923.39ms, mfu 3.85%
  data: 214.0ms, grad_accum: 880.5ms (fw: 12.4ms, bw: 165.6ms)
  grad_proc: 18.1ms, optimizer: 1.4ms, param_check: 19.3ms
  loss_proc: 0.0ms, instability: 69.7ms
  cleanup: 0.3ms, gpu_sync: 0.3ms
  measured: 919.9ms, unaccounted: 3.5ms (0.4%)
iter 1580: loss 3.7638, time 938.44ms, mfu 3.88%
  data: 219.2ms, grad_accum: 899.7ms (fw: 13.5ms, bw: 168.2ms)
  grad_proc: 15.9ms, optimizer: 1.5ms, param_check: 18.7ms
  loss_proc: 0.0ms, instability: 69.9ms
  cleanup: 0.3ms, gpu_sync: 0.3ms
  measured: 936.3ms, unaccounted: 2.1ms (0.2%)

--- Starting validation at iteration 1600 ---
Using validation set with samples from all 14 stages
  Validation complete: 20 batches processed (320 samples), avg loss = 3.9229
  Per-stage validation losses:
    Stage 0 (sticky): 3.5683 (23 samples) - ratio=0.4
    Stage 1 (sticky): 3.4795 (16 samples) - ratio=0.6
    Stage 2 (random): 4.0164 (23 samples) - max_ratio=0.5
    Stage 3 (sticky): 3.7744 (7 samples) - ratio=0.6
    Stage 4 (sticky): 3.1338 (16 samples) - ratio=0.6
    Stage 5 (random): 3.2768 (23 samples) - max_ratio=0.2
    Stage 6 (sticky): 3.3550 (7 samples) - ratio=0.2
    Stage 7 (sticky): 6.6708 (16 samples) - ratio=0.4
    Stage 8 (sticky): 5.4985 (23 samples) - ratio=0.4
    Stage 9 (sticky): 4.4972 (7 samples) - ratio=0.6
    Stage 10 (sticky): 3.2893 (23 samples) - ratio=0.7
    Stage 11 (sticky): 3.5628 (16 samples) - ratio=0.8
    Stage 12 (sticky): 3.6746 (7 samples) - ratio=0.8
    Stage 13 (sticky): 3.5054 (23 samples) - ratio=0.9
--- Validation complete ---
step 1600: train loss 3.6619, val loss 3.9229, lr 0.000160
Stage 0 (sticky): target_ratio=0.4, p1=0.1, p2=0.3, stale_count=0
  val model vs random: 25.81x better
  val avg correct prob: 0.3227 (random: 0.0125)
  val signal to noise: 12.38 (median: 0.74)
  Most likely guess correct P %: 45.4%
  Stage 0: New best val loss 3.9229, reset stale count to 0

saving checkpoint to out/ckpt_unmasking_1600.pt
iter 1600: loss 3.7856, time 4543.63ms, mfu 3.57%
  data: 216.8ms, grad_accum: 887.8ms (fw: 13.1ms, bw: 168.0ms)
  grad_proc: 18.1ms, optimizer: 1.6ms, param_check: 19.7ms
  loss_proc: 0.0ms, instability: 70.1ms
  cleanup: 0.5ms, gpu_sync: 0.3ms
  measured: 927.9ms, unaccounted: 3615.7ms (79.6%)
  validation: 8507.73ms (data: 19.70ms, forward: 0.00ms, loss: 55.08ms)
Masking: stage=0 (sticky), actual_ratio=0.501, target=0.4, p1=0.1, p2=0.3
iter 1620: loss 3.7518, time 966.99ms, mfu 3.62%
  data: 219.5ms, grad_accum: 909.5ms (fw: 16.4ms, bw: 168.5ms)
  grad_proc: 18.0ms, optimizer: 1.6ms, param_check: 19.8ms
  loss_proc: 0.0ms, instability: 70.5ms
  cleanup: 0.2ms, gpu_sync: 0.3ms
  measured: 949.3ms, unaccounted: 17.7ms (1.8%)
iter 1640: loss 3.6201, time 991.80ms, mfu 3.65%
  data: 226.1ms, grad_accum: 914.5ms (fw: 12.1ms, bw: 173.6ms)
  grad_proc: 17.9ms, optimizer: 1.5ms, param_check: 19.5ms
  loss_proc: 0.0ms, instability: 70.2ms
  cleanup: 0.2ms, gpu_sync: 0.3ms
  measured: 953.8ms, unaccounted: 38.0ms (3.8%)
iter 1660: loss 3.7685, time 1001.53ms, mfu 3.67%
  data: 240.1ms, grad_accum: 953.2ms (fw: 13.6ms, bw: 183.0ms)
  grad_proc: 18.8ms, optimizer: 1.8ms, param_check: 19.6ms
  loss_proc: 0.0ms, instability: 72.8ms
  cleanup: 0.4ms, gpu_sync: 0.3ms
  measured: 993.9ms, unaccounted: 7.6ms (0.8%)
iter 1680: loss 3.6469, time 1003.50ms, mfu 3.69%
  data: 229.1ms, grad_accum: 951.2ms (fw: 13.0ms, bw: 177.3ms)
  grad_proc: 16.8ms, optimizer: 1.5ms, param_check: 18.8ms
  loss_proc: 0.0ms, instability: 76.9ms
  cleanup: 0.5ms, gpu_sync: 0.3ms
  measured: 989.0ms, unaccounted: 14.5ms (1.4%)
iter 1700: loss 3.7662, time 1028.09ms, mfu 3.70%
  data: 225.3ms, grad_accum: 923.7ms (fw: 12.8ms, bw: 174.7ms)
  grad_proc: 18.9ms, optimizer: 1.1ms, param_check: 18.7ms
  loss_proc: 0.0ms, instability: 78.7ms
  cleanup: 0.2ms, gpu_sync: 0.4ms
  measured: 963.0ms, unaccounted: 65.1ms (6.3%)
iter 1720: loss 3.6329, time 933.31ms, mfu 3.74%
  data: 225.5ms, grad_accum: 916.6ms (fw: 13.2ms, bw: 172.1ms)
  grad_proc: 16.6ms, optimizer: 1.5ms, param_check: 18.7ms
  loss_proc: 0.0ms, instability: 71.5ms
  cleanup: 0.3ms, gpu_sync: 0.4ms
  measured: 954.0ms, unaccounted: -20.7ms (-2.2%)
iter 1740: loss 3.5562, time 995.51ms, mfu 3.76%
  data: 230.1ms, grad_accum: 949.2ms (fw: 15.0ms, bw: 178.0ms)
  grad_proc: 18.0ms, optimizer: 1.7ms, param_check: 19.8ms
  loss_proc: 0.0ms, instability: 72.4ms
  cleanup: 0.2ms, gpu_sync: 0.5ms
  measured: 989.3ms, unaccounted: 6.2ms (0.6%)
iter 1760: loss 3.6787, time 932.17ms, mfu 3.80%
  data: 223.6ms, grad_accum: 919.1ms (fw: 12.6ms, bw: 173.3ms)
  grad_proc: 16.6ms, optimizer: 1.5ms, param_check: 19.5ms
  loss_proc: 0.0ms, instability: 69.4ms
  cleanup: 0.2ms, gpu_sync: 0.4ms
  measured: 957.3ms, unaccounted: -25.1ms (-2.7%)
iter 1780: loss 3.6457, time 1069.78ms, mfu 3.78%
  data: 249.7ms, grad_accum: 1038.8ms (fw: 13.7ms, bw: 195.4ms)
  grad_proc: 17.9ms, optimizer: 1.7ms, param_check: 19.9ms
  loss_proc: 0.0ms, instability: 82.4ms
  cleanup: 0.3ms, gpu_sync: 0.4ms
  measured: 1079.0ms, unaccounted: -9.2ms (-0.9%)

--- Starting validation at iteration 1800 ---
Using validation set with samples from all 14 stages
  Validation complete: 20 batches processed (320 samples), avg loss = 3.8542
  Per-stage validation losses:
    Stage 0 (sticky): 3.5046 (23 samples) - ratio=0.4
    Stage 1 (sticky): 3.4388 (16 samples) - ratio=0.6
    Stage 2 (random): 3.9659 (23 samples) - max_ratio=0.5
    Stage 3 (sticky): 3.6766 (7 samples) - ratio=0.6
    Stage 4 (sticky): 3.0938 (16 samples) - ratio=0.6
    Stage 5 (random): 3.2510 (23 samples) - max_ratio=0.2
    Stage 6 (sticky): 3.3340 (7 samples) - ratio=0.2
    Stage 7 (sticky): 6.4785 (16 samples) - ratio=0.4
    Stage 8 (sticky): 5.3640 (23 samples) - ratio=0.4
    Stage 9 (sticky): 4.3832 (7 samples) - ratio=0.6
    Stage 10 (sticky): 3.2335 (23 samples) - ratio=0.7
    Stage 11 (sticky): 3.4919 (16 samples) - ratio=0.8
    Stage 12 (sticky): 3.6180 (7 samples) - ratio=0.8
    Stage 13 (sticky): 3.4654 (23 samples) - ratio=0.9
--- Validation complete ---
step 1800: train loss 3.6350, val loss 3.8542, lr 0.000180
Stage 0 (sticky): target_ratio=0.4, p1=0.1, p2=0.3, stale_count=0
  val model vs random: 26.34x better
  val avg correct prob: 0.3293 (random: 0.0125)
  val signal to noise: 13.13 (median: 0.79)
  Most likely guess correct P %: 46.4%
  Stage 0: New best val loss 3.8542, reset stale count to 0

saving checkpoint to out/ckpt_unmasking_1800.pt
iter 1800: loss 3.6270, time 5302.09ms, mfu 3.47%
  data: 245.1ms, grad_accum: 1028.8ms (fw: 12.9ms, bw: 189.7ms)
  grad_proc: 17.5ms, optimizer: 1.6ms, param_check: 20.5ms
  loss_proc: 0.0ms, instability: 83.0ms
  cleanup: 0.2ms, gpu_sync: 0.4ms
  measured: 1069.0ms, unaccounted: 4233.1ms (79.8%)
  validation: 8058.20ms (data: 21.10ms, forward: 0.00ms, loss: 59.08ms)
Masking: stage=0 (sticky), actual_ratio=0.493, target=0.4, p1=0.1, p2=0.3
iter 1820: loss 3.6223, time 1076.17ms, mfu 3.49%
  data: 241.4ms, grad_accum: 1020.2ms (fw: 14.3ms, bw: 184.6ms)
  grad_proc: 17.6ms, optimizer: 1.5ms, param_check: 21.3ms
  loss_proc: 0.0ms, instability: 82.8ms
  cleanup: 0.7ms, gpu_sync: 0.4ms
  measured: 1061.5ms, unaccounted: 14.7ms (1.4%)
iter 1840: loss 3.7865, time 1015.36ms, mfu 3.52%
  data: 228.3ms, grad_accum: 944.5ms (fw: 13.9ms, bw: 176.8ms)
  grad_proc: 17.2ms, optimizer: 1.7ms, param_check: 18.7ms
  loss_proc: 0.0ms, instability: 73.9ms
  cleanup: 0.2ms, gpu_sync: 0.3ms
  measured: 982.5ms, unaccounted: 32.8ms (3.2%)
iter 1860: loss 3.5756, time 1009.11ms, mfu 3.55%
  data: 221.8ms, grad_accum: 912.7ms (fw: 12.2ms, bw: 170.7ms)
  grad_proc: 16.3ms, optimizer: 1.5ms, param_check: 18.2ms
  loss_proc: 0.0ms, instability: 74.4ms
  cleanup: 0.2ms, gpu_sync: 0.3ms
  measured: 949.2ms, unaccounted: 59.9ms (5.9%)
iter 1880: loss 3.8087, time 930.60ms, mfu 3.61%
  data: 223.5ms, grad_accum: 926.9ms (fw: 12.4ms, bw: 174.8ms)
  grad_proc: 15.5ms, optimizer: 1.5ms, param_check: 18.5ms
  loss_proc: 0.0ms, instability: 70.5ms
  cleanup: 0.1ms, gpu_sync: 0.4ms
  measured: 962.8ms, unaccounted: -32.2ms (-3.5%)
iter 1900: loss 3.6374, time 926.35ms, mfu 3.67%
  data: 218.3ms, grad_accum: 894.1ms (fw: 12.7ms, bw: 169.3ms)
  grad_proc: 16.3ms, optimizer: 1.6ms, param_check: 18.4ms
  loss_proc: 0.0ms, instability: 69.7ms
  cleanup: 0.4ms, gpu_sync: 0.3ms
  measured: 931.0ms, unaccounted: -4.6ms (-0.5%)
iter 1920: loss 3.6377, time 1005.71ms, mfu 3.69%
  data: 222.1ms, grad_accum: 907.7ms (fw: 12.8ms, bw: 172.1ms)
  grad_proc: 16.9ms, optimizer: 1.5ms, param_check: 19.7ms
  loss_proc: 0.1ms, instability: 74.9ms
  cleanup: 0.4ms, gpu_sync: 0.4ms
  measured: 946.5ms, unaccounted: 59.2ms (5.9%)
iter 1940: loss 3.5911, time 946.58ms, mfu 3.73%
  data: 226.6ms, grad_accum: 931.6ms (fw: 12.3ms, bw: 175.0ms)
  grad_proc: 15.2ms, optimizer: 1.6ms, param_check: 18.0ms
  loss_proc: 0.0ms, instability: 68.9ms
  cleanup: 0.4ms, gpu_sync: 0.3ms
  measured: 967.2ms, unaccounted: -20.6ms (-2.2%)
iter 1960: loss 3.6195, time 931.78ms, mfu 3.77%
  data: 217.7ms, grad_accum: 896.4ms (fw: 12.3ms, bw: 168.2ms)
  grad_proc: 16.6ms, optimizer: 1.5ms, param_check: 18.5ms
  loss_proc: 0.0ms, instability: 70.9ms
  cleanup: 0.2ms, gpu_sync: 0.4ms
  measured: 933.5ms, unaccounted: -1.7ms (-0.2%)
iter 1980: loss 3.5522, time 933.91ms, mfu 3.81%
  data: 225.1ms, grad_accum: 936.9ms (fw: 12.4ms, bw: 172.6ms)
  grad_proc: 18.9ms, optimizer: 1.8ms, param_check: 20.7ms
  loss_proc: 0.0ms, instability: 72.2ms
  cleanup: 0.2ms, gpu_sync: 0.4ms
  measured: 978.8ms, unaccounted: -44.9ms (-4.8%)

--- Starting validation at iteration 2000 ---
Using validation set with samples from all 14 stages
  Validation complete: 20 batches processed (320 samples), avg loss = 3.7919
  Per-stage validation losses:
    Stage 0 (sticky): 3.4437 (23 samples) - ratio=0.4
    Stage 1 (sticky): 3.4263 (16 samples) - ratio=0.6
    Stage 2 (random): 3.9188 (23 samples) - max_ratio=0.5
    Stage 3 (sticky): 3.6042 (7 samples) - ratio=0.6
    Stage 4 (sticky): 3.0617 (16 samples) - ratio=0.6
    Stage 5 (random): 3.2138 (23 samples) - max_ratio=0.2
    Stage 6 (sticky): 3.3104 (7 samples) - ratio=0.2
    Stage 7 (sticky): 6.3902 (16 samples) - ratio=0.4
    Stage 8 (sticky): 5.1987 (23 samples) - ratio=0.4
    Stage 9 (sticky): 4.2756 (7 samples) - ratio=0.6
    Stage 10 (sticky): 3.1769 (23 samples) - ratio=0.7
    Stage 11 (sticky): 3.4414 (16 samples) - ratio=0.8
    Stage 12 (sticky): 3.5583 (7 samples) - ratio=0.8
    Stage 13 (sticky): 3.4332 (23 samples) - ratio=0.9
--- Validation complete ---
step 2000: train loss 3.5707, val loss 3.7919, lr 0.000200
Stage 0 (sticky): target_ratio=0.4, p1=0.1, p2=0.3, stale_count=0
  val model vs random: 27.09x better
  val avg correct prob: 0.3386 (random: 0.0125)
  val signal to noise: 13.40 (median: 0.83)
  Most likely guess correct P %: 47.2%
  Stage 0: New best val loss 3.7919, reset stale count to 0

saving checkpoint to out/ckpt_unmasking_2000.pt
iter 2000: loss 3.5028, time 6466.39ms, mfu 3.49%
  data: 255.8ms, grad_accum: 986.0ms (fw: 32.7ms, bw: 175.8ms)
  grad_proc: 18.1ms, optimizer: 2.1ms, param_check: 24.0ms
  loss_proc: 0.1ms, instability: 65.9ms
  cleanup: 0.2ms, gpu_sync: 0.8ms
  measured: 1031.3ms, unaccounted: 5435.1ms (84.1%)
  validation: 7765.43ms (data: 28.14ms, forward: 0.00ms, loss: 63.91ms)
Masking: stage=0 (sticky), actual_ratio=0.522, target=0.4, p1=0.1, p2=0.3
iter 2020: loss 3.6262, time 1061.67ms, mfu 3.50%
  data: 225.6ms, grad_accum: 932.6ms (fw: 18.3ms, bw: 175.7ms)
  grad_proc: 17.1ms, optimizer: 1.3ms, param_check: 19.7ms
  loss_proc: 0.0ms, instability: 73.8ms
  cleanup: 0.2ms, gpu_sync: 0.9ms
  measured: 971.7ms, unaccounted: 89.9ms (8.5%)
iter 2040: loss 3.6546, time 933.10ms, mfu 3.57%
  data: 218.0ms, grad_accum: 897.7ms (fw: 12.2ms, bw: 169.0ms)
  grad_proc: 17.2ms, optimizer: 1.7ms, param_check: 19.1ms
  loss_proc: 0.0ms, instability: 70.6ms
  cleanup: 0.4ms, gpu_sync: 0.9ms
  measured: 937.0ms, unaccounted: -3.9ms (-0.4%)
iter 2060: loss 3.6241, time 940.25ms, mfu 3.62%
  data: 234.0ms, grad_accum: 946.5ms (fw: 12.2ms, bw: 182.4ms)
  grad_proc: 18.3ms, optimizer: 1.5ms, param_check: 19.4ms
  loss_proc: 0.0ms, instability: 70.4ms
  cleanup: 0.3ms, gpu_sync: 0.9ms
  measured: 986.9ms, unaccounted: -46.6ms (-5.0%)
iter 2080: loss 3.6144, time 1083.14ms, mfu 3.62%
  data: 233.6ms, grad_accum: 977.7ms (fw: 12.8ms, bw: 177.6ms)
  grad_proc: 17.5ms, optimizer: 1.5ms, param_check: 19.5ms
  loss_proc: 0.0ms, instability: 77.4ms
  cleanup: 0.2ms, gpu_sync: 0.9ms
  measured: 1017.3ms, unaccounted: 65.9ms (6.1%)
iter 2100: loss 3.5755, time 933.04ms, mfu 3.67%
  data: 223.1ms, grad_accum: 912.5ms (fw: 12.1ms, bw: 170.4ms)
  grad_proc: 17.8ms, optimizer: 1.9ms, param_check: 19.0ms
  loss_proc: 0.0ms, instability: 70.7ms
  cleanup: 0.2ms, gpu_sync: 0.9ms
  measured: 952.3ms, unaccounted: -19.3ms (-2.1%)
iter 2120: loss 3.5986, time 943.34ms, mfu 3.72%
  data: 223.8ms, grad_accum: 913.0ms (fw: 12.2ms, bw: 169.9ms)
  grad_proc: 16.9ms, optimizer: 1.6ms, param_check: 18.8ms
  loss_proc: 0.0ms, instability: 71.5ms
  cleanup: 0.3ms, gpu_sync: 0.8ms
  measured: 951.3ms, unaccounted: -7.9ms (-0.8%)
iter 2140: loss 3.5684, time 934.01ms, mfu 3.76%
  data: 217.1ms, grad_accum: 900.5ms (fw: 12.1ms, bw: 168.4ms)
  grad_proc: 16.2ms, optimizer: 1.6ms, param_check: 18.5ms
  loss_proc: 0.0ms, instability: 70.8ms
  cleanup: 0.1ms, gpu_sync: 0.8ms
  measured: 937.7ms, unaccounted: -3.7ms (-0.4%)
iter 2160: loss 3.5610, time 934.44ms, mfu 3.80%
  data: 218.2ms, grad_accum: 902.8ms (fw: 12.2ms, bw: 169.2ms)
  grad_proc: 16.7ms, optimizer: 1.5ms, param_check: 19.0ms
  loss_proc: 0.0ms, instability: 71.2ms
  cleanup: 0.2ms, gpu_sync: 0.8ms
  measured: 940.9ms, unaccounted: -6.5ms (-0.7%)
iter 2180: loss 3.5777, time 947.26ms, mfu 3.83%
  data: 219.1ms, grad_accum: 903.8ms (fw: 12.9ms, bw: 169.5ms)
  grad_proc: 16.3ms, optimizer: 1.5ms, param_check: 19.0ms
  loss_proc: 0.0ms, instability: 70.7ms
  cleanup: 0.5ms, gpu_sync: 0.9ms
  measured: 941.9ms, unaccounted: 5.4ms (0.6%)

--- Starting validation at iteration 2200 ---
Using validation set with samples from all 14 stages
  Validation complete: 20 batches processed (320 samples), avg loss = 3.7308
  Per-stage validation losses:
    Stage 0 (sticky): 3.3974 (23 samples) - ratio=0.4
    Stage 1 (sticky): 3.3687 (16 samples) - ratio=0.6
    Stage 2 (random): 3.8610 (23 samples) - max_ratio=0.5
    Stage 3 (sticky): 3.5345 (7 samples) - ratio=0.6
    Stage 4 (sticky): 3.0321 (16 samples) - ratio=0.6
    Stage 5 (random): 3.1810 (23 samples) - max_ratio=0.2
    Stage 6 (sticky): 3.2908 (7 samples) - ratio=0.2
    Stage 7 (sticky): 6.2201 (16 samples) - ratio=0.4
    Stage 8 (sticky): 5.0837 (23 samples) - ratio=0.4
    Stage 9 (sticky): 4.2114 (7 samples) - ratio=0.6
    Stage 10 (sticky): 3.1180 (23 samples) - ratio=0.7
    Stage 11 (sticky): 3.3969 (16 samples) - ratio=0.8
    Stage 12 (sticky): 3.5022 (7 samples) - ratio=0.8
    Stage 13 (sticky): 3.3884 (23 samples) - ratio=0.9
--- Validation complete ---
step 2200: train loss 3.5131, val loss 3.7308, lr 0.000200
Stage 0 (sticky): target_ratio=0.4, p1=0.1, p2=0.3, stale_count=0
  val model vs random: 27.45x better
  val avg correct prob: 0.3432 (random: 0.0125)
  val signal to noise: 13.25 (median: 0.89)
  Most likely guess correct P %: 48.1%
  Stage 0: New best val loss 3.7308, reset stale count to 0

saving checkpoint to out/ckpt_unmasking_2200.pt
iter 2200: loss 3.6254, time 5623.08ms, mfu 3.51%
  data: 223.5ms, grad_accum: 910.7ms (fw: 26.8ms, bw: 169.9ms)
  grad_proc: 16.7ms, optimizer: 1.5ms, param_check: 19.6ms
  loss_proc: 0.0ms, instability: 58.4ms
  cleanup: 0.4ms, gpu_sync: 0.4ms
  measured: 949.3ms, unaccounted: 4673.7ms (83.1%)
  validation: 7467.62ms (data: 27.18ms, forward: 0.00ms, loss: 67.02ms)
Masking: stage=0 (sticky), actual_ratio=0.509, target=0.4, p1=0.1, p2=0.3
iter 2220: loss 3.6421, time 934.49ms, mfu 3.58%
  data: 217.7ms, grad_accum: 902.8ms (fw: 12.5ms, bw: 168.9ms)
  grad_proc: 16.3ms, optimizer: 1.7ms, param_check: 18.5ms
  loss_proc: 0.0ms, instability: 70.8ms
  cleanup: 0.2ms, gpu_sync: 0.4ms
  measured: 939.9ms, unaccounted: -5.4ms (-0.6%)
iter 2240: loss 3.4121, time 939.38ms, mfu 3.63%
  data: 217.1ms, grad_accum: 903.3ms (fw: 12.0ms, bw: 168.8ms)
  grad_proc: 16.3ms, optimizer: 1.3ms, param_check: 18.8ms
  loss_proc: 0.0ms, instability: 70.9ms
  cleanup: 0.2ms, gpu_sync: 0.4ms
  measured: 940.4ms, unaccounted: -1.0ms (-0.1%)
iter 2260: loss 3.6142, time 945.37ms, mfu 3.68%
  data: 217.2ms, grad_accum: 897.9ms (fw: 12.4ms, bw: 169.1ms)
  grad_proc: 17.5ms, optimizer: 1.3ms, param_check: 20.0ms
  loss_proc: 0.0ms, instability: 70.1ms
  cleanup: 0.4ms, gpu_sync: 0.4ms
  measured: 937.5ms, unaccounted: 7.9ms (0.8%)
iter 2280: loss 3.6285, time 937.34ms, mfu 3.72%
  data: 219.4ms, grad_accum: 896.4ms (fw: 12.8ms, bw: 168.7ms)
  grad_proc: 16.8ms, optimizer: 1.6ms, param_check: 18.0ms
  loss_proc: 0.0ms, instability: 70.2ms
  cleanup: 0.3ms, gpu_sync: 0.5ms
  measured: 933.5ms, unaccounted: 3.8ms (0.4%)
iter 2300: loss 3.6053, time 936.40ms, mfu 3.77%
  data: 217.6ms, grad_accum: 896.7ms (fw: 12.7ms, bw: 169.4ms)
  grad_proc: 16.5ms, optimizer: 1.4ms, param_check: 18.6ms
  loss_proc: 0.0ms, instability: 70.9ms
  cleanup: 0.3ms, gpu_sync: 0.5ms
  measured: 933.9ms, unaccounted: 2.5ms (0.3%)
iter 2320: loss 3.6275, time 938.63ms, mfu 3.80%
  data: 217.3ms, grad_accum: 891.0ms (fw: 12.7ms, bw: 166.6ms)
  grad_proc: 16.2ms, optimizer: 1.6ms, param_check: 19.0ms
  loss_proc: 0.0ms, instability: 70.4ms
  cleanup: 0.2ms, gpu_sync: 0.5ms
  measured: 928.5ms, unaccounted: 10.2ms (1.1%)
iter 2340: loss 3.5544, time 939.92ms, mfu 3.83%
  data: 216.3ms, grad_accum: 890.5ms (fw: 12.6ms, bw: 167.3ms)
  grad_proc: 15.8ms, optimizer: 1.6ms, param_check: 18.3ms
  loss_proc: 0.0ms, instability: 70.3ms
  cleanup: 0.2ms, gpu_sync: 0.6ms
  measured: 926.9ms, unaccounted: 13.0ms (1.4%)
iter 2360: loss 3.5972, time 942.57ms, mfu 3.86%
  data: 217.6ms, grad_accum: 897.1ms (fw: 12.7ms, bw: 167.7ms)
  grad_proc: 16.7ms, optimizer: 1.5ms, param_check: 19.5ms
  loss_proc: 0.0ms, instability: 70.2ms
  cleanup: 0.3ms, gpu_sync: 0.5ms
  measured: 935.6ms, unaccounted: 7.0ms (0.7%)
iter 2380: loss 3.5019, time 934.85ms, mfu 3.89%
  data: 218.9ms, grad_accum: 899.4ms (fw: 12.1ms, bw: 169.6ms)
  grad_proc: 16.0ms, optimizer: 1.6ms, param_check: 17.9ms
  loss_proc: 0.0ms, instability: 71.5ms
  cleanup: 0.3ms, gpu_sync: 0.4ms
  measured: 935.6ms, unaccounted: -0.7ms (-0.1%)

--- Starting validation at iteration 2400 ---
Using validation set with samples from all 14 stages
  Validation complete: 20 batches processed (320 samples), avg loss = 3.6909
  Per-stage validation losses:
    Stage 0 (sticky): 3.3613 (23 samples) - ratio=0.4
    Stage 1 (sticky): 3.3677 (16 samples) - ratio=0.6
    Stage 2 (random): 3.8239 (23 samples) - max_ratio=0.5
    Stage 3 (sticky): 3.4861 (7 samples) - ratio=0.6
    Stage 4 (sticky): 3.0105 (16 samples) - ratio=0.6
    Stage 5 (random): 3.1727 (23 samples) - max_ratio=0.2
    Stage 6 (sticky): 3.2711 (7 samples) - ratio=0.2
    Stage 7 (sticky): 6.0944 (16 samples) - ratio=0.4
    Stage 8 (sticky): 5.0138 (23 samples) - ratio=0.4
    Stage 9 (sticky): 4.0807 (7 samples) - ratio=0.6
    Stage 10 (sticky): 3.0886 (23 samples) - ratio=0.7
    Stage 11 (sticky): 3.3739 (16 samples) - ratio=0.8
    Stage 12 (sticky): 3.4658 (7 samples) - ratio=0.8
    Stage 13 (sticky): 3.3736 (23 samples) - ratio=0.9
--- Validation complete ---
step 2400: train loss 3.4616, val loss 3.6909, lr 0.000200
Stage 0 (sticky): target_ratio=0.4, p1=0.1, p2=0.3, stale_count=0
  val model vs random: 28.94x better
  val avg correct prob: 0.3617 (random: 0.0125)
  val signal to noise: 15.53 (median: 0.92)
  Most likely guess correct P %: 48.9%
  Stage 0: New best val loss 3.6909, reset stale count to 0

saving checkpoint to out/ckpt_unmasking_2400.pt
iter 2400: loss 3.5616, time 5612.85ms, mfu 3.57%
  data: 222.4ms, grad_accum: 895.4ms (fw: 21.5ms, bw: 167.2ms)
  grad_proc: 19.2ms, optimizer: 1.9ms, param_check: 19.4ms
  loss_proc: 0.0ms, instability: 61.5ms
  cleanup: 0.0ms, gpu_sync: 0.4ms
  measured: 936.4ms, unaccounted: 4676.5ms (83.3%)
  validation: 7223.30ms (data: 20.03ms, forward: 0.00ms, loss: 68.25ms)
Masking: stage=0 (sticky), actual_ratio=0.502, target=0.4, p1=0.1, p2=0.3
iter 2420: loss 3.7581, time 948.56ms, mfu 3.62%
  data: 220.1ms, grad_accum: 902.1ms (fw: 12.5ms, bw: 170.5ms)
  grad_proc: 18.7ms, optimizer: 1.4ms, param_check: 20.4ms
  loss_proc: 0.0ms, instability: 71.1ms
  cleanup: 0.3ms, gpu_sync: 0.3ms
  measured: 943.2ms, unaccounted: 5.4ms (0.6%)
iter 2440: loss 3.5719, time 929.34ms, mfu 3.68%
  data: 217.6ms, grad_accum: 893.9ms (fw: 12.6ms, bw: 169.2ms)
  grad_proc: 16.3ms, optimizer: 1.4ms, param_check: 18.7ms
  loss_proc: 0.0ms, instability: 70.1ms
  cleanup: 0.3ms, gpu_sync: 0.4ms
  measured: 931.1ms, unaccounted: -1.8ms (-0.2%)
iter 2460: loss 3.6435, time 927.82ms, mfu 3.73%
  data: 215.4ms, grad_accum: 894.7ms (fw: 12.7ms, bw: 165.7ms)
  grad_proc: 16.7ms, optimizer: 1.4ms, param_check: 19.1ms
  loss_proc: 0.0ms, instability: 70.0ms
  cleanup: 0.2ms, gpu_sync: 0.4ms
  measured: 932.5ms, unaccounted: -4.7ms (-0.5%)
iter 2480: loss 3.4925, time 927.47ms, mfu 3.77%
  data: 217.1ms, grad_accum: 894.2ms (fw: 12.5ms, bw: 167.5ms)
  grad_proc: 18.3ms, optimizer: 1.6ms, param_check: 19.8ms
  loss_proc: 0.0ms, instability: 69.7ms
  cleanup: 0.4ms, gpu_sync: 0.4ms
  measured: 934.5ms, unaccounted: -7.1ms (-0.8%)
iter 2500: loss 3.4867, time 931.03ms, mfu 3.81%
  data: 214.8ms, grad_accum: 891.1ms (fw: 12.6ms, bw: 165.8ms)
  grad_proc: 17.2ms, optimizer: 1.7ms, param_check: 18.5ms
  loss_proc: 0.0ms, instability: 70.4ms
  cleanup: 0.4ms, gpu_sync: 0.4ms
  measured: 929.2ms, unaccounted: 1.8ms (0.2%)
iter 2520: loss 3.5470, time 928.13ms, mfu 3.85%
  data: 215.3ms, grad_accum: 883.0ms (fw: 12.4ms, bw: 166.7ms)
  grad_proc: 17.4ms, optimizer: 1.3ms, param_check: 19.4ms
  loss_proc: 0.0ms, instability: 70.0ms
  cleanup: 0.4ms, gpu_sync: 0.4ms
  measured: 921.8ms, unaccounted: 6.3ms (0.7%)
iter 2540: loss 3.6659, time 922.71ms, mfu 3.88%
  data: 215.7ms, grad_accum: 894.9ms (fw: 12.1ms, bw: 166.6ms)
  grad_proc: 17.0ms, optimizer: 1.5ms, param_check: 18.7ms
  loss_proc: 0.0ms, instability: 69.7ms
  cleanup: 0.2ms, gpu_sync: 0.3ms
  measured: 932.5ms, unaccounted: -9.8ms (-1.1%)
iter 2560: loss 3.4615, time 931.38ms, mfu 3.91%
  data: 212.6ms, grad_accum: 887.2ms (fw: 12.3ms, bw: 164.2ms)
  grad_proc: 15.6ms, optimizer: 1.5ms, param_check: 18.2ms
  loss_proc: 0.0ms, instability: 71.0ms
  cleanup: 0.3ms, gpu_sync: 0.4ms
  measured: 923.3ms, unaccounted: 8.1ms (0.9%)
iter 2580: loss 3.4861, time 923.59ms, mfu 3.94%
  data: 214.8ms, grad_accum: 889.3ms (fw: 13.2ms, bw: 167.1ms)
  grad_proc: 17.1ms, optimizer: 1.8ms, param_check: 18.6ms
  loss_proc: 0.0ms, instability: 69.5ms
  cleanup: 0.5ms, gpu_sync: 0.4ms
  measured: 927.7ms, unaccounted: -4.1ms (-0.4%)

--- Starting validation at iteration 2600 ---
Using validation set with samples from all 14 stages
  Validation complete: 20 batches processed (320 samples), avg loss = 3.6501
  Per-stage validation losses:
    Stage 0 (sticky): 3.3334 (23 samples) - ratio=0.4
    Stage 1 (sticky): 3.3634 (16 samples) - ratio=0.6
    Stage 2 (random): 3.8091 (23 samples) - max_ratio=0.5
    Stage 3 (sticky): 3.4420 (7 samples) - ratio=0.6
    Stage 4 (sticky): 2.9940 (16 samples) - ratio=0.6
    Stage 5 (random): 3.1619 (23 samples) - max_ratio=0.2
    Stage 6 (sticky): 3.2396 (7 samples) - ratio=0.2
    Stage 7 (sticky): 5.9102 (16 samples) - ratio=0.4
    Stage 8 (sticky): 4.8993 (23 samples) - ratio=0.4
    Stage 9 (sticky): 4.0439 (7 samples) - ratio=0.6
    Stage 10 (sticky): 3.0363 (23 samples) - ratio=0.7
    Stage 11 (sticky): 3.3350 (16 samples) - ratio=0.8
    Stage 12 (sticky): 3.4642 (7 samples) - ratio=0.8
    Stage 13 (sticky): 3.3651 (23 samples) - ratio=0.9
--- Validation complete ---
step 2600: train loss 3.4303, val loss 3.6501, lr 0.000199
Stage 0 (sticky): target_ratio=0.4, p1=0.1, p2=0.3, stale_count=0
  val model vs random: 29.45x better
  val avg correct prob: 0.3681 (random: 0.0125)
  val signal to noise: 15.65 (median: 0.96)
  Most likely guess correct P %: 49.4%
  Stage 0: New best val loss 3.6501, reset stale count to 0

saving checkpoint to out/ckpt_unmasking_2600.pt
iter 2600: loss 3.6553, time 5123.60ms, mfu 3.62%
  data: 219.5ms, grad_accum: 897.9ms (fw: 27.6ms, bw: 166.9ms)
  grad_proc: 16.0ms, optimizer: 1.6ms, param_check: 18.8ms
  loss_proc: 0.0ms, instability: 57.3ms
  cleanup: 0.4ms, gpu_sync: 0.3ms
  measured: 935.0ms, unaccounted: 4188.6ms (81.8%)
  validation: 6976.40ms (data: 20.46ms, forward: 0.00ms, loss: 69.59ms)
Masking: stage=0 (sticky), actual_ratio=0.504, target=0.4, p1=0.1, p2=0.3
iter 2620: loss 3.4621, time 937.47ms, mfu 3.67%
  data: 222.2ms, grad_accum: 924.6ms (fw: 12.3ms, bw: 173.8ms)
  grad_proc: 17.0ms, optimizer: 1.6ms, param_check: 18.5ms
  loss_proc: 0.0ms, instability: 70.7ms
  cleanup: 0.3ms, gpu_sync: 0.3ms
  measured: 962.3ms, unaccounted: -24.8ms (-2.6%)
iter 2640: loss 3.5152, time 933.40ms, mfu 3.72%
  data: 219.4ms, grad_accum: 898.1ms (fw: 12.3ms, bw: 169.4ms)
  grad_proc: 18.5ms, optimizer: 1.3ms, param_check: 20.0ms
  loss_proc: 0.0ms, instability: 70.1ms
  cleanup: 0.2ms, gpu_sync: 0.3ms
  measured: 938.4ms, unaccounted: -5.0ms (-0.5%)
iter 2660: loss 3.4568, time 927.11ms, mfu 3.77%
  data: 218.3ms, grad_accum: 894.9ms (fw: 12.0ms, bw: 168.4ms)
  grad_proc: 16.8ms, optimizer: 1.6ms, param_check: 18.4ms
  loss_proc: 0.0ms, instability: 70.2ms
  cleanup: 0.4ms, gpu_sync: 0.4ms
  measured: 932.4ms, unaccounted: -5.3ms (-0.6%)
iter 2680: loss 3.4587, time 933.68ms, mfu 3.80%
  data: 216.3ms, grad_accum: 884.3ms (fw: 12.3ms, bw: 167.8ms)
  grad_proc: 16.0ms, optimizer: 1.6ms, param_check: 18.3ms
  loss_proc: 0.1ms, instability: 70.6ms
  cleanup: 0.4ms, gpu_sync: 0.4ms
  measured: 921.0ms, unaccounted: 12.6ms (1.4%)
iter 2700: loss 3.4397, time 933.10ms, mfu 3.84%
  data: 215.4ms, grad_accum: 889.2ms (fw: 12.2ms, bw: 167.0ms)
  grad_proc: 15.9ms, optimizer: 1.4ms, param_check: 18.7ms
  loss_proc: 0.0ms, instability: 71.0ms
  cleanup: 0.0ms, gpu_sync: 0.4ms
  measured: 925.6ms, unaccounted: 7.5ms (0.8%)
iter 2720: loss 3.4212, time 930.30ms, mfu 3.87%
  data: 218.0ms, grad_accum: 893.6ms (fw: 12.1ms, bw: 167.6ms)
  grad_proc: 17.2ms, optimizer: 1.5ms, param_check: 18.9ms
  loss_proc: 0.0ms, instability: 71.0ms
  cleanup: 0.2ms, gpu_sync: 0.4ms
  measured: 931.8ms, unaccounted: -1.5ms (-0.2%)
iter 2740: loss 3.3712, time 1026.12ms, mfu 3.86%
  data: 218.6ms, grad_accum: 900.2ms (fw: 14.6ms, bw: 168.9ms)
  grad_proc: 16.3ms, optimizer: 1.4ms, param_check: 19.4ms
  loss_proc: 0.0ms, instability: 76.4ms
  cleanup: 0.1ms, gpu_sync: 0.4ms
  measured: 937.6ms, unaccounted: 88.5ms (8.6%)
iter 2760: loss 3.6769, time 1050.49ms, mfu 3.84%
  data: 238.1ms, grad_accum: 1017.6ms (fw: 13.8ms, bw: 184.8ms)
  grad_proc: 17.6ms, optimizer: 1.5ms, param_check: 21.4ms
  loss_proc: 0.0ms, instability: 82.4ms
  cleanup: 0.2ms, gpu_sync: 0.4ms
  measured: 1058.7ms, unaccounted: -8.2ms (-0.8%)
iter 2780: loss 3.5554, time 1078.38ms, mfu 3.82%
  data: 243.2ms, grad_accum: 1039.5ms (fw: 13.3ms, bw: 188.9ms)
  grad_proc: 18.6ms, optimizer: 1.9ms, param_check: 20.8ms
  loss_proc: 0.0ms, instability: 83.7ms
  cleanup: 0.4ms, gpu_sync: 0.5ms
  measured: 1081.7ms, unaccounted: -3.3ms (-0.3%)

--- Starting validation at iteration 2800 ---
Using validation set with samples from all 14 stages
  Validation complete: 20 batches processed (320 samples), avg loss = 3.6176
  Per-stage validation losses:
    Stage 0 (sticky): 3.2913 (23 samples) - ratio=0.4
    Stage 1 (sticky): 3.3277 (16 samples) - ratio=0.6
    Stage 2 (random): 3.7771 (23 samples) - max_ratio=0.5
    Stage 3 (sticky): 3.3955 (7 samples) - ratio=0.6
    Stage 4 (sticky): 2.9689 (16 samples) - ratio=0.6
    Stage 5 (random): 3.1305 (23 samples) - max_ratio=0.2
    Stage 6 (sticky): 3.2515 (7 samples) - ratio=0.2
    Stage 7 (sticky): 5.8692 (16 samples) - ratio=0.4
    Stage 8 (sticky): 4.8594 (23 samples) - ratio=0.4
    Stage 9 (sticky): 4.0039 (7 samples) - ratio=0.6
    Stage 10 (sticky): 3.0083 (23 samples) - ratio=0.7
    Stage 11 (sticky): 3.3037 (16 samples) - ratio=0.8
    Stage 12 (sticky): 3.4035 (7 samples) - ratio=0.8
    Stage 13 (sticky): 3.3480 (23 samples) - ratio=0.9
--- Validation complete ---
step 2800: train loss 3.4442, val loss 3.6176, lr 0.000198
Stage 0 (sticky): target_ratio=0.4, p1=0.1, p2=0.3, stale_count=0
  val model vs random: 30.11x better
  val avg correct prob: 0.3764 (random: 0.0125)
  val signal to noise: 16.86 (median: 1.01)
  Most likely guess correct P %: 50.1%
  Stage 0: New best val loss 3.6176, reset stale count to 0

saving checkpoint to out/ckpt_unmasking_2800.pt
iter 2800: loss 3.5357, time 5089.36ms, mfu 3.51%
  data: 245.6ms, grad_accum: 1018.5ms (fw: 14.4ms, bw: 188.3ms)
  grad_proc: 18.6ms, optimizer: 1.7ms, param_check: 20.5ms
  loss_proc: 0.0ms, instability: 69.2ms
  cleanup: 0.6ms, gpu_sync: 0.5ms
  measured: 1060.4ms, unaccounted: 4029.0ms (79.2%)
  validation: 6772.77ms (data: 20.32ms, forward: 0.00ms, loss: 68.43ms)
Masking: stage=0 (sticky), actual_ratio=0.494, target=0.4, p1=0.1, p2=0.3
iter 2820: loss 3.5257, time 1031.50ms, mfu 3.54%
  data: 233.2ms, grad_accum: 950.9ms (fw: 14.6ms, bw: 178.1ms)
  grad_proc: 19.0ms, optimizer: 1.6ms, param_check: 20.4ms
  loss_proc: 0.0ms, instability: 73.0ms
  cleanup: 0.2ms, gpu_sync: 0.6ms
  measured: 992.7ms, unaccounted: 38.8ms (3.8%)
iter 2840: loss 3.4525, time 1149.14ms, mfu 3.52%
  data: 247.3ms, grad_accum: 1010.0ms (fw: 14.0ms, bw: 192.9ms)
  grad_proc: 20.3ms, optimizer: 1.6ms, param_check: 21.6ms
  loss_proc: 0.0ms, instability: 88.8ms
  cleanup: 0.3ms, gpu_sync: 0.5ms
  measured: 1054.4ms, unaccounted: 94.8ms (8.2%)
iter 2860: loss 3.4674, time 1014.67ms, mfu 3.55%
  data: 247.3ms, grad_accum: 1015.6ms (fw: 12.8ms, bw: 191.7ms)
  grad_proc: 19.5ms, optimizer: 1.7ms, param_check: 20.9ms
  loss_proc: 0.0ms, instability: 76.3ms
  cleanup: 0.5ms, gpu_sync: 0.4ms
  measured: 1058.7ms, unaccounted: -44.0ms (-4.3%)
iter 2880: loss 3.5425, time 995.56ms, mfu 3.58%
  data: 235.4ms, grad_accum: 966.8ms (fw: 13.6ms, bw: 183.3ms)
  grad_proc: 16.8ms, optimizer: 1.9ms, param_check: 18.8ms
  loss_proc: 0.0ms, instability: 73.8ms
  cleanup: 0.2ms, gpu_sync: 0.5ms
  measured: 1005.0ms, unaccounted: -9.5ms (-0.9%)
iter 2900: loss 3.3944, time 1024.98ms, mfu 3.60%
  data: 227.3ms, grad_accum: 944.8ms (fw: 12.8ms, bw: 176.9ms)
  grad_proc: 17.3ms, optimizer: 1.8ms, param_check: 19.1ms
  loss_proc: 0.0ms, instability: 77.8ms
  cleanup: 0.2ms, gpu_sync: 0.6ms
  measured: 983.8ms, unaccounted: 41.2ms (4.0%)
iter 2920: loss 3.4878, time 990.08ms, mfu 3.64%
  data: 228.7ms, grad_accum: 943.9ms (fw: 12.4ms, bw: 175.8ms)
  grad_proc: 17.0ms, optimizer: 1.5ms, param_check: 19.7ms
  loss_proc: 0.0ms, instability: 75.5ms
  cleanup: 0.5ms, gpu_sync: 0.7ms
  measured: 983.4ms, unaccounted: 6.7ms (0.7%)
iter 2940: loss 3.5345, time 1012.20ms, mfu 3.65%
  data: 234.2ms, grad_accum: 971.9ms (fw: 13.1ms, bw: 181.8ms)
  grad_proc: 19.1ms, optimizer: 1.5ms, param_check: 20.4ms
  loss_proc: 0.0ms, instability: 76.7ms
  cleanup: 0.4ms, gpu_sync: 0.6ms
  measured: 1014.0ms, unaccounted: -1.8ms (-0.2%)
iter 2960: loss 3.4871, time 1025.19ms, mfu 3.67%
  data: 233.1ms, grad_accum: 964.7ms (fw: 13.1ms, bw: 181.4ms)
  grad_proc: 16.9ms, optimizer: 1.5ms, param_check: 19.9ms
  loss_proc: 0.0ms, instability: 78.0ms
  cleanup: 0.4ms, gpu_sync: 0.6ms
  measured: 1003.9ms, unaccounted: 21.2ms (2.1%)
iter 2980: loss 3.4306, time 963.40ms, mfu 3.70%
  data: 227.7ms, grad_accum: 961.2ms (fw: 13.3ms, bw: 174.7ms)
  grad_proc: 19.7ms, optimizer: 1.4ms, param_check: 21.5ms
  loss_proc: 0.0ms, instability: 71.3ms
  cleanup: 0.3ms, gpu_sync: 0.6ms
  measured: 1004.8ms, unaccounted: -41.4ms (-4.3%)

--- Starting validation at iteration 3000 ---
Using validation set with samples from all 14 stages
  Validation complete: 20 batches processed (320 samples), avg loss = 3.5630
  Per-stage validation losses:
    Stage 0 (sticky): 3.2526 (23 samples) - ratio=0.4
    Stage 1 (sticky): 3.3018 (16 samples) - ratio=0.6
    Stage 2 (random): 3.7386 (23 samples) - max_ratio=0.5
    Stage 3 (sticky): 3.3408 (7 samples) - ratio=0.6
    Stage 4 (sticky): 2.9370 (16 samples) - ratio=0.6
    Stage 5 (random): 3.0964 (23 samples) - max_ratio=0.2
    Stage 6 (sticky): 3.2272 (7 samples) - ratio=0.2
    Stage 7 (sticky): 5.7431 (16 samples) - ratio=0.4
    Stage 8 (sticky): 4.7522 (23 samples) - ratio=0.4
    Stage 9 (sticky): 3.8847 (7 samples) - ratio=0.6
    Stage 10 (sticky): 2.9512 (23 samples) - ratio=0.7
    Stage 11 (sticky): 3.2539 (16 samples) - ratio=0.8
    Stage 12 (sticky): 3.3591 (7 samples) - ratio=0.8
    Stage 13 (sticky): 3.3154 (23 samples) - ratio=0.9
--- Validation complete ---
step 3000: train loss 3.4002, val loss 3.5630, lr 0.000198
Stage 0 (sticky): target_ratio=0.4, p1=0.1, p2=0.3, stale_count=0
  val model vs random: 30.59x better
  val avg correct prob: 0.3824 (random: 0.0125)
  val signal to noise: 17.18 (median: 1.06)
  Most likely guess correct P %: 50.8%
  Stage 0: New best val loss 3.5630, reset stale count to 0

saving checkpoint to out/ckpt_unmasking_3000.pt
iter 3000: loss 3.6104, time 6635.20ms, mfu 3.39%
  data: 243.6ms, grad_accum: 974.5ms (fw: 24.1ms, bw: 176.2ms)
  grad_proc: 17.9ms, optimizer: 1.9ms, param_check: 25.2ms
  loss_proc: 0.0ms, instability: 68.1ms
  cleanup: 0.6ms, gpu_sync: 0.6ms
  measured: 1020.6ms, unaccounted: 5614.6ms (84.6%)
  validation: 6683.98ms (data: 34.36ms, forward: 0.00ms, loss: 64.33ms)
Masking: stage=0 (sticky), actual_ratio=0.519, target=0.4, p1=0.1, p2=0.3
iter 3020: loss 3.5205, time 1026.03ms, mfu 3.43%
  data: 239.6ms, grad_accum: 974.1ms (fw: 12.7ms, bw: 185.0ms)
  grad_proc: 18.1ms, optimizer: 1.6ms, param_check: 20.8ms
  loss_proc: 0.0ms, instability: 78.8ms
  cleanup: 0.4ms, gpu_sync: 0.6ms
  measured: 1015.5ms, unaccounted: 10.6ms (1.0%)
iter 3040: loss 3.4426, time 998.17ms, mfu 3.47%
  data: 238.0ms, grad_accum: 981.3ms (fw: 12.6ms, bw: 184.7ms)
  grad_proc: 17.9ms, optimizer: 1.6ms, param_check: 20.2ms
  loss_proc: 0.0ms, instability: 76.2ms
  cleanup: 0.4ms, gpu_sync: 0.6ms
  measured: 1022.0ms, unaccounted: -23.9ms (-2.4%)
iter 3060: loss 3.4718, time 1008.24ms, mfu 3.51%
  data: 235.9ms, grad_accum: 973.7ms (fw: 12.5ms, bw: 184.6ms)
  grad_proc: 18.9ms, optimizer: 1.5ms, param_check: 20.3ms
  loss_proc: 0.0ms, instability: 77.1ms
  cleanup: 0.3ms, gpu_sync: 0.6ms
  measured: 1015.3ms, unaccounted: -7.0ms (-0.7%)
iter 3080: loss 3.3570, time 894.00ms, mfu 3.59%
  data: 237.3ms, grad_accum: 967.5ms (fw: 14.3ms, bw: 182.4ms)
  grad_proc: 17.2ms, optimizer: 1.5ms, param_check: 19.8ms
  loss_proc: 0.0ms, instability: 63.9ms
  cleanup: 0.2ms, gpu_sync: 0.6ms
  measured: 1006.7ms, unaccounted: -112.7ms (-12.6%)
iter 3100: loss 3.5791, time 1020.77ms, mfu 3.61%
  data: 234.9ms, grad_accum: 966.2ms (fw: 13.5ms, bw: 180.9ms)
  grad_proc: 16.7ms, optimizer: 1.7ms, param_check: 19.4ms
  loss_proc: 0.0ms, instability: 77.2ms
  cleanup: 0.2ms, gpu_sync: 0.6ms
  measured: 1004.9ms, unaccounted: 15.9ms (1.6%)
iter 3120: loss 3.4285, time 1006.45ms, mfu 3.64%
  data: 233.8ms, grad_accum: 963.7ms (fw: 12.8ms, bw: 180.4ms)
  grad_proc: 18.1ms, optimizer: 1.7ms, param_check: 19.7ms
  loss_proc: 0.0ms, instability: 77.0ms
  cleanup: 0.3ms, gpu_sync: 0.6ms
  measured: 1004.0ms, unaccounted: 2.4ms (0.2%)
iter 3140: loss 3.4752, time 1015.95ms, mfu 3.65%
  data: 233.0ms, grad_accum: 966.5ms (fw: 12.5ms, bw: 179.7ms)
  grad_proc: 18.0ms, optimizer: 1.3ms, param_check: 20.1ms
  loss_proc: 0.0ms, instability: 76.8ms
  cleanup: 0.2ms, gpu_sync: 0.8ms
  measured: 1006.9ms, unaccounted: 9.1ms (0.9%)
iter 3160: loss 3.4301, time 968.28ms, mfu 3.69%
  data: 228.2ms, grad_accum: 933.8ms (fw: 13.1ms, bw: 177.1ms)
  grad_proc: 16.9ms, optimizer: 1.6ms, param_check: 18.4ms
  loss_proc: 0.0ms, instability: 73.1ms
  cleanup: 0.5ms, gpu_sync: 0.7ms
  measured: 971.8ms, unaccounted: -3.5ms (-0.4%)
iter 3180: loss 3.4123, time 1100.33ms, mfu 3.67%
  data: 229.9ms, grad_accum: 954.4ms (fw: 14.5ms, bw: 179.8ms)
  grad_proc: 16.6ms, optimizer: 1.5ms, param_check: 19.5ms
  loss_proc: 0.0ms, instability: 80.7ms
  cleanup: 0.2ms, gpu_sync: 0.6ms
  measured: 992.8ms, unaccounted: 107.6ms (9.8%)

--- Starting validation at iteration 3200 ---
Using validation set with samples from all 14 stages
  Validation complete: 20 batches processed (320 samples), avg loss = 3.5456
  Per-stage validation losses:
    Stage 0 (sticky): 3.2245 (23 samples) - ratio=0.4
    Stage 1 (sticky): 3.2961 (16 samples) - ratio=0.6
    Stage 2 (random): 3.7301 (23 samples) - max_ratio=0.5
    Stage 3 (sticky): 3.2989 (7 samples) - ratio=0.6
    Stage 4 (sticky): 2.9239 (16 samples) - ratio=0.6
    Stage 5 (random): 3.0976 (23 samples) - max_ratio=0.2
    Stage 6 (sticky): 3.1977 (7 samples) - ratio=0.2
    Stage 7 (sticky): 5.7163 (16 samples) - ratio=0.4
    Stage 8 (sticky): 4.7169 (23 samples) - ratio=0.4
    Stage 9 (sticky): 3.8753 (7 samples) - ratio=0.6
    Stage 10 (sticky): 2.9436 (23 samples) - ratio=0.7
    Stage 11 (sticky): 3.2317 (16 samples) - ratio=0.8
    Stage 12 (sticky): 3.3456 (7 samples) - ratio=0.8
    Stage 13 (sticky): 3.3004 (23 samples) - ratio=0.9
--- Validation complete ---
step 3200: train loss 3.3909, val loss 3.5456, lr 0.000196
Stage 0 (sticky): target_ratio=0.4, p1=0.1, p2=0.3, stale_count=0
  val model vs random: 30.91x better
  val avg correct prob: 0.3864 (random: 0.0125)
  val signal to noise: 17.16 (median: 1.09)
  Most likely guess correct P %: 51.2%
  Stage 0: New best val loss 3.5456, reset stale count to 0

saving checkpoint to out/ckpt_unmasking_3200.pt
iter 3200: loss 3.4209, time 4746.60ms, mfu 3.39%
  data: 224.9ms, grad_accum: 930.6ms (fw: 12.7ms, bw: 173.6ms)
  grad_proc: 18.3ms, optimizer: 1.3ms, param_check: 19.9ms
  loss_proc: 0.0ms, instability: 74.4ms
  cleanup: 0.3ms, gpu_sync: 0.5ms
  measured: 970.9ms, unaccounted: 3775.7ms (79.5%)
  validation: 6499.60ms (data: 34.64ms, forward: 0.00ms, loss: 66.54ms)
Masking: stage=0 (sticky), actual_ratio=0.503, target=0.4, p1=0.1, p2=0.3
iter 3220: loss 3.4007, time 959.33ms, mfu 3.45%
  data: 227.8ms, grad_accum: 929.6ms (fw: 13.6ms, bw: 174.1ms)
  grad_proc: 17.1ms, optimizer: 1.4ms, param_check: 18.8ms
  loss_proc: 0.0ms, instability: 71.8ms
  cleanup: 0.2ms, gpu_sync: 0.5ms
  measured: 967.7ms, unaccounted: -8.4ms (-0.9%)
iter 3240: loss 3.3803, time 1044.08ms, mfu 3.48%
  data: 231.2ms, grad_accum: 944.4ms (fw: 15.4ms, bw: 178.0ms)
  grad_proc: 18.3ms, optimizer: 1.4ms, param_check: 18.3ms
  loss_proc: 0.0ms, instability: 76.8ms
  cleanup: 0.3ms, gpu_sync: 0.5ms
  measured: 983.2ms, unaccounted: 60.9ms (5.8%)
iter 3260: loss 3.3350, time 967.02ms, mfu 3.53%
  data: 227.3ms, grad_accum: 932.9ms (fw: 13.1ms, bw: 176.5ms)
  grad_proc: 18.0ms, optimizer: 1.6ms, param_check: 20.5ms
  loss_proc: 0.0ms, instability: 71.2ms
  cleanup: 0.4ms, gpu_sync: 0.5ms
  measured: 973.9ms, unaccounted: -6.9ms (-0.7%)
iter 3280: loss 3.3807, time 987.39ms, mfu 3.57%
  data: 230.5ms, grad_accum: 944.1ms (fw: 12.4ms, bw: 179.1ms)
  grad_proc: 16.1ms, optimizer: 1.6ms, param_check: 19.2ms
  loss_proc: 0.0ms, instability: 74.3ms
  cleanup: 0.4ms, gpu_sync: 0.5ms
  measured: 981.9ms, unaccounted: 5.5ms (0.6%)
iter 3300: loss 3.4894, time 1092.71ms, mfu 3.57%
  data: 244.9ms, grad_accum: 989.4ms (fw: 12.7ms, bw: 188.4ms)
  grad_proc: 19.5ms, optimizer: 1.6ms, param_check: 19.2ms
  loss_proc: 0.0ms, instability: 82.3ms
  cleanup: 0.2ms, gpu_sync: 0.4ms
  measured: 1030.4ms, unaccounted: 62.3ms (5.7%)
iter 3320: loss 3.4854, time 1003.47ms, mfu 3.60%
  data: 237.2ms, grad_accum: 983.8ms (fw: 12.9ms, bw: 184.9ms)
  grad_proc: 18.8ms, optimizer: 1.7ms, param_check: 20.0ms
  loss_proc: 0.0ms, instability: 76.9ms
  cleanup: 0.3ms, gpu_sync: 0.3ms
  measured: 1024.9ms, unaccounted: -21.4ms (-2.1%)
iter 3340: loss 3.3917, time 968.52ms, mfu 3.64%
  data: 234.7ms, grad_accum: 969.0ms (fw: 12.6ms, bw: 182.0ms)
  grad_proc: 17.4ms, optimizer: 1.7ms, param_check: 20.9ms
  loss_proc: 0.0ms, instability: 73.4ms
  cleanup: 0.4ms, gpu_sync: 0.3ms
  measured: 1009.7ms, unaccounted: -41.1ms (-4.2%)
iter 3360: loss 3.5669, time 1018.18ms, mfu 3.65%
  data: 241.3ms, grad_accum: 993.5ms (fw: 12.6ms, bw: 187.3ms)
  grad_proc: 17.9ms, optimizer: 1.5ms, param_check: 19.4ms
  loss_proc: 0.0ms, instability: 79.4ms
  cleanup: 0.3ms, gpu_sync: 0.3ms
  measured: 1032.9ms, unaccounted: -14.7ms (-1.4%)
iter 3380: loss 3.2584, time 1057.58ms, mfu 3.66%
  data: 237.4ms, grad_accum: 990.7ms (fw: 13.0ms, bw: 185.1ms)
  grad_proc: 18.5ms, optimizer: 1.9ms, param_check: 19.5ms
  loss_proc: 0.0ms, instability: 79.5ms
  cleanup: 0.3ms, gpu_sync: 0.4ms
  measured: 1031.3ms, unaccounted: 26.3ms (2.5%)

--- Starting validation at iteration 3400 ---
Using validation set with samples from all 14 stages
  Validation complete: 20 batches processed (320 samples), avg loss = 3.5132
  Per-stage validation losses:
    Stage 0 (sticky): 3.1934 (23 samples) - ratio=0.4
    Stage 1 (sticky): 3.2645 (16 samples) - ratio=0.6
    Stage 2 (random): 3.6985 (23 samples) - max_ratio=0.5
    Stage 3 (sticky): 3.2493 (7 samples) - ratio=0.6
    Stage 4 (sticky): 2.9034 (16 samples) - ratio=0.6
    Stage 5 (random): 3.0757 (23 samples) - max_ratio=0.2
    Stage 6 (sticky): 3.1695 (7 samples) - ratio=0.2
    Stage 7 (sticky): 5.6640 (16 samples) - ratio=0.4
    Stage 8 (sticky): 4.7084 (23 samples) - ratio=0.4
    Stage 9 (sticky): 3.8086 (7 samples) - ratio=0.6
    Stage 10 (sticky): 2.8922 (23 samples) - ratio=0.7
    Stage 11 (sticky): 3.1872 (16 samples) - ratio=0.8
    Stage 12 (sticky): 3.3046 (7 samples) - ratio=0.8
    Stage 13 (sticky): 3.2886 (23 samples) - ratio=0.9
--- Validation complete ---
step 3400: train loss 3.3874, val loss 3.5132, lr 0.000195
Stage 0 (sticky): target_ratio=0.4, p1=0.1, p2=0.3, stale_count=0
  val model vs random: 31.55x better
  val avg correct prob: 0.3944 (random: 0.0125)
  val signal to noise: 17.68 (median: 1.16)
  Most likely guess correct P %: 52.0%
  Stage 0: New best val loss 3.5132, reset stale count to 0

saving checkpoint to out/ckpt_unmasking_3400.pt
iter 3400: loss 3.5029, time 5485.28ms, mfu 3.36%
  data: 247.2ms, grad_accum: 1001.1ms (fw: 16.4ms, bw: 188.2ms)
  grad_proc: 20.1ms, optimizer: 1.6ms, param_check: 21.4ms
  loss_proc: 0.0ms, instability: 73.2ms
  cleanup: 0.2ms, gpu_sync: 0.5ms
  measured: 1045.0ms, unaccounted: 4440.2ms (80.9%)
  validation: 6374.14ms (data: 23.00ms, forward: 0.00ms, loss: 61.80ms)
Masking: stage=0 (sticky), actual_ratio=0.505, target=0.4, p1=0.1, p2=0.3
iter 3420: loss 3.5435, time 1115.83ms, mfu 3.37%
  data: 258.6ms, grad_accum: 1073.6ms (fw: 13.6ms, bw: 204.9ms)
  grad_proc: 17.9ms, optimizer: 1.2ms, param_check: 20.6ms
  loss_proc: 0.0ms, instability: 86.8ms
  cleanup: 0.1ms, gpu_sync: 0.5ms
  measured: 1114.0ms, unaccounted: 1.9ms (0.2%)
iter 3440: loss 3.4600, time 1002.85ms, mfu 3.42%
  data: 235.6ms, grad_accum: 971.6ms (fw: 12.5ms, bw: 180.7ms)
  grad_proc: 19.0ms, optimizer: 1.6ms, param_check: 20.5ms
  loss_proc: 0.0ms, instability: 75.2ms
  cleanup: 0.2ms, gpu_sync: 0.5ms
  measured: 1013.5ms, unaccounted: -10.7ms (-1.1%)
iter 3460: loss 3.5118, time 1016.06ms, mfu 3.46%
  data: 235.9ms, grad_accum: 969.3ms (fw: 13.4ms, bw: 184.2ms)
  grad_proc: 17.6ms, optimizer: 1.5ms, param_check: 20.2ms
  loss_proc: 0.0ms, instability: 76.4ms
  cleanup: 0.2ms, gpu_sync: 0.5ms
  measured: 1009.3ms, unaccounted: 6.7ms (0.7%)
iter 3480: loss 3.3837, time 1006.78ms, mfu 3.50%
  data: 238.6ms, grad_accum: 980.1ms (fw: 12.9ms, bw: 185.9ms)
  grad_proc: 18.3ms, optimizer: 1.4ms, param_check: 20.1ms
  loss_proc: 0.0ms, instability: 77.0ms
  cleanup: 0.4ms, gpu_sync: 0.4ms
  measured: 1020.7ms, unaccounted: -13.9ms (-1.4%)
iter 3500: loss 3.4363, time 1005.51ms, mfu 3.53%
  data: 233.6ms, grad_accum: 966.5ms (fw: 12.6ms, bw: 183.6ms)
  grad_proc: 19.7ms, optimizer: 1.3ms, param_check: 21.7ms
  loss_proc: 0.0ms, instability: 76.7ms
  cleanup: 0.0ms, gpu_sync: 0.5ms
  measured: 1009.7ms, unaccounted: -4.2ms (-0.4%)
iter 3520: loss 3.4662, time 1020.89ms, mfu 3.56%
  data: 235.5ms, grad_accum: 984.4ms (fw: 13.4ms, bw: 180.9ms)
  grad_proc: 18.2ms, optimizer: 1.5ms, param_check: 20.3ms
  loss_proc: 0.0ms, instability: 76.7ms
  cleanup: 0.2ms, gpu_sync: 0.5ms
  measured: 1025.2ms, unaccounted: -4.3ms (-0.4%)
iter 3540: loss 3.4282, time 1006.29ms, mfu 3.59%
  data: 236.5ms, grad_accum: 981.8ms (fw: 12.5ms, bw: 182.0ms)
  grad_proc: 18.5ms, optimizer: 1.3ms, param_check: 18.8ms
  loss_proc: 0.0ms, instability: 75.9ms
  cleanup: 0.2ms, gpu_sync: 0.6ms
  measured: 1021.2ms, unaccounted: -14.9ms (-1.5%)
iter 3560: loss 3.4017, time 1000.13ms, mfu 3.62%
  data: 226.4ms, grad_accum: 928.8ms (fw: 12.6ms, bw: 177.1ms)
  grad_proc: 17.9ms, optimizer: 1.4ms, param_check: 20.1ms
  loss_proc: 0.1ms, instability: 75.3ms
  cleanup: 0.4ms, gpu_sync: 0.6ms
  measured: 969.1ms, unaccounted: 31.0ms (3.1%)
iter 3580: loss 3.3560, time 995.51ms, mfu 3.65%
  data: 241.6ms, grad_accum: 994.1ms (fw: 14.0ms, bw: 185.5ms)
  grad_proc: 17.6ms, optimizer: 1.8ms, param_check: 20.0ms
  loss_proc: 0.0ms, instability: 76.1ms
  cleanup: 0.3ms, gpu_sync: 0.5ms
  measured: 1034.4ms, unaccounted: -38.8ms (-3.9%)

--- Starting validation at iteration 3600 ---
Using validation set with samples from all 14 stages
  Validation complete: 20 batches processed (320 samples), avg loss = 3.4909
  Per-stage validation losses:
    Stage 0 (sticky): 3.1671 (23 samples) - ratio=0.4
    Stage 1 (sticky): 3.2654 (16 samples) - ratio=0.6
    Stage 2 (random): 3.6714 (23 samples) - max_ratio=0.5
    Stage 3 (sticky): 3.2319 (7 samples) - ratio=0.6
    Stage 4 (sticky): 2.8930 (16 samples) - ratio=0.6
    Stage 5 (random): 3.0731 (23 samples) - max_ratio=0.2
    Stage 6 (sticky): 3.1788 (7 samples) - ratio=0.2
    Stage 7 (sticky): 5.5894 (16 samples) - ratio=0.4
    Stage 8 (sticky): 4.6357 (23 samples) - ratio=0.4
    Stage 9 (sticky): 3.7815 (7 samples) - ratio=0.6
    Stage 10 (sticky): 2.8924 (23 samples) - ratio=0.7
    Stage 11 (sticky): 3.1774 (16 samples) - ratio=0.8
    Stage 12 (sticky): 3.2933 (7 samples) - ratio=0.8
    Stage 13 (sticky): 3.2635 (23 samples) - ratio=0.9
--- Validation complete ---
step 3600: train loss 3.3342, val loss 3.4909, lr 0.000194
Stage 0 (sticky): target_ratio=0.4, p1=0.1, p2=0.3, stale_count=0
  val model vs random: 31.33x better
  val avg correct prob: 0.3916 (random: 0.0125)
  val signal to noise: 17.72 (median: 1.17)
  Most likely guess correct P %: 52.2%
  Stage 0: New best val loss 3.4909, reset stale count to 0

saving checkpoint to out/ckpt_unmasking_3600.pt
iter 3600: loss 3.4922, time 4836.03ms, mfu 3.36%
  data: 227.8ms, grad_accum: 936.9ms (fw: 12.8ms, bw: 176.9ms)
  grad_proc: 16.6ms, optimizer: 1.5ms, param_check: 19.0ms
  loss_proc: 0.0ms, instability: 73.2ms
  cleanup: 0.2ms, gpu_sync: 0.4ms
  measured: 974.5ms, unaccounted: 3861.6ms (79.9%)
  validation: 6230.47ms (data: 23.14ms, forward: 0.00ms, loss: 61.51ms)
Masking: stage=0 (sticky), actual_ratio=0.523, target=0.4, p1=0.1, p2=0.3
iter 3620: loss 3.6189, time 980.01ms, mfu 3.42%
  data: 226.4ms, grad_accum: 929.7ms (fw: 12.8ms, bw: 175.2ms)
  grad_proc: 17.0ms, optimizer: 1.5ms, param_check: 18.5ms
  loss_proc: 0.0ms, instability: 72.8ms
  cleanup: 0.5ms, gpu_sync: 0.3ms
  measured: 967.4ms, unaccounted: 12.6ms (1.3%)
iter 3640: loss 3.3775, time 967.76ms, mfu 3.48%
  data: 235.5ms, grad_accum: 963.1ms (fw: 13.4ms, bw: 185.0ms)
  grad_proc: 18.5ms, optimizer: 1.5ms, param_check: 19.6ms
  loss_proc: 0.0ms, instability: 73.1ms
  cleanup: 0.3ms, gpu_sync: 0.4ms
  measured: 1003.3ms, unaccounted: -35.5ms (-3.7%)
iter 3660: loss 3.4288, time 985.44ms, mfu 3.52%
  data: 229.7ms, grad_accum: 944.2ms (fw: 14.7ms, bw: 177.9ms)
  grad_proc: 16.3ms, optimizer: 1.5ms, param_check: 19.3ms
  loss_proc: 0.0ms, instability: 72.2ms
  cleanup: 0.4ms, gpu_sync: 0.4ms
  measured: 982.1ms, unaccounted: 3.3ms (0.3%)
iter 3680: loss 3.4393, time 986.42ms, mfu 3.56%
  data: 232.9ms, grad_accum: 947.9ms (fw: 12.8ms, bw: 179.1ms)
  grad_proc: 17.1ms, optimizer: 1.6ms, param_check: 20.2ms
  loss_proc: 0.0ms, instability: 74.5ms
  cleanup: 0.5ms, gpu_sync: 0.4ms
  measured: 987.6ms, unaccounted: -1.2ms (-0.1%)
iter 3700: loss 3.3555, time 980.75ms, mfu 3.60%
  data: 232.1ms, grad_accum: 942.1ms (fw: 12.9ms, bw: 178.9ms)
  grad_proc: 19.2ms, optimizer: 1.4ms, param_check: 21.2ms
  loss_proc: 0.0ms, instability: 74.3ms
  cleanup: 0.4ms, gpu_sync: 0.3ms
  measured: 984.5ms, unaccounted: -3.8ms (-0.4%)
iter 3720: loss 3.3623, time 985.55ms, mfu 3.64%
  data: 228.2ms, grad_accum: 947.2ms (fw: 13.0ms, bw: 177.9ms)
  grad_proc: 16.2ms, optimizer: 1.4ms, param_check: 19.3ms
  loss_proc: 0.0ms, instability: 74.9ms
  cleanup: 0.2ms, gpu_sync: 0.4ms
  measured: 984.8ms, unaccounted: 0.8ms (0.1%)
iter 3740: loss 3.4687, time 1016.79ms, mfu 3.65%
  data: 233.8ms, grad_accum: 964.7ms (fw: 13.3ms, bw: 181.5ms)
  grad_proc: 18.3ms, optimizer: 1.6ms, param_check: 21.3ms
  loss_proc: 0.0ms, instability: 77.5ms
  cleanup: 0.4ms, gpu_sync: 0.4ms
  measured: 1006.7ms, unaccounted: 10.1ms (1.0%)
iter 3760: loss 3.3270, time 1017.78ms, mfu 3.67%
  data: 227.5ms, grad_accum: 938.6ms (fw: 15.7ms, bw: 177.1ms)
  grad_proc: 16.5ms, optimizer: 1.5ms, param_check: 18.7ms
  loss_proc: 0.0ms, instability: 72.8ms
  cleanup: 0.3ms, gpu_sync: 0.4ms
  measured: 975.9ms, unaccounted: 41.9ms (4.1%)
iter 3780: loss 3.4447, time 991.19ms, mfu 3.69%
  data: 222.1ms, grad_accum: 912.9ms (fw: 13.3ms, bw: 171.9ms)
  grad_proc: 15.9ms, optimizer: 1.8ms, param_check: 18.0ms
  loss_proc: 0.0ms, instability: 73.6ms
  cleanup: 0.4ms, gpu_sync: 0.4ms
  measured: 949.5ms, unaccounted: 41.7ms (4.2%)

--- Starting validation at iteration 3800 ---
Using validation set with samples from all 14 stages
  Validation complete: 20 batches processed (320 samples), avg loss = 3.4693
  Per-stage validation losses:
    Stage 0 (sticky): 3.1521 (23 samples) - ratio=0.4
    Stage 1 (sticky): 3.2683 (16 samples) - ratio=0.6
    Stage 2 (random): 3.6633 (23 samples) - max_ratio=0.5
    Stage 3 (sticky): 3.1797 (7 samples) - ratio=0.6
    Stage 4 (sticky): 2.8891 (16 samples) - ratio=0.6
    Stage 5 (random): 3.0625 (23 samples) - max_ratio=0.2
    Stage 6 (sticky): 3.1554 (7 samples) - ratio=0.2
    Stage 7 (sticky): 5.5605 (16 samples) - ratio=0.4
    Stage 8 (sticky): 4.5643 (23 samples) - ratio=0.4
    Stage 9 (sticky): 3.7263 (7 samples) - ratio=0.6
    Stage 10 (sticky): 2.8516 (23 samples) - ratio=0.7
    Stage 11 (sticky): 3.1664 (16 samples) - ratio=0.8
    Stage 12 (sticky): 3.2977 (7 samples) - ratio=0.8
    Stage 13 (sticky): 3.2775 (23 samples) - ratio=0.9
--- Validation complete ---
step 3800: train loss 3.3443, val loss 3.4693, lr 0.000192
Stage 0 (sticky): target_ratio=0.4, p1=0.1, p2=0.3, stale_count=0
  val model vs random: 32.26x better
  val avg correct prob: 0.4033 (random: 0.0125)
  val signal to noise: 18.84 (median: 1.23)
  Most likely guess correct P %: 52.6%
  Stage 0: New best val loss 3.4693, reset stale count to 0

saving checkpoint to out/ckpt_unmasking_3800.pt
iter 3800: loss 3.4808, time 5225.57ms, mfu 3.40%
  data: 234.2ms, grad_accum: 961.6ms (fw: 12.8ms, bw: 182.1ms)
  grad_proc: 18.2ms, optimizer: 1.6ms, param_check: 20.5ms
  loss_proc: 0.0ms, instability: 77.2ms
  cleanup: 0.4ms, gpu_sync: 0.4ms
  measured: 1002.6ms, unaccounted: 4223.0ms (80.8%)
  validation: 6116.68ms (data: 20.86ms, forward: 0.00ms, loss: 62.28ms)
Masking: stage=0 (sticky), actual_ratio=0.503, target=0.4, p1=0.1, p2=0.3
iter 3820: loss 3.3138, time 1108.94ms, mfu 3.41%
  data: 235.5ms, grad_accum: 970.3ms (fw: 13.1ms, bw: 181.5ms)
  grad_proc: 19.4ms, optimizer: 1.6ms, param_check: 21.5ms
  loss_proc: 0.0ms, instability: 85.5ms
  cleanup: 0.5ms, gpu_sync: 0.5ms
  measured: 1013.8ms, unaccounted: 95.1ms (8.6%)
iter 3840: loss 3.4407, time 1092.04ms, mfu 3.42%
  data: 253.5ms, grad_accum: 1049.4ms (fw: 13.5ms, bw: 198.5ms)
  grad_proc: 17.2ms, optimizer: 1.5ms, param_check: 21.1ms
  loss_proc: 0.0ms, instability: 84.2ms
  cleanup: 0.3ms, gpu_sync: 0.4ms
  measured: 1089.9ms, unaccounted: 2.1ms (0.2%)
iter 3860: loss 3.4302, time 1005.63ms, mfu 3.46%
  data: 245.5ms, grad_accum: 1019.5ms (fw: 12.5ms, bw: 191.4ms)
  grad_proc: 19.0ms, optimizer: 1.8ms, param_check: 22.7ms
  loss_proc: 0.0ms, instability: 78.6ms
  cleanup: 0.3ms, gpu_sync: 0.5ms
  measured: 1063.8ms, unaccounted: -58.2ms (-5.8%)
iter 3880: loss 3.4793, time 1004.91ms, mfu 3.50%
  data: 312.8ms, grad_accum: 1144.5ms (fw: 15.9ms, bw: 194.6ms)
  grad_proc: 22.0ms, optimizer: 3.3ms, param_check: 36.7ms
  loss_proc: 0.0ms, instability: 72.5ms
  cleanup: 0.6ms, gpu_sync: 0.6ms
  measured: 1207.7ms, unaccounted: -202.8ms (-20.2%)
iter 3900: loss 3.4439, time 987.64ms, mfu 3.55%
  data: 252.9ms, grad_accum: 1049.5ms (fw: 12.6ms, bw: 197.5ms)
  grad_proc: 17.8ms, optimizer: 1.2ms, param_check: 20.8ms
  loss_proc: 0.0ms, instability: 75.6ms
  cleanup: 0.6ms, gpu_sync: 0.6ms
  measured: 1090.5ms, unaccounted: -102.9ms (-10.4%)
iter 3920: loss 3.2744, time 992.54ms, mfu 3.58%
  data: 240.1ms, grad_accum: 994.1ms (fw: 13.2ms, bw: 188.7ms)
  grad_proc: 17.8ms, optimizer: 1.6ms, param_check: 19.6ms
  loss_proc: 0.0ms, instability: 75.2ms
  cleanup: 0.5ms, gpu_sync: 0.5ms
  measured: 1034.2ms, unaccounted: -41.7ms (-4.2%)
iter 3940: loss 3.3431, time 991.28ms, mfu 3.61%
  data: 234.0ms, grad_accum: 959.5ms (fw: 12.9ms, bw: 180.0ms)
  grad_proc: 19.0ms, optimizer: 1.5ms, param_check: 20.8ms
  loss_proc: 0.0ms, instability: 75.4ms
  cleanup: 0.2ms, gpu_sync: 0.5ms
  measured: 1001.6ms, unaccounted: -10.3ms (-1.0%)
iter 3960: loss 3.2808, time 993.76ms, mfu 3.64%
  data: 231.7ms, grad_accum: 956.4ms (fw: 13.2ms, bw: 179.5ms)
  grad_proc: 17.8ms, optimizer: 1.5ms, param_check: 19.8ms
  loss_proc: 0.0ms, instability: 75.1ms
  cleanup: 0.5ms, gpu_sync: 0.4ms
  measured: 996.4ms, unaccounted: -2.6ms (-0.3%)
iter 3980: loss 3.2908, time 1054.47ms, mfu 3.65%
  data: 248.3ms, grad_accum: 1018.0ms (fw: 13.0ms, bw: 195.2ms)
  grad_proc: 18.9ms, optimizer: 1.7ms, param_check: 21.0ms
  loss_proc: 0.0ms, instability: 80.7ms
  cleanup: 0.4ms, gpu_sync: 0.4ms
  measured: 1060.5ms, unaccounted: -6.0ms (-0.6%)

--- Starting validation at iteration 4000 ---
Using validation set with samples from all 14 stages
  Validation complete: 20 batches processed (320 samples), avg loss = 3.4391
  Per-stage validation losses:
    Stage 0 (sticky): 3.1257 (23 samples) - ratio=0.4
    Stage 1 (sticky): 3.2471 (16 samples) - ratio=0.6
    Stage 2 (random): 3.6309 (23 samples) - max_ratio=0.5
    Stage 3 (sticky): 3.1377 (7 samples) - ratio=0.6
    Stage 4 (sticky): 2.8559 (16 samples) - ratio=0.6
    Stage 5 (random): 3.0485 (23 samples) - max_ratio=0.2
    Stage 6 (sticky): 3.1506 (7 samples) - ratio=0.2
    Stage 7 (sticky): 5.4520 (16 samples) - ratio=0.4
    Stage 8 (sticky): 4.5392 (23 samples) - ratio=0.4
    Stage 9 (sticky): 3.7042 (7 samples) - ratio=0.6
    Stage 10 (sticky): 2.8182 (23 samples) - ratio=0.7
    Stage 11 (sticky): 3.1409 (16 samples) - ratio=0.8
    Stage 12 (sticky): 3.2582 (7 samples) - ratio=0.8
    Stage 13 (sticky): 3.2551 (23 samples) - ratio=0.9
--- Validation complete ---
step 4000: train loss 3.3283, val loss 3.4391, lr 0.000190
Stage 0 (sticky): target_ratio=0.4, p1=0.1, p2=0.3, stale_count=0
  val model vs random: 32.50x better
  val avg correct prob: 0.4062 (random: 0.0125)
  val signal to noise: 19.33 (median: 1.26)
  Most likely guess correct P %: 53.1%
  Stage 0: New best val loss 3.4391, reset stale count to 0

saving checkpoint to out/ckpt_unmasking_4000.pt
iter 4000: loss 3.3558, time 5021.41ms, mfu 3.36%
  data: 247.7ms, grad_accum: 1014.9ms (fw: 14.1ms, bw: 192.2ms)
  grad_proc: 18.8ms, optimizer: 1.7ms, param_check: 20.8ms
  loss_proc: 0.0ms, instability: 76.0ms
  cleanup: 0.6ms, gpu_sync: 0.5ms
  measured: 1057.2ms, unaccounted: 3964.2ms (78.9%)
  validation: 6005.73ms (data: 21.16ms, forward: 0.00ms, loss: 62.24ms)
Masking: stage=0 (sticky), actual_ratio=0.509, target=0.4, p1=0.1, p2=0.3
iter 4020: loss 3.4279, time 1013.63ms, mfu 3.40%
  data: 232.4ms, grad_accum: 968.7ms (fw: 12.7ms, bw: 181.8ms)
  grad_proc: 19.3ms, optimizer: 1.5ms, param_check: 19.9ms
  loss_proc: 0.0ms, instability: 77.1ms
  cleanup: 0.4ms, gpu_sync: 0.4ms
  measured: 1010.2ms, unaccounted: 3.4ms (0.3%)
iter 4040: loss 3.2866, time 998.16ms, mfu 3.45%
  data: 233.3ms, grad_accum: 968.8ms (fw: 13.7ms, bw: 183.3ms)
  grad_proc: 17.0ms, optimizer: 1.4ms, param_check: 20.0ms
  loss_proc: 0.0ms, instability: 75.4ms
  cleanup: 0.2ms, gpu_sync: 0.5ms
  measured: 1007.8ms, unaccounted: -9.6ms (-1.0%)
iter 4060: loss 3.4928, time 1123.78ms, mfu 3.45%
  data: 242.5ms, grad_accum: 989.3ms (fw: 15.2ms, bw: 187.9ms)
  grad_proc: 18.9ms, optimizer: 1.8ms, param_check: 21.9ms
  loss_proc: 0.0ms, instability: 81.4ms
  cleanup: 0.5ms, gpu_sync: 0.4ms
  measured: 1032.7ms, unaccounted: 91.0ms (8.1%)
iter 4080: loss 3.4703, time 1024.17ms, mfu 3.48%
  data: 238.5ms, grad_accum: 998.6ms (fw: 12.6ms, bw: 186.2ms)
  grad_proc: 17.9ms, optimizer: 1.7ms, param_check: 20.2ms
  loss_proc: 0.0ms, instability: 77.5ms
  cleanup: 0.5ms, gpu_sync: 0.3ms
  measured: 1039.2ms, unaccounted: -15.0ms (-1.5%)
iter 4100: loss 3.3001, time 1021.27ms, mfu 3.52%
  data: 235.1ms, grad_accum: 973.1ms (fw: 13.4ms, bw: 184.0ms)
  grad_proc: 19.1ms, optimizer: 1.4ms, param_check: 21.6ms
  loss_proc: 0.1ms, instability: 76.3ms
  cleanup: 0.3ms, gpu_sync: 0.3ms
  measured: 1015.7ms, unaccounted: 5.6ms (0.5%)
iter 4120: loss 3.3297, time 1011.34ms, mfu 3.55%
  data: 244.0ms, grad_accum: 1008.9ms (fw: 13.8ms, bw: 191.6ms)
  grad_proc: 19.0ms, optimizer: 1.5ms, param_check: 20.3ms
  loss_proc: 0.0ms, instability: 76.1ms
  cleanup: 0.3ms, gpu_sync: 0.3ms
  measured: 1050.3ms, unaccounted: -39.0ms (-3.9%)
iter 4140: loss 3.3079, time 1005.44ms, mfu 3.58%
  data: 233.0ms, grad_accum: 970.6ms (fw: 12.7ms, bw: 182.8ms)
  grad_proc: 18.5ms, optimizer: 1.4ms, param_check: 19.7ms
  loss_proc: 0.0ms, instability: 77.4ms
  cleanup: 0.1ms, gpu_sync: 0.2ms
  measured: 1010.5ms, unaccounted: -5.1ms (-0.5%)
iter 4160: loss 3.5112, time 1020.90ms, mfu 3.60%
  data: 234.0ms, grad_accum: 965.4ms (fw: 13.1ms, bw: 183.0ms)
  grad_proc: 18.1ms, optimizer: 1.5ms, param_check: 19.6ms
  loss_proc: 0.0ms, instability: 77.0ms
  cleanup: 0.4ms, gpu_sync: 0.3ms
  measured: 1005.3ms, unaccounted: 15.6ms (1.5%)
iter 4180: loss 3.5276, time 999.69ms, mfu 3.63%
  data: 240.6ms, grad_accum: 981.2ms (fw: 13.1ms, bw: 187.0ms)
  grad_proc: 17.2ms, optimizer: 1.5ms, param_check: 20.2ms
  loss_proc: 0.0ms, instability: 74.5ms
  cleanup: 0.6ms, gpu_sync: 0.4ms
  measured: 1021.0ms, unaccounted: -21.3ms (-2.1%)

--- Starting validation at iteration 4200 ---
Using validation set with samples from all 14 stages
  Validation complete: 20 batches processed (320 samples), avg loss = 3.4193
  Per-stage validation losses:
    Stage 0 (sticky): 3.0992 (23 samples) - ratio=0.4
    Stage 1 (sticky): 3.2420 (16 samples) - ratio=0.6
    Stage 2 (random): 3.6200 (23 samples) - max_ratio=0.5
    Stage 3 (sticky): 3.1206 (7 samples) - ratio=0.6
    Stage 4 (sticky): 2.8539 (16 samples) - ratio=0.6
    Stage 5 (random): 3.0552 (23 samples) - max_ratio=0.2
    Stage 6 (sticky): 3.1342 (7 samples) - ratio=0.2
    Stage 7 (sticky): 5.4008 (16 samples) - ratio=0.4
    Stage 8 (sticky): 4.5036 (23 samples) - ratio=0.4
    Stage 9 (sticky): 3.6441 (7 samples) - ratio=0.6
    Stage 10 (sticky): 2.7997 (23 samples) - ratio=0.7
    Stage 11 (sticky): 3.1199 (16 samples) - ratio=0.8
    Stage 12 (sticky): 3.2403 (7 samples) - ratio=0.8
    Stage 13 (sticky): 3.2373 (23 samples) - ratio=0.9
--- Validation complete ---
step 4200: train loss 3.3063, val loss 3.4193, lr 0.000188
Stage 0 (sticky): target_ratio=0.4, p1=0.1, p2=0.3, stale_count=0
  val model vs random: 33.20x better
  val avg correct prob: 0.4150 (random: 0.0125)
  val signal to noise: 20.01 (median: 1.34)
  Most likely guess correct P %: 53.7%
  Stage 0: New best val loss 3.4193, reset stale count to 0

saving checkpoint to out/ckpt_unmasking_4200.pt
iter 4200: loss 3.4827, time 4911.91ms, mfu 3.34%
  data: 233.4ms, grad_accum: 983.0ms (fw: 12.6ms, bw: 180.8ms)
  grad_proc: 16.2ms, optimizer: 1.6ms, param_check: 19.6ms
  loss_proc: 0.0ms, instability: 82.9ms
  cleanup: 0.3ms, gpu_sync: 0.4ms
  measured: 1021.1ms, unaccounted: 3890.9ms (79.2%)
  validation: 5899.23ms (data: 20.57ms, forward: 0.00ms, loss: 61.81ms)
Masking: stage=0 (sticky), actual_ratio=0.492, target=0.4, p1=0.1, p2=0.3
iter 4220: loss 3.3528, time 975.16ms, mfu 3.41%
  data: 234.2ms, grad_accum: 956.1ms (fw: 15.2ms, bw: 176.0ms)
  grad_proc: 18.6ms, optimizer: 1.9ms, param_check: 20.6ms
  loss_proc: 0.0ms, instability: 70.9ms
  cleanup: 0.4ms, gpu_sync: 0.5ms
  measured: 998.1ms, unaccounted: -22.9ms (-2.4%)
iter 4240: loss 3.5497, time 952.77ms, mfu 3.47%
  data: 223.5ms, grad_accum: 917.5ms (fw: 14.1ms, bw: 171.0ms)
  grad_proc: 18.2ms, optimizer: 1.7ms, param_check: 19.8ms
  loss_proc: 0.0ms, instability: 70.3ms
  cleanup: 0.0ms, gpu_sync: 0.4ms
  measured: 957.5ms, unaccounted: -4.7ms (-0.5%)
iter 4260: loss 3.5014, time 946.84ms, mfu 3.53%
  data: 221.0ms, grad_accum: 909.0ms (fw: 12.3ms, bw: 172.8ms)
  grad_proc: 18.1ms, optimizer: 1.5ms, param_check: 19.2ms
  loss_proc: 0.0ms, instability: 70.7ms
  cleanup: 0.5ms, gpu_sync: 0.5ms
  measured: 948.9ms, unaccounted: -2.1ms (-0.2%)
iter 4280: loss 3.4524, time 947.63ms, mfu 3.59%
  data: 221.3ms, grad_accum: 910.2ms (fw: 12.4ms, bw: 170.5ms)
  grad_proc: 17.2ms, optimizer: 1.4ms, param_check: 18.3ms
  loss_proc: 0.0ms, instability: 70.9ms
  cleanup: 0.1ms, gpu_sync: 0.6ms
  measured: 947.9ms, unaccounted: -0.3ms (-0.0%)
iter 4300: loss 3.2265, time 949.79ms, mfu 3.64%
  data: 218.3ms, grad_accum: 898.7ms (fw: 12.8ms, bw: 167.6ms)
  grad_proc: 15.9ms, optimizer: 1.7ms, param_check: 18.8ms
  loss_proc: 0.0ms, instability: 70.2ms
  cleanup: 0.2ms, gpu_sync: 0.7ms
  measured: 935.9ms, unaccounted: 13.9ms (1.5%)
iter 4320: loss 3.4732, time 929.42ms, mfu 3.69%
  data: 217.8ms, grad_accum: 899.3ms (fw: 12.3ms, bw: 168.7ms)
  grad_proc: 16.7ms, optimizer: 1.5ms, param_check: 18.6ms
  loss_proc: 0.0ms, instability: 70.7ms
  cleanup: 0.2ms, gpu_sync: 0.7ms
  measured: 937.1ms, unaccounted: -7.6ms (-0.8%)
iter 4340: loss 3.2944, time 939.55ms, mfu 3.73%
  data: 219.2ms, grad_accum: 901.0ms (fw: 12.0ms, bw: 168.9ms)
  grad_proc: 18.9ms, optimizer: 1.7ms, param_check: 19.5ms
  loss_proc: 0.0ms, instability: 70.6ms
  cleanup: 0.6ms, gpu_sync: 0.7ms
  measured: 942.4ms, unaccounted: -2.9ms (-0.3%)
iter 4360: loss 3.3302, time 937.31ms, mfu 3.77%
  data: 216.8ms, grad_accum: 904.8ms (fw: 12.3ms, bw: 168.7ms)
  grad_proc: 18.5ms, optimizer: 1.7ms, param_check: 19.1ms
  loss_proc: 0.0ms, instability: 70.7ms
  cleanup: 0.4ms, gpu_sync: 0.6ms
  measured: 945.0ms, unaccounted: -7.7ms (-0.8%)
iter 4380: loss 3.4540, time 936.08ms, mfu 3.81%
  data: 221.0ms, grad_accum: 909.3ms (fw: 12.3ms, bw: 171.0ms)
  grad_proc: 16.7ms, optimizer: 1.6ms, param_check: 19.0ms
  loss_proc: 0.0ms, instability: 70.3ms
  cleanup: 0.2ms, gpu_sync: 0.6ms
  measured: 947.3ms, unaccounted: -11.2ms (-1.2%)

--- Starting validation at iteration 4400 ---
Using validation set with samples from all 14 stages
  Validation complete: 20 batches processed (320 samples), avg loss = 3.3961
  Per-stage validation losses:
    Stage 0 (sticky): 3.0803 (23 samples) - ratio=0.4
    Stage 1 (sticky): 3.2105 (16 samples) - ratio=0.6
    Stage 2 (random): 3.6036 (23 samples) - max_ratio=0.5
    Stage 3 (sticky): 3.1064 (7 samples) - ratio=0.6
    Stage 4 (sticky): 2.8391 (16 samples) - ratio=0.6
    Stage 5 (random): 3.0272 (23 samples) - max_ratio=0.2
    Stage 6 (sticky): 3.1155 (7 samples) - ratio=0.2
    Stage 7 (sticky): 5.3482 (16 samples) - ratio=0.4
    Stage 8 (sticky): 4.4423 (23 samples) - ratio=0.4
    Stage 9 (sticky): 3.6480 (7 samples) - ratio=0.6
    Stage 10 (sticky): 2.7862 (23 samples) - ratio=0.7
    Stage 11 (sticky): 3.0953 (16 samples) - ratio=0.8
    Stage 12 (sticky): 3.2326 (7 samples) - ratio=0.8
    Stage 13 (sticky): 3.2238 (23 samples) - ratio=0.9
--- Validation complete ---
step 4400: train loss 3.2494, val loss 3.3961, lr 0.000186
Stage 0 (sticky): target_ratio=0.4, p1=0.1, p2=0.3, stale_count=0
  val model vs random: 32.59x better
  val avg correct prob: 0.4074 (random: 0.0125)
  val signal to noise: 19.19 (median: 1.32)
  Most likely guess correct P %: 53.8%
  Stage 0: New best val loss 3.3961, reset stale count to 0

saving checkpoint to out/ckpt_unmasking_4400.pt
iter 4400: loss 3.2219, time 4569.24ms, mfu 3.51%
  data: 217.2ms, grad_accum: 900.3ms (fw: 12.7ms, bw: 168.1ms)
  grad_proc: 18.4ms, optimizer: 1.6ms, param_check: 20.4ms
  loss_proc: 0.0ms, instability: 70.3ms
  cleanup: 0.5ms, gpu_sync: 0.5ms
  measured: 941.5ms, unaccounted: 3627.7ms (79.4%)
  validation: 5791.62ms (data: 20.12ms, forward: 0.00ms, loss: 58.06ms)
Masking: stage=0 (sticky), actual_ratio=0.509, target=0.4, p1=0.1, p2=0.3
iter 4420: loss 3.6046, time 934.69ms, mfu 3.58%
  data: 216.0ms, grad_accum: 897.2ms (fw: 13.3ms, bw: 168.1ms)
  grad_proc: 17.3ms, optimizer: 1.5ms, param_check: 19.4ms
  loss_proc: 0.0ms, instability: 70.1ms
  cleanup: 0.3ms, gpu_sync: 0.4ms
  measured: 936.0ms, unaccounted: -1.3ms (-0.1%)
iter 4440: loss 3.4375, time 945.08ms, mfu 3.63%
  data: 221.1ms, grad_accum: 906.3ms (fw: 12.5ms, bw: 170.6ms)
  grad_proc: 18.7ms, optimizer: 1.6ms, param_check: 20.0ms
  loss_proc: 0.0ms, instability: 71.5ms
  cleanup: 0.2ms, gpu_sync: 0.5ms
  measured: 947.3ms, unaccounted: -2.3ms (-0.2%)
iter 4460: loss 3.3721, time 936.91ms, mfu 3.68%
  data: 216.9ms, grad_accum: 897.1ms (fw: 12.9ms, bw: 167.7ms)
  grad_proc: 16.2ms, optimizer: 1.6ms, param_check: 18.7ms
  loss_proc: 0.0ms, instability: 69.9ms
  cleanup: 0.2ms, gpu_sync: 0.4ms
  measured: 934.2ms, unaccounted: 2.7ms (0.3%)
iter 4480: loss 3.2474, time 930.57ms, mfu 3.73%
  data: 216.5ms, grad_accum: 896.3ms (fw: 12.8ms, bw: 167.6ms)
  grad_proc: 15.9ms, optimizer: 1.9ms, param_check: 18.3ms
  loss_proc: 0.0ms, instability: 70.1ms
  cleanup: 0.4ms, gpu_sync: 0.3ms
  measured: 933.2ms, unaccounted: -2.6ms (-0.3%)
iter 4500: loss 3.3993, time 950.10ms, mfu 3.76%
  data: 217.4ms, grad_accum: 905.6ms (fw: 13.0ms, bw: 168.4ms)
  grad_proc: 15.2ms, optimizer: 1.6ms, param_check: 18.6ms
  loss_proc: 0.0ms, instability: 71.7ms
  cleanup: 0.3ms, gpu_sync: 0.3ms
  measured: 941.5ms, unaccounted: 8.6ms (0.9%)
iter 4520: loss 3.3592, time 923.19ms, mfu 3.81%
  data: 216.9ms, grad_accum: 894.3ms (fw: 12.5ms, bw: 168.6ms)
  grad_proc: 15.3ms, optimizer: 1.5ms, param_check: 18.1ms
  loss_proc: 0.0ms, instability: 69.7ms
  cleanup: 0.3ms, gpu_sync: 0.2ms
  measured: 929.6ms, unaccounted: -6.4ms (-0.7%)
iter 4540: loss 3.4196, time 929.24ms, mfu 3.84%
  data: 217.7ms, grad_accum: 901.7ms (fw: 13.3ms, bw: 168.6ms)
  grad_proc: 16.3ms, optimizer: 1.4ms, param_check: 18.9ms
  loss_proc: 0.0ms, instability: 68.8ms
  cleanup: 0.2ms, gpu_sync: 0.2ms
  measured: 938.7ms, unaccounted: -9.4ms (-1.0%)
iter 4560: loss 3.2386, time 944.62ms, mfu 3.87%
  data: 218.1ms, grad_accum: 901.7ms (fw: 13.8ms, bw: 167.0ms)
  grad_proc: 16.5ms, optimizer: 1.7ms, param_check: 19.0ms
  loss_proc: 0.0ms, instability: 69.4ms
  cleanup: 0.5ms, gpu_sync: 0.3ms
  measured: 939.6ms, unaccounted: 5.0ms (0.5%)
iter 4580: loss 3.4417, time 952.05ms, mfu 3.89%
  data: 216.5ms, grad_accum: 899.3ms (fw: 12.3ms, bw: 168.0ms)
  grad_proc: 16.0ms, optimizer: 1.7ms, param_check: 19.5ms
  loss_proc: 0.0ms, instability: 73.0ms
  cleanup: 0.3ms, gpu_sync: 0.3ms
  measured: 937.0ms, unaccounted: 15.0ms (1.6%)

--- Starting validation at iteration 4600 ---
Using validation set with samples from all 14 stages
  Validation complete: 20 batches processed (320 samples), avg loss = 3.3594
  Per-stage validation losses:
    Stage 0 (sticky): 3.0487 (23 samples) - ratio=0.4
    Stage 1 (sticky): 3.1901 (16 samples) - ratio=0.6
    Stage 2 (random): 3.5705 (23 samples) - max_ratio=0.5
    Stage 3 (sticky): 3.0366 (7 samples) - ratio=0.6
    Stage 4 (sticky): 2.8074 (16 samples) - ratio=0.6
    Stage 5 (random): 3.0068 (23 samples) - max_ratio=0.2
    Stage 6 (sticky): 3.0925 (7 samples) - ratio=0.2
    Stage 7 (sticky): 5.3014 (16 samples) - ratio=0.4
    Stage 8 (sticky): 4.3906 (23 samples) - ratio=0.4
    Stage 9 (sticky): 3.5809 (7 samples) - ratio=0.6
    Stage 10 (sticky): 2.7463 (23 samples) - ratio=0.7
    Stage 11 (sticky): 3.0571 (16 samples) - ratio=0.8
    Stage 12 (sticky): 3.1957 (7 samples) - ratio=0.8
    Stage 13 (sticky): 3.1999 (23 samples) - ratio=0.9
--- Validation complete ---
step 4600: train loss 3.2936, val loss 3.3594, lr 0.000184
Stage 0 (sticky): target_ratio=0.4, p1=0.1, p2=0.3, stale_count=0
  val model vs random: 33.35x better
  val avg correct prob: 0.4168 (random: 0.0125)
  val signal to noise: 19.81 (median: 1.40)
  Most likely guess correct P %: 54.4%
  Stage 0: New best val loss 3.3594, reset stale count to 0

saving checkpoint to out/ckpt_unmasking_4600.pt
iter 4600: loss 3.3774, time 5184.57ms, mfu 3.58%
  data: 220.3ms, grad_accum: 909.2ms (fw: 12.8ms, bw: 170.3ms)
  grad_proc: 15.9ms, optimizer: 1.7ms, param_check: 18.4ms
  loss_proc: 0.0ms, instability: 70.2ms
  cleanup: 0.3ms, gpu_sync: 0.4ms
  measured: 945.7ms, unaccounted: 4238.8ms (81.8%)
  validation: 5719.00ms (data: 20.10ms, forward: 0.00ms, loss: 61.92ms)
Masking: stage=0 (sticky), actual_ratio=0.485, target=0.4, p1=0.1, p2=0.3
iter 4620: loss 3.3939, time 934.53ms, mfu 3.63%
  data: 218.8ms, grad_accum: 902.9ms (fw: 12.2ms, bw: 169.6ms)
  grad_proc: 16.0ms, optimizer: 1.3ms, param_check: 18.7ms
  loss_proc: 0.0ms, instability: 70.6ms
  cleanup: 0.3ms, gpu_sync: 0.4ms
  measured: 939.6ms, unaccounted: -5.1ms (-0.5%)
iter 4640: loss 3.3201, time 941.89ms, mfu 3.68%
  data: 216.8ms, grad_accum: 894.7ms (fw: 12.2ms, bw: 168.3ms)
  grad_proc: 18.3ms, optimizer: 1.5ms, param_check: 19.1ms
  loss_proc: 0.0ms, instability: 70.8ms
  cleanup: 0.2ms, gpu_sync: 0.3ms
  measured: 934.0ms, unaccounted: 7.9ms (0.8%)
iter 4660: loss 3.5155, time 936.49ms, mfu 3.73%
  data: 217.1ms, grad_accum: 899.6ms (fw: 12.3ms, bw: 167.8ms)
  grad_proc: 17.2ms, optimizer: 1.3ms, param_check: 18.9ms
  loss_proc: 0.0ms, instability: 70.8ms
  cleanup: 0.2ms, gpu_sync: 0.3ms
  measured: 937.6ms, unaccounted: -1.1ms (-0.1%)
iter 4680: loss 3.2797, time 841.42ms, mfu 3.81%
  data: 221.0ms, grad_accum: 902.1ms (fw: 12.7ms, bw: 170.0ms)
  grad_proc: 17.9ms, optimizer: 1.4ms, param_check: 20.0ms
  loss_proc: 0.0ms, instability: 60.7ms
  cleanup: 0.1ms, gpu_sync: 0.3ms
  measured: 941.8ms, unaccounted: -100.3ms (-11.9%)
iter 4700: loss 3.2693, time 961.21ms, mfu 3.84%
  data: 218.8ms, grad_accum: 907.0ms (fw: 12.5ms, bw: 169.4ms)
  grad_proc: 16.9ms, optimizer: 1.3ms, param_check: 19.5ms
  loss_proc: 0.0ms, instability: 72.9ms
  cleanup: 0.1ms, gpu_sync: 0.3ms
  measured: 945.1ms, unaccounted: 16.1ms (1.7%)
iter 4720: loss 3.4452, time 917.33ms, mfu 3.87%
  data: 221.1ms, grad_accum: 907.1ms (fw: 12.6ms, bw: 171.0ms)
  grad_proc: 15.6ms, optimizer: 1.2ms, param_check: 18.2ms
  loss_proc: 0.0ms, instability: 69.1ms
  cleanup: 0.5ms, gpu_sync: 0.3ms
  measured: 942.9ms, unaccounted: -25.6ms (-2.8%)
iter 4740: loss 3.3035, time 939.57ms, mfu 3.90%
  data: 218.9ms, grad_accum: 905.9ms (fw: 12.5ms, bw: 170.2ms)
  grad_proc: 16.8ms, optimizer: 1.7ms, param_check: 19.3ms
  loss_proc: 0.0ms, instability: 70.2ms
  cleanup: 0.5ms, gpu_sync: 0.3ms
  measured: 944.6ms, unaccounted: -5.0ms (-0.5%)
iter 4760: loss 3.2442, time 942.12ms, mfu 3.92%
  data: 218.1ms, grad_accum: 904.0ms (fw: 12.7ms, bw: 167.6ms)
  grad_proc: 16.9ms, optimizer: 1.2ms, param_check: 18.7ms
  loss_proc: 0.0ms, instability: 71.2ms
  cleanup: 0.5ms, gpu_sync: 0.2ms
  measured: 941.5ms, unaccounted: 0.6ms (0.1%)
iter 4780: loss 3.3064, time 1009.74ms, mfu 3.91%
  data: 251.4ms, grad_accum: 951.9ms (fw: 20.9ms, bw: 171.1ms)
  grad_proc: 20.9ms, optimizer: 2.4ms, param_check: 27.6ms
  loss_proc: 0.0ms, instability: 63.2ms
  cleanup: 0.6ms, gpu_sync: 0.2ms
  measured: 1003.6ms, unaccounted: 6.1ms (0.6%)

--- Starting validation at iteration 4800 ---
Using validation set with samples from all 14 stages
  Validation complete: 20 batches processed (320 samples), avg loss = 3.3584
  Per-stage validation losses:
    Stage 0 (sticky): 3.0424 (23 samples) - ratio=0.4
    Stage 1 (sticky): 3.1996 (16 samples) - ratio=0.6
    Stage 2 (random): 3.5804 (23 samples) - max_ratio=0.5
    Stage 3 (sticky): 3.0436 (7 samples) - ratio=0.6
    Stage 4 (sticky): 2.8151 (16 samples) - ratio=0.6
    Stage 5 (random): 3.0115 (23 samples) - max_ratio=0.2
    Stage 6 (sticky): 3.0934 (7 samples) - ratio=0.2
    Stage 7 (sticky): 5.3232 (16 samples) - ratio=0.4
    Stage 8 (sticky): 4.3519 (23 samples) - ratio=0.4
    Stage 9 (sticky): 3.5962 (7 samples) - ratio=0.6
    Stage 10 (sticky): 2.7400 (23 samples) - ratio=0.7
    Stage 11 (sticky): 3.0688 (16 samples) - ratio=0.8
    Stage 12 (sticky): 3.1963 (7 samples) - ratio=0.8
    Stage 13 (sticky): 3.1899 (23 samples) - ratio=0.9
--- Validation complete ---
step 4800: train loss 3.2557, val loss 3.3584, lr 0.000181
Stage 0 (sticky): target_ratio=0.4, p1=0.1, p2=0.3, stale_count=0
  val model vs random: 33.57x better
  val avg correct prob: 0.4196 (random: 0.0125)
  val signal to noise: 19.96 (median: 1.43)
  Most likely guess correct P %: 54.6%
  Stage 0: New best val loss 3.3584, reset stale count to 0

saving checkpoint to out/ckpt_unmasking_4800.pt
iter 4800: loss 3.3458, time 4869.62ms, mfu 3.60%
  data: 247.0ms, grad_accum: 985.0ms (fw: 12.1ms, bw: 187.9ms)
  grad_proc: 17.5ms, optimizer: 1.7ms, param_check: 19.8ms
  loss_proc: 0.0ms, instability: 71.8ms
  cleanup: 0.3ms, gpu_sync: 0.1ms
  measured: 1024.4ms, unaccounted: 3845.2ms (79.0%)
  validation: 5638.30ms (data: 20.13ms, forward: 0.00ms, loss: 63.95ms)
Masking: stage=0 (sticky), actual_ratio=0.495, target=0.4, p1=0.1, p2=0.3
iter 4820: loss 3.4283, time 939.88ms, mfu 3.65%
  data: 217.0ms, grad_accum: 905.5ms (fw: 12.2ms, bw: 168.1ms)
  grad_proc: 17.8ms, optimizer: 1.5ms, param_check: 19.1ms
  loss_proc: 0.0ms, instability: 70.8ms
  cleanup: 0.1ms, gpu_sync: 0.1ms
  measured: 944.1ms, unaccounted: -4.2ms (-0.4%)
iter 4840: loss 3.2837, time 969.23ms, mfu 3.69%
  data: 220.6ms, grad_accum: 911.0ms (fw: 12.5ms, bw: 170.6ms)
  grad_proc: 18.0ms, optimizer: 1.6ms, param_check: 19.2ms
  loss_proc: 0.0ms, instability: 72.6ms
  cleanup: 0.1ms, gpu_sync: 0.2ms
  measured: 950.1ms, unaccounted: 19.1ms (2.0%)
iter 4860: loss 3.2480, time 928.56ms, mfu 3.74%
  data: 219.2ms, grad_accum: 902.4ms (fw: 12.1ms, bw: 170.4ms)
  grad_proc: 16.8ms, optimizer: 1.5ms, param_check: 19.2ms
  loss_proc: 0.0ms, instability: 70.5ms
  cleanup: 0.1ms, gpu_sync: 0.3ms
  measured: 940.2ms, unaccounted: -11.7ms (-1.3%)
iter 4880: loss 3.2669, time 931.93ms, mfu 3.78%
  data: 218.4ms, grad_accum: 901.4ms (fw: 12.5ms, bw: 167.7ms)
  grad_proc: 16.1ms, optimizer: 1.5ms, param_check: 19.1ms
  loss_proc: 0.0ms, instability: 70.6ms
  cleanup: 0.2ms, gpu_sync: 0.3ms
  measured: 938.5ms, unaccounted: -6.6ms (-0.7%)
iter 4900: loss 3.3506, time 963.24ms, mfu 3.80%
  data: 217.2ms, grad_accum: 899.4ms (fw: 12.6ms, bw: 168.6ms)
  grad_proc: 17.4ms, optimizer: 1.2ms, param_check: 19.6ms
  loss_proc: 0.0ms, instability: 70.7ms
  cleanup: 0.3ms, gpu_sync: 0.4ms
  measured: 938.2ms, unaccounted: 25.0ms (2.6%)
iter 4920: loss 3.3882, time 976.53ms, mfu 3.82%
  data: 218.6ms, grad_accum: 903.9ms (fw: 13.2ms, bw: 169.5ms)
  grad_proc: 15.3ms, optimizer: 1.5ms, param_check: 18.1ms
  loss_proc: 0.0ms, instability: 73.8ms
  cleanup: 0.1ms, gpu_sync: 0.3ms
  measured: 939.3ms, unaccounted: 37.3ms (3.8%)
iter 4940: loss 3.3424, time 944.88ms, mfu 3.85%
  data: 217.6ms, grad_accum: 899.4ms (fw: 12.5ms, bw: 170.1ms)
  grad_proc: 17.2ms, optimizer: 1.6ms, param_check: 18.6ms
  loss_proc: 0.0ms, instability: 70.7ms
  cleanup: 0.2ms, gpu_sync: 0.3ms
  measured: 937.3ms, unaccounted: 7.6ms (0.8%)
iter 4960: loss 3.3746, time 976.33ms, mfu 3.86%
  data: 218.5ms, grad_accum: 906.9ms (fw: 12.7ms, bw: 169.1ms)
  grad_proc: 16.9ms, optimizer: 1.5ms, param_check: 19.6ms
  loss_proc: 0.0ms, instability: 73.9ms
  cleanup: 0.3ms, gpu_sync: 0.3ms
  measured: 945.6ms, unaccounted: 30.8ms (3.1%)
iter 4980: loss 3.3598, time 933.99ms, mfu 3.89%
  data: 223.3ms, grad_accum: 904.4ms (fw: 12.6ms, bw: 171.0ms)
  grad_proc: 17.3ms, optimizer: 1.5ms, param_check: 18.7ms
  loss_proc: 0.0ms, instability: 70.4ms
  cleanup: 0.5ms, gpu_sync: 0.3ms
  measured: 942.7ms, unaccounted: -8.7ms (-0.9%)

*** GRADIENT INSTABILITY at iter 4995 (no clipping) ***
NaN gradients: True, Inf gradients: True
Total gradient norm: nan
*** ATTEMPTING RECOVERY FROM CHECKPOINT ***

*** RELOADING FROM CHECKPOINT ***
Reloading from checkpoint: ckpt_unmasking_8000.pt
Training context restored: stage=0, entropy_ema=1.0000
Model and optimizer reloaded from iteration 7999
*** CHECKPOINT RELOAD COMPLETE ***


--- Starting validation at iteration 8000 ---
Using validation set with samples from all 14 stages
  Validation complete: 20 batches processed (320 samples), avg loss = 3.1665
  Per-stage validation losses:
    Stage 0 (sticky): 2.8724 (23 samples) - ratio=0.4
    Stage 1 (sticky): 3.0692 (16 samples) - ratio=0.6
    Stage 2 (random): 3.4055 (23 samples) - max_ratio=0.5
    Stage 3 (sticky): 2.8029 (7 samples) - ratio=0.6
    Stage 4 (sticky): 2.7064 (16 samples) - ratio=0.6
    Stage 5 (random): 2.9032 (23 samples) - max_ratio=0.2
    Stage 6 (sticky): 2.9837 (7 samples) - ratio=0.2
    Stage 7 (sticky): 4.9157 (16 samples) - ratio=0.4
    Stage 8 (sticky): 4.0445 (23 samples) - ratio=0.4
    Stage 9 (sticky): 3.2698 (7 samples) - ratio=0.6
    Stage 10 (sticky): 2.5310 (23 samples) - ratio=0.7
    Stage 11 (sticky): 2.8861 (16 samples) - ratio=0.8
    Stage 12 (sticky): 3.0549 (7 samples) - ratio=0.8
    Stage 13 (sticky): 3.0637 (23 samples) - ratio=0.9
--- Validation complete ---
step 8000: train loss 3.0918, val loss 3.1665, lr 0.000125
Stage 0 (sticky): target_ratio=0.4, p1=0.1, p2=0.3, stale_count=1
  val model vs random: 36.57x better
  val avg correct prob: 0.4571 (random: 0.0125)
  val signal to noise: 24.22 (median: 1.97)
  Most likely guess correct P %: 58.0%
  Stage 0: Val loss stale count 2/6

saving checkpoint to out/ckpt_unmasking_8000.pt
iter 8000: loss 3.1575, time 4544.93ms, mfu -100.00%
  data: 221.3ms, grad_accum: 911.6ms (fw: 12.5ms, bw: 172.2ms)
  grad_proc: 39.1ms, optimizer: 1.5ms, param_check: 18.3ms
  loss_proc: 0.0ms, instability: 72.7ms
  cleanup: 0.1ms, gpu_sync: 0.3ms
  measured: 970.9ms, unaccounted: 3574.0ms (78.6%)
  validation: 5551.80ms (data: 19.92ms, forward: 0.00ms, loss: 64.33ms)
Masking: stage=0 (sticky), actual_ratio=0.518, target=0.4, p1=0.1, p2=0.3
[1;34mwandb[0m: 
[1;34mwandb[0m: 🚀 View run [33mshkspr_char_diff_moderate_first_unmasking[0m at: [34mhttps://wandb.ai/adamskrodzki-/experiments_diffusion/runs/0hbwwka3[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb\run-20250903_080529-0hbwwka3\logs[0m
