Calculating target architecture based on schedule...

============================================================
       TARGET MODEL ARCHITECTURE (at end of schedule)
============================================================
  n_layer                | 6
  n_head                 | 12
  n_embd                 | 768
  n_hidden               | 1536
  block_size             | 1024
  vocab_size             | 50304
  dropout                | 0.0
  bias                   | False
  embedding_mode         | standard
  attn_lora_rank         | 0
  embedding_rank         | 0
  lora_alpha             | 0.0
============================================================


============================================================
           INITIAL MODEL ARCHITECTURE (at Iter 0)
============================================================
  n_layer                | 3
  n_head                 | 12
  n_embd                 | 768
  n_hidden               | 768
  block_size             | 1024
  vocab_size             | 50304
  dropout                | 0.0
  bias                   | False
  embedding_mode         | lora
  attn_lora_rank         | 48
  embedding_rank         | 48
  lora_alpha             | 1.0
============================================================


Detailed parameter count:
  total                  | Total:   52,149,504 | Trainable:    8,207,616
  token_embeddings       | Total:    2,451,456 | Trainable:    2,451,456
  position_embeddings    | Total:            0 | Trainable:            0
  attention_layers       | Total:    7,520,256 | Trainable:    2,211,840
  feed_forward_layers    | Total:    3,538,944 | Trainable:    3,538,944
  layer_norms            | Total:        4,608 | Trainable:        4,608
  final_layer_norm       | Total:          768 | Trainable:          768
  language_model_head    | Total:   38,633,472 | Trainable:            0
------------------------------------------------------------
eval every:100
step 0: train loss 10.9658, val loss 10.9694
--- Model Analysis ---
  MLP Rank Utilization (L0): 71.09% (546/768)
--- Model Analysis ---
  MLP Rank Utilization (L1): 71.09% (546/768)
--- Model Analysis ---
  MLP Rank Utilization (L2): 71.22% (547/768)
  Average Attention Entropy:  5.8975
----------------------
merge_lora_weights 10.969396591186523 6.7 1500
iter 0: loss 10.9636, lr 0.00005, time 9678.34ms, mfu -100.00%
iter 10: loss 9.6060, lr 0.00055, time 550.77ms, mfu 33.55%
iter 20: loss 9.4098, lr 0.00104, time 558.90ms, mfu 33.50%
iter 30: loss 9.2854, lr 0.00154, time 561.06ms, mfu 33.45%
iter 40: loss 9.0702, lr 0.00204, time 569.29ms, mfu 33.35%
iter 50: loss 9.0694, lr 0.00254, time 568.93ms, mfu 33.26%
iter 60: loss 8.8983, lr 0.00303, time 573.72ms, mfu 33.16%
iter 70: loss 8.6753, lr 0.00353, time 580.20ms, mfu 33.03%
iter 80: loss 8.6940, lr 0.00403, time 584.27ms, mfu 32.89%
iter 90: loss 8.5460, lr 0.00453, time 585.94ms, mfu 32.75%
step 100: train loss 8.5963, val loss 8.5800
--- Model Analysis ---
  MLP Rank Utilization (L0): 64.97% (499/768)
--- Model Analysis ---
  MLP Rank Utilization (L1): 66.41% (510/768)
--- Model Analysis ---
  MLP Rank Utilization (L2): 67.58% (519/768)
  Average Attention Entropy:  5.5201
----------------------
saving checkpoint to out
merge_lora_weights 8.580021858215332 6.7 1500
iter 100: loss 8.5007, lr 0.00502, time 5387.93ms, mfu 29.82%
iter 110: loss 8.4685, lr 0.00552, time 573.19ms, mfu 30.06%
iter 120: loss 8.3556, lr 0.00602, time 579.20ms, mfu 30.25%
iter 130: loss 8.3364, lr 0.00652, time 578.96ms, mfu 30.41%
iter 140: loss 8.2934, lr 0.00701, time 575.61ms, mfu 30.58%
iter 150: loss 8.3713, lr 0.00751, time 574.24ms, mfu 30.74%
iter 160: loss 8.2181, lr 0.00801, time 574.53ms, mfu 30.89%
iter 170: loss 8.1461, lr 0.00851, time 575.07ms, mfu 31.01%
iter 180: loss 8.0674, lr 0.00900, time 570.84ms, mfu 31.15%
iter 190: loss 8.1281, lr 0.00950, time 574.91ms, mfu 31.25%
step 200: train loss 7.9997, val loss 7.9887
--- Model Analysis ---
  MLP Rank Utilization (L0): 60.29% (463/768)
--- Model Analysis ---
  MLP Rank Utilization (L1): 50.65% (389/768)
--- Model Analysis ---
  MLP Rank Utilization (L2): 52.21% (401/768)
  Average Attention Entropy:  4.5526
----------------------
saving checkpoint to out
merge_lora_weights 7.988671779632568 6.7 1500
iter 200: loss 7.9108, lr 0.01000, time 5570.89ms, mfu 28.45%
iter 210: loss 7.8367, lr 0.01000, time 571.22ms, mfu 28.84%
iter 220: loss 7.7925, lr 0.01000, time 576.53ms, mfu 29.16%
iter 230: loss 7.7864, lr 0.01000, time 573.27ms, mfu 29.47%
iter 240: loss 7.8471, lr 0.01000, time 578.23ms, mfu 29.72%
iter 250: loss 7.7967, lr 0.01000, time 579.66ms, mfu 29.94%
iter 260: loss 7.6529, lr 0.01000, time 578.67ms, mfu 30.14%
iter 270: loss 7.6868, lr 0.01000, time 578.43ms, mfu 30.32%
iter 280: loss 7.7566, lr 0.01000, time 581.54ms, mfu 30.46%
iter 290: loss 7.5781, lr 0.01000, time 579.62ms, mfu 30.61%
step 300: train loss 7.6768, val loss 7.6876
--- Model Analysis ---
  MLP Rank Utilization (L0): 57.03% (438/768)
--- Model Analysis ---
  MLP Rank Utilization (L1): 44.92% (345/768)
--- Model Analysis ---
  MLP Rank Utilization (L2): 43.62% (335/768)
  Average Attention Entropy:  4.3323
----------------------
saving checkpoint to out
merge_lora_weights 7.687646389007568 6.7 1500
iter 300: loss 7.7173, lr 0.01000, time 5526.03ms, mfu 27.88%
iter 310: loss 7.5798, lr 0.01000, time 570.98ms, mfu 28.33%
iter 320: loss 7.7185, lr 0.01000, time 575.17ms, mfu 28.71%
iter 330: loss 7.5625, lr 0.01000, time 577.94ms, mfu 29.04%
iter 340: loss 7.5769, lr 0.01000, time 578.46ms, mfu 29.33%
iter 350: loss 7.4559, lr 0.01000, time 580.18ms, mfu 29.58%
iter 360: loss 7.5038, lr 0.01000, time 578.18ms, mfu 29.82%
iter 370: loss 7.4633, lr 0.01000, time 579.29ms, mfu 30.03%
iter 380: loss 7.5305, lr 0.01000, time 581.73ms, mfu 30.20%
iter 390: loss 7.5352, lr 0.01000, time 581.17ms, mfu 30.36%
step 400: train loss 7.4672, val loss 7.4099
--- Model Analysis ---
  MLP Rank Utilization (L0): 54.30% (417/768)
--- Model Analysis ---
  MLP Rank Utilization (L1): 44.40% (341/768)
--- Model Analysis ---
  MLP Rank Utilization (L2): 42.58% (327/768)
  Average Attention Entropy:  4.2726
----------------------
saving checkpoint to out
merge_lora_weights 7.409903049468994 6.7 1500
iter 400: loss 7.5068, lr 0.01000, time 5521.91ms, mfu 27.66%
iter 410: loss 7.6234, lr 0.01000, time 570.89ms, mfu 28.13%
iter 420: loss 7.5693, lr 0.01000, time 578.40ms, mfu 28.51%
iter 430: loss 7.3351, lr 0.01000, time 579.93ms, mfu 28.85%
iter 440: loss 7.5023, lr 0.01000, time 579.13ms, mfu 29.15%
iter 450: loss 7.2778, lr 0.01000, time 580.43ms, mfu 29.42%
iter 460: loss 7.4186, lr 0.01000, time 579.04ms, mfu 29.67%
iter 470: loss 7.3561, lr 0.01000, time 583.73ms, mfu 29.87%
iter 480: loss 7.4256, lr 0.01000, time 579.72ms, mfu 30.07%
iter 490: loss 7.3576, lr 0.01000, time 579.49ms, mfu 30.25%
step 500: train loss 7.3122, val loss 7.3262
--- Model Analysis ---
  MLP Rank Utilization (L0): 51.69% (397/768)
--- Model Analysis ---
  MLP Rank Utilization (L1): 45.18% (347/768)
--- Model Analysis ---
  MLP Rank Utilization (L2): 42.58% (327/768)
  Average Attention Entropy:  4.1653
----------------------
saving checkpoint to out
merge_lora_weights 7.326183319091797 6.7 1500
iter 500: loss 7.4007, lr 0.01000, time 5232.28ms, mfu 27.58%
iter 510: loss 7.2895, lr 0.01000, time 573.00ms, mfu 28.05%
iter 520: loss 7.2816, lr 0.01000, time 577.59ms, mfu 28.44%
iter 530: loss 7.3968, lr 0.01000, time 574.61ms, mfu 28.81%
iter 540: loss 7.5532, lr 0.01000, time 581.59ms, mfu 29.11%
iter 550: loss 7.2593, lr 0.01000, time 580.49ms, mfu 29.38%
iter 560: loss 7.1852, lr 0.01000, time 582.85ms, mfu 29.62%
iter 570: loss 7.1782, lr 0.01000, time 581.89ms, mfu 29.83%
iter 580: loss 7.2706, lr 0.01000, time 578.90ms, mfu 30.04%
iter 590: loss 7.2159, lr 0.01000, time 579.05ms, mfu 30.23%
step 600: train loss 7.2525, val loss 7.2559
--- Model Analysis ---
  MLP Rank Utilization (L0): 49.09% (377/768)
--- Model Analysis ---
  MLP Rank Utilization (L1): 45.83% (352/768)
--- Model Analysis ---
  MLP Rank Utilization (L2): 42.58% (327/768)
  Average Attention Entropy:  4.1250
----------------------
saving checkpoint to out
merge_lora_weights 7.255868434906006 6.7 1500
iter 600: loss 7.2074, lr 0.01000, time 5377.32ms, mfu 27.55%
iter 610: loss 7.1458, lr 0.01000, time 575.80ms, mfu 28.00%
iter 620: loss 7.3582, lr 0.01000, time 573.94ms, mfu 28.42%
iter 630: loss 7.2988, lr 0.01000, time 577.46ms, mfu 28.78%
iter 640: loss 7.3313, lr 0.01000, time 579.40ms, mfu 29.09%
iter 650: loss 7.3036, lr 0.01000, time 581.03ms, mfu 29.36%
iter 660: loss 7.1159, lr 0.01000, time 579.87ms, mfu 29.61%
iter 670: loss 7.1696, lr 0.01000, time 580.30ms, mfu 29.84%
iter 680: loss 7.3166, lr 0.01000, time 579.40ms, mfu 30.04%
iter 690: loss 7.1093, lr 0.01000, time 578.70ms, mfu 30.23%
step 700: train loss 7.1642, val loss 7.2361
--- Model Analysis ---
  MLP Rank Utilization (L0): 46.74% (359/768)
--- Model Analysis ---
  MLP Rank Utilization (L1): 46.74% (359/768)
--- Model Analysis ---
  MLP Rank Utilization (L2): 42.45% (326/768)
  Average Attention Entropy:  3.9963
----------------------
saving checkpoint to out
merge_lora_weights 7.23606014251709 6.7 1500
iter 700: loss 7.2350, lr 0.01000, time 5068.47ms, mfu 27.57%
iter 710: loss 7.2149, lr 0.01000, time 572.55ms, mfu 28.04%
iter 720: loss 7.0748, lr 0.01000, time 579.01ms, mfu 28.43%
iter 730: loss 7.0434, lr 0.01000, time 583.81ms, mfu 28.75%
iter 740: loss 7.2059, lr 0.01000, time 580.78ms, mfu 29.06%
iter 750: loss 7.1080, lr 0.01000, time 577.65ms, mfu 29.35%
iter 760: loss 7.0370, lr 0.01000, time 579.68ms, mfu 29.61%
iter 770: loss 7.1070, lr 0.01000, time 579.45ms, mfu 29.84%
iter 780: loss 7.1200, lr 0.01000, time 579.78ms, mfu 30.04%
iter 790: loss 7.1786, lr 0.01000, time 578.95ms, mfu 30.23%
step 800: train loss 7.1335, val loss 7.1748
--- Model Analysis ---
  MLP Rank Utilization (L0): 44.66% (343/768)
--- Model Analysis ---
  MLP Rank Utilization (L1): 47.40% (364/768)
--- Model Analysis ---
  MLP Rank Utilization (L2): 42.71% (328/768)
  Average Attention Entropy:  3.9434
----------------------
saving checkpoint to out
merge_lora_weights 7.174752712249756 6.7 1500
iter 800: loss 7.0511, lr 0.01000, time 4931.10ms, mfu 27.58%
iter 810: loss 7.1136, lr 0.01000, time 576.71ms, mfu 28.03%
iter 820: loss 7.2325, lr 0.01000, time 579.06ms, mfu 28.41%
iter 830: loss 7.0686, lr 0.01000, time 579.87ms, mfu 28.76%
iter 840: loss 7.2241, lr 0.01000, time 578.40ms, mfu 29.08%
iter 850: loss 7.0925, lr 0.01000, time 579.28ms, mfu 29.36%
iter 860: loss 7.1098, lr 0.01000, time 579.57ms, mfu 29.61%
iter 870: loss 7.1397, lr 0.01000, time 580.01ms, mfu 29.84%
iter 880: loss 7.2140, lr 0.01000, time 578.59ms, mfu 30.05%
iter 890: loss 6.9616, lr 0.01000, time 580.61ms, mfu 30.23%
step 900: train loss 7.0726, val loss 7.0933
--- Model Analysis ---
  MLP Rank Utilization (L0): 43.23% (332/768)
--- Model Analysis ---
  MLP Rank Utilization (L1): 48.05% (369/768)
--- Model Analysis ---
  MLP Rank Utilization (L2): 43.23% (332/768)
  Average Attention Entropy:  3.8281
----------------------
saving checkpoint to out
merge_lora_weights 7.0933332443237305 6.7 1500
iter 900: loss 7.0997, lr 0.01000, time 5454.71ms, mfu 27.54%
iter 910: loss 6.9848, lr 0.01000, time 569.80ms, mfu 28.03%
iter 920: loss 6.7166, lr 0.01000, time 579.63ms, mfu 28.42%
iter 930: loss 6.9575, lr 0.01000, time 583.08ms, mfu 28.75%
iter 940: loss 7.1867, lr 0.01000, time 579.48ms, mfu 29.06%
iter 950: loss 7.1579, lr 0.01000, time 580.01ms, mfu 29.34%
iter 960: loss 6.9296, lr 0.01000, time 580.22ms, mfu 29.59%
iter 970: loss 6.9871, lr 0.01000, time 578.43ms, mfu 29.83%
iter 980: loss 7.2454, lr 0.01000, time 579.66ms, mfu 30.03%
iter 990: loss 6.9643, lr 0.01000, time 581.73ms, mfu 30.21%
step 1000: train loss 7.0519, val loss 7.0322
--- Model Analysis ---
  MLP Rank Utilization (L0): 42.06% (323/768)
--- Model Analysis ---
  MLP Rank Utilization (L1): 48.70% (374/768)
--- Model Analysis ---
  MLP Rank Utilization (L2): 43.75% (336/768)
  Average Attention Entropy:  3.7974
----------------------
saving checkpoint to out
merge_lora_weights 7.032151699066162 6.7 1500
iter 1000: loss 7.1024, lr 0.01000, time 5686.56ms, mfu 27.51%
iter 1010: loss 7.0160, lr 0.01000, time 568.20ms, mfu 28.01%
iter 1020: loss 7.1033, lr 0.01000, time 578.26ms, mfu 28.41%
iter 1030: loss 7.1181, lr 0.01000, time 579.76ms, mfu 28.75%
iter 1040: loss 7.0208, lr 0.01000, time 578.91ms, mfu 29.07%
iter 1050: loss 7.1309, lr 0.01000, time 580.51ms, mfu 29.35%
iter 1060: loss 6.9582, lr 0.01000, time 579.78ms, mfu 29.60%
iter 1070: loss 7.0515, lr 0.01000, time 580.54ms, mfu 29.82%
iter 1080: loss 6.9387, lr 0.01000, time 579.79ms, mfu 30.03%
iter 1090: loss 7.0217, lr 0.01000, time 579.04ms, mfu 30.22%
step 1100: train loss 7.0193, val loss 7.0103
--- Model Analysis ---
  MLP Rank Utilization (L0): 41.41% (318/768)
--- Model Analysis ---
  MLP Rank Utilization (L1): 49.22% (378/768)
--- Model Analysis ---
  MLP Rank Utilization (L2): 44.14% (339/768)
  Average Attention Entropy:  3.7630
----------------------
saving checkpoint to out
merge_lora_weights 7.01025915145874 6.7 1500
iter 1100: loss 6.9880, lr 0.01000, time 5528.65ms, mfu 27.53%
iter 1110: loss 6.9342, lr 0.01000, time 574.46ms, mfu 27.99%
iter 1120: loss 7.0103, lr 0.01000, time 579.42ms, mfu 28.38%
iter 1130: loss 6.9102, lr 0.01000, time 582.69ms, mfu 28.72%
iter 1140: loss 7.0053, lr 0.01000, time 576.39ms, mfu 29.05%
iter 1150: loss 6.9238, lr 0.01000, time 578.77ms, mfu 29.34%
iter 1160: loss 7.0117, lr 0.01000, time 579.92ms, mfu 29.59%
iter 1170: loss 6.8116, lr 0.01000, time 584.65ms, mfu 29.79%
iter 1180: loss 6.9847, lr 0.01000, time 580.37ms, mfu 30.00%
iter 1190: loss 6.9982, lr 0.01000, time 578.03ms, mfu 30.20%
Traceback (most recent call last):
  File "/teamspace/studios/this_studio/nanoGPT/train.py", line 836, in <module>
    break # Exit the while loop after a blocking re-evaluation
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py", line 655, in _fn
    return fn(*args, **kwargs)
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
  File "/teamspace/studios/this_studio/nanoGPT/model.py", line 444, in forward
    def forward(self, idx, targets=None, return_attention=False):
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py", line 838, in _fn
    return fn(*args, **kwargs)
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py", line 1201, in forward
    return compiled_fn(full_args)
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 315, in runtime_wrapper
    all_outs = call_func_at_runtime_with_args(
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/_functorch/_aot_autograd/utils.py", line 126, in call_func_at_runtime_with_args
    out = normalize_as_list(f(args))
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/_functorch/_aot_autograd/utils.py", line 100, in g
    return f(*args)
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/autograd/function.py", line 575, in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 1937, in forward
    fw_outs = call_func_at_runtime_with_args(
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/_functorch/_aot_autograd/utils.py", line 126, in call_func_at_runtime_with_args
    out = normalize_as_list(f(args))
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 495, in wrapper
    return compiled_fn(runtime_args)
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/_inductor/output_code.py", line 460, in __call__
    return self.current_callable(inputs)
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/_inductor/utils.py", line 2404, in run
    return model(new_inputs)
  File "/tmp/torchinductor_adamskrodzki/67/c67uk6apmrm3tv6q7nbr45p7hci5bo3ntleaz3i3qu2gxksksoqt.py", line 1703, in call
    triton_poi_fused__to_copy_t_15.run(primals_2, buf105, 38633472, stream=stream0)
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/_inductor/runtime/triton_heuristics.py", line 956, in run
    return launcher(
  File "<string>", line 5, in launcher
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/triton/backends/nvidia/driver.py", line 529, in __call__
    self.launch(gridX, gridY, gridZ, stream, function, self.launch_cooperative_grid, global_scratch, *args)
KeyboardInterrupt
Traceback (most recent call last):
  File "/teamspace/studios/this_studio/nanoGPT/train.py", line 836, in <module>
    break # Exit the while loop after a blocking re-evaluation
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py", line 655, in _fn
    return fn(*args, **kwargs)
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
  File "/teamspace/studios/this_studio/nanoGPT/model.py", line 444, in forward
    def forward(self, idx, targets=None, return_attention=False):
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py", line 838, in _fn
    return fn(*args, **kwargs)
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py", line 1201, in forward
    return compiled_fn(full_args)
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 315, in runtime_wrapper
    all_outs = call_func_at_runtime_with_args(
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/_functorch/_aot_autograd/utils.py", line 126, in call_func_at_runtime_with_args
    out = normalize_as_list(f(args))
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/_functorch/_aot_autograd/utils.py", line 100, in g
    return f(*args)
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/autograd/function.py", line 575, in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 1937, in forward
    fw_outs = call_func_at_runtime_with_args(
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/_functorch/_aot_autograd/utils.py", line 126, in call_func_at_runtime_with_args
    out = normalize_as_list(f(args))
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 495, in wrapper
    return compiled_fn(runtime_args)
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/_inductor/output_code.py", line 460, in __call__
    return self.current_callable(inputs)
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/_inductor/utils.py", line 2404, in run
    return model(new_inputs)
  File "/tmp/torchinductor_adamskrodzki/67/c67uk6apmrm3tv6q7nbr45p7hci5bo3ntleaz3i3qu2gxksksoqt.py", line 1703, in call
    triton_poi_fused__to_copy_t_15.run(primals_2, buf105, 38633472, stream=stream0)
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/_inductor/runtime/triton_heuristics.py", line 956, in run
    return launcher(
  File "<string>", line 5, in launcher
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/triton/backends/nvidia/driver.py", line 529, in __call__
    self.launch(gridX, gridY, gridZ, stream, function, self.launch_cooperative_grid, global_scratch, *args)
KeyboardInterrupt
Exception ignored in atexit callback: <function _start_and_connect_service.<locals>.teardown_atexit at 0x7fa8707d57e0>
Traceback (most recent call last):
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/wandb/sdk/lib/service/service_connection.py", line 54, in teardown_atexit
    conn.teardown(hooks.exit_code)
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/wandb/sdk/lib/service/service_connection.py", line 182, in teardown
    self._router.join()
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/wandb/sdk/interface/router.py", line 75, in join
    self._thread.join()
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/threading.py", line 1096, in join
    self._wait_for_tstate_lock()
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/threading.py", line 1116, in _wait_for_tstate_lock
    if lock.acquire(block, timeout):
KeyboardInterrupt:
Exception ignored in atexit callback: <function shutdown_compile_workers at 0x7fa88fb792d0>
Traceback (most recent call last):
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/_inductor/async_compile.py", line 113, in shutdown_compile_workers
    pool.shutdown()
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/_inductor/compile_worker/subproc_pool.py", line 239, in shutdown
    self.process.wait(300)
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/subprocess.py", line 1209, in wait
    return self._wait(timeout=timeout)
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/subprocess.py", line 1937, in _wait
    time.sleep(delay)
KeyboardInterrupt:
