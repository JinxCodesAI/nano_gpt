Calculating target architecture based on schedule...

============================================================
       TARGET MODEL ARCHITECTURE (at end of schedule)
============================================================
  n_layer                | 24
  n_head                 | 12
  n_embd                 | 768
  n_hidden               | 3072
  block_size             | 1024
  vocab_size             | 50304
  dropout                | 0.0
  bias                   | False
  embedding_mode         | lora
  attn_lora_rank         | 48
  embedding_rank         | 48
  lora_alpha             | 1.0
============================================================


============================================================
           INITIAL MODEL ARCHITECTURE (at Iter 0)
============================================================
  n_layer                | 3
  n_head                 | 12
  n_embd                 | 768
  n_hidden               | 768
  block_size             | 1024
  vocab_size             | 50304
  dropout                | 0.0
  bias                   | False
  embedding_mode         | lora
  attn_lora_rank         | 48
  embedding_rank         | 48
  lora_alpha             | 1.0
============================================================


Detailed parameter count:
  total                  | Total:   52,149,504 | Trainable:    8,207,616
  token_embeddings       | Total:    2,451,456 | Trainable:    2,451,456
  position_embeddings    | Total:            0 | Trainable:            0
  attention_layers       | Total:    7,520,256 | Trainable:    2,211,840
  feed_forward_layers    | Total:    3,538,944 | Trainable:    3,538,944
  layer_norms            | Total:        4,608 | Trainable:        4,608
  final_layer_norm       | Total:          768 | Trainable:          768
  language_model_head    | Total:   38,633,472 | Trainable:            0
------------------------------------------------------------
eval every:100
step 0: train loss 10.9660, val loss 10.9694
merge_lora_weights 10.969396591186523 8.0 500
iter 0: loss 10.9664, lr 0.00005, time 7016.91ms, mfu -100.00%
iter 10: loss 9.5656, lr 0.00055, time 570.51ms, mfu 32.39%
iter 20: loss 9.4593, lr 0.00104, time 559.87ms, mfu 32.45%
iter 30: loss 9.2490, lr 0.00154, time 563.58ms, mfu 32.49%
iter 40: loss 9.1633, lr 0.00204, time 565.60ms, mfu 32.51%
iter 50: loss 9.0045, lr 0.00254, time 576.03ms, mfu 32.46%
iter 60: loss 8.9382, lr 0.00303, time 579.03ms, mfu 32.41%
iter 70: loss 8.7452, lr 0.00353, time 580.02ms, mfu 32.35%
iter 80: loss 8.7993, lr 0.00403, time 581.75ms, mfu 32.30%
iter 90: loss 8.5138, lr 0.00453, time 568.88ms, mfu 32.31%
step 100: train loss 8.5569, val loss 8.5812
saving checkpoint to out
merge_lora_weights 8.58122730255127 8.0 500
iter 100: loss 8.5882, lr 0.00502, time 4659.53ms, mfu 29.48%
iter 110: loss 8.4352, lr 0.00552, time 583.05ms, mfu 29.70%
iter 120: loss 8.5060, lr 0.00602, time 583.87ms, mfu 29.90%
iter 130: loss 8.3489, lr 0.00652, time 580.00ms, mfu 30.09%
iter 140: loss 8.2798, lr 0.00701, time 580.66ms, mfu 30.27%
iter 150: loss 8.3444, lr 0.00751, time 579.76ms, mfu 30.43%
iter 160: loss 8.1718, lr 0.00801, time 576.84ms, mfu 30.59%
iter 170: loss 8.0470, lr 0.00851, time 578.50ms, mfu 30.72%
iter 180: loss 8.2153, lr 0.00900, time 575.38ms, mfu 30.86%
iter 190: loss 7.9762, lr 0.00950, time 578.66ms, mfu 30.97%
step 200: train loss 7.9600, val loss 7.9969
saving checkpoint to out

=== SCALING OPERATION TRIGGERED (DDP SYNC) ===
Operation: merge_lora_weights
Trigger reason: Loss threshold
Current val loss: 7.9969, Trigger loss: 8.0000
Iterations since last op: 200, Max wait: 500
Executing operation: merge_lora_weights first burn with value: None
Performing architectural operation: merge_lora_weights
Merging LoRA weights into main weights...
LoRA weights merged and reset.

Detailed parameter count:
  total                  | Total:   52,149,504 | Trainable:    8,207,616
  token_embeddings       | Total:    2,451,456 | Trainable:    2,451,456
  position_embeddings    | Total:            0 | Trainable:            0
  attention_layers       | Total:    7,520,256 | Trainable:    2,211,840
  feed_forward_layers    | Total:    3,538,944 | Trainable:    3,538,944
  layer_norms            | Total:        4,608 | Trainable:        4,608
  final_layer_norm       | Total:          768 | Trainable:          768
  language_model_head    | Total:   38,633,472 | Trainable:            0
------------------------------------------------------------
Re-configuring optimizer after architectural change...
num decayed parameter tensors: 17, with 8,202,240 parameters
num non-decayed parameter tensors: 7, with 5,376 parameters
using fused AdamW: True
Transferring optimizer state for existing parameters...
Transferred optimizer state for 0 / 28 parameters
Re-compiling the model...

============================================================
             ARCHITECTURE CHANGE (at Iter 200)
============================================================
  n_layer                | 3
  n_head                 | 12
  n_embd                 | 768
  n_hidden               | 768
  block_size             | 1024
  vocab_size             | 50304
  dropout                | 0.0
  bias                   | False
  embedding_mode         | lora
  attn_lora_rank         | 48
  embedding_rank         | 48
  lora_alpha             | 1.0
============================================================

Architectural operation completed successfully.
=== SCALING OPERATION COMPLETE ===


=== SCALING OPERATION TRIGGERED (DDP SYNC) ===
Operation: change_lr
Trigger reason: Loss threshold
Current val loss: 7.9969, Trigger loss: 100.0000
Iterations since last op: 0, Max wait: 1
Executing operation: change_lr  with value: 0.7
LR multiplier: 10.0000 -> 7.0000
=== SCALING OPERATION COMPLETE ===


=== SCALING OPERATION TRIGGERED (DDP SYNC) ===
Operation: reset_lr_schedule
Trigger reason: Loss threshold
Current val loss: 7.9969, Trigger loss: 100.0000
Iterations since last op: 0, Max wait: 1
Executing operation: reset_lr_schedule  with value: None
LR schedule offset: 0 -> 200
=== SCALING OPERATION COMPLETE ===

merge_lora_weights 7.996891021728516 7.4 300
iter 200: loss 11.5601, lr 0.01000, time 4688.89ms, mfu 28.27%
iter 210: loss 8.5242, lr 0.00038, time 573.72ms, mfu 28.66%
iter 220: loss 8.0354, lr 0.00073, time 576.93ms, mfu 29.00%
iter 230: loss 8.0114, lr 0.00108, time 580.08ms, mfu 29.29%
iter 240: loss 7.9580, lr 0.00143, time 580.25ms, mfu 29.54%
iter 250: loss 7.9177, lr 0.00178, time 580.00ms, mfu 29.77%
iter 260: loss 7.8111, lr 0.00212, time 579.33ms, mfu 29.99%
iter 270: loss 7.8518, lr 0.00247, time 578.74ms, mfu 30.18%
iter 280: loss 7.8920, lr 0.00282, time 580.19ms, mfu 30.35%
iter 290: loss 7.8634, lr 0.00317, time 582.23ms, mfu 30.49%
step 300: train loss 7.7868, val loss 7.7859
saving checkpoint to out
merge_lora_weights 7.785858154296875 7.4 300
iter 300: loss 7.7850, lr 0.00352, time 4976.17ms, mfu 27.81%
iter 310: loss 7.7115, lr 0.00387, time 577.66ms, mfu 28.23%
iter 320: loss 7.7234, lr 0.00421, time 579.66ms, mfu 28.59%
iter 330: loss 7.6905, lr 0.00456, time 580.77ms, mfu 28.92%
iter 340: loss 7.7067, lr 0.00491, time 579.33ms, mfu 29.21%
iter 350: loss 7.8105, lr 0.00526, time 582.41ms, mfu 29.47%
iter 360: loss 7.8197, lr 0.00561, time 577.28ms, mfu 29.72%
iter 370: loss 7.6992, lr 0.00596, time 578.80ms, mfu 29.94%
iter 380: loss 7.6440, lr 0.00630, time 580.48ms, mfu 30.13%
iter 390: loss 7.5052, lr 0.00665, time 577.92ms, mfu 30.32%
step 400: train loss 7.6348, val loss 7.5478
saving checkpoint to out
merge_lora_weights 7.54775857925415 7.4 300
iter 400: loss 7.5067, lr 0.00700, time 4550.42ms, mfu 27.69%
iter 410: loss 7.6226, lr 0.00700, time 574.53ms, mfu 28.14%
iter 420: loss 7.5402, lr 0.00700, time 581.07ms, mfu 28.50%
iter 430: loss 7.6129, lr 0.00700, time 581.86ms, mfu 28.83%
iter 440: loss 7.6326, lr 0.00700, time 581.11ms, mfu 29.13%
iter 450: loss 7.5632, lr 0.00700, time 580.43ms, mfu 29.40%
iter 460: loss 7.4860, lr 0.00700, time 579.74ms, mfu 29.65%
iter 470: loss 7.5601, lr 0.00700, time 580.64ms, mfu 29.86%
iter 480: loss 7.5184, lr 0.00700, time 580.07ms, mfu 30.06%
iter 490: loss 7.3335, lr 0.00700, time 582.01ms, mfu 30.23%
step 500: train loss 7.3601, val loss 7.4033
saving checkpoint to out

=== SCALING OPERATION TRIGGERED (DDP SYNC) ===
Operation: merge_lora_weights
Trigger reason: Timeout
Current val loss: 7.4033, Trigger loss: 7.4000
Iterations since last op: 300, Max wait: 300
Executing operation: merge_lora_weights second burn with value: None
Performing architectural operation: merge_lora_weights
Merging LoRA weights into main weights...
LoRA weights merged and reset.

Detailed parameter count:
  total                  | Total:   52,149,504 | Trainable:    8,207,616
  token_embeddings       | Total:    2,451,456 | Trainable:    2,451,456
  position_embeddings    | Total:            0 | Trainable:            0
  attention_layers       | Total:    7,520,256 | Trainable:    2,211,840
  feed_forward_layers    | Total:    3,538,944 | Trainable:    3,538,944
  layer_norms            | Total:        4,608 | Trainable:        4,608
  final_layer_norm       | Total:          768 | Trainable:          768
  language_model_head    | Total:   38,633,472 | Trainable:            0
------------------------------------------------------------
Re-configuring optimizer after architectural change...
num decayed parameter tensors: 17, with 8,202,240 parameters
num non-decayed parameter tensors: 7, with 5,376 parameters
using fused AdamW: True
Transferring optimizer state for existing parameters...
Transferred optimizer state for 0 / 28 parameters
Re-compiling the model...

============================================================
             ARCHITECTURE CHANGE (at Iter 500)
============================================================
  n_layer                | 3
  n_head                 | 12
  n_embd                 | 768
  n_hidden               | 768
  block_size             | 1024
  vocab_size             | 50304
  dropout                | 0.0
  bias                   | False
  embedding_mode         | lora
  attn_lora_rank         | 48
  embedding_rank         | 48
  lora_alpha             | 1.0
============================================================

Architectural operation completed successfully.
=== SCALING OPERATION COMPLETE ===


=== SCALING OPERATION TRIGGERED (DDP SYNC) ===
Operation: change_lr
Trigger reason: Loss threshold
Current val loss: 7.4033, Trigger loss: 100.0000
Iterations since last op: 0, Max wait: 1
Executing operation: change_lr  with value: 0.2
LR multiplier: 7.0000 -> 1.4000
=== SCALING OPERATION COMPLETE ===


=== SCALING OPERATION TRIGGERED (DDP SYNC) ===
Operation: change_warmup_iters
Trigger reason: Loss threshold
Current val loss: 7.4033, Trigger loss: 100.0000
Iterations since last op: 0, Max wait: 1
Executing operation: change_warmup_iters  with value: 5
Warmup iters multiplier: 1.0000 -> 5.0000
=== SCALING OPERATION COMPLETE ===


=== SCALING OPERATION TRIGGERED (DDP SYNC) ===
Operation: reset_lr_schedule
Trigger reason: Loss threshold
Current val loss: 7.4033, Trigger loss: 100.0000
Iterations since last op: 0, Max wait: 1
Executing operation: reset_lr_schedule  with value: None
LR schedule offset: 200 -> 500
=== SCALING OPERATION COMPLETE ===


=== SCALING OPERATION TRIGGERED (DDP SYNC) ===
Operation: decrease_vocab_lora_scaling
Trigger reason: Loss threshold
Current val loss: 7.4033, Trigger loss: 100.0000
Iterations since last op: 0, Max wait: 1
Executing operation: decrease_vocab_lora_scaling pre resize with value: 2
Performing architectural operation: decrease_vocab_lora_scaling
Resizing embedding LoRA rank to 96.

Detailed parameter count:
  total                  | Total:   54,600,960 | Trainable:   10,659,072
  token_embeddings       | Total:    4,902,912 | Trainable:    4,902,912
  position_embeddings    | Total:            0 | Trainable:            0
  attention_layers       | Total:    7,520,256 | Trainable:    2,211,840
  feed_forward_layers    | Total:    3,538,944 | Trainable:    3,538,944
  layer_norms            | Total:        4,608 | Trainable:        4,608
  final_layer_norm       | Total:          768 | Trainable:          768
  language_model_head    | Total:   38,633,472 | Trainable:            0
------------------------------------------------------------
Re-configuring optimizer after architectural change...
num decayed parameter tensors: 17, with 10,653,696 parameters
num non-decayed parameter tensors: 7, with 5,376 parameters
using fused AdamW: True
Transferring optimizer state for existing parameters...
Transferred optimizer state for 0 / 28 parameters
Re-compiling the model...

============================================================
             ARCHITECTURE CHANGE (at Iter 500)
============================================================
  n_layer                | 3
  n_head                 | 12
  n_embd                 | 768
  n_hidden               | 768
  block_size             | 1024
  vocab_size             | 50304
  dropout                | 0.0
  bias                   | False
  embedding_mode         | lora
  attn_lora_rank         | 48
  embedding_rank         | 96
  lora_alpha             | 1.0
============================================================

Architectural operation completed successfully.
Re-evaluating validation loss after operation...
W0720 13:40:15.364000 61799 /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/_inductor/utils.py:1250] [0/2] Not enough SMs to use max_autotune_gemm mode
New val loss after operation: 7.6984
iter 500: loss 7.7935, lr 0.00700, time 24970.78ms, mfu 27.29%
iter 510: loss 7.6620, lr 0.00002, time 572.54ms, mfu 27.93%
iter 520: loss 7.3966, lr 0.00003, time 582.87ms, mfu 28.44%
iter 530: loss 7.2661, lr 0.00004, time 585.63ms, mfu 28.89%
iter 540: loss 7.4304, lr 0.00006, time 584.30ms, mfu 29.30%
iter 550: loss 7.3880, lr 0.00007, time 586.40ms, mfu 29.66%
iter 560: loss 7.2994, lr 0.00009, time 590.74ms, mfu 29.95%
iter 570: loss 7.3633, lr 0.00010, time 585.38ms, mfu 30.25%
iter 580: loss 7.3539, lr 0.00011, time 587.25ms, mfu 30.51%
iter 590: loss 7.3367, lr 0.00013, time 578.90ms, mfu 30.79%
step 600: train loss 7.2890, val loss 7.2925
saving checkpoint to out

=== SCALING OPERATION TRIGGERED (DDP SYNC) ===
Operation: decrease_attn_lora_scaling
Trigger reason: Loss threshold
Current val loss: 7.2925, Trigger loss: 100.0000
Iterations since last op: 100, Max wait: 1
Executing operation: decrease_attn_lora_scaling pre resize2 with value: 2
Performing architectural operation: decrease_attn_lora_scaling
Resizing attention LoRA rank to 96.

Detailed parameter count:
  total                  | Total:   55,043,328 | Trainable:   11,101,440
  token_embeddings       | Total:    4,902,912 | Trainable:    4,902,912
  position_embeddings    | Total:            0 | Trainable:            0
  attention_layers       | Total:    7,962,624 | Trainable:    2,654,208
  feed_forward_layers    | Total:    3,538,944 | Trainable:    3,538,944
  layer_norms            | Total:        4,608 | Trainable:        4,608
  final_layer_norm       | Total:          768 | Trainable:          768
  language_model_head    | Total:   38,633,472 | Trainable:            0
------------------------------------------------------------
Re-configuring optimizer after architectural change...
num decayed parameter tensors: 17, with 11,096,064 parameters
num non-decayed parameter tensors: 7, with 5,376 parameters
using fused AdamW: True
Transferring optimizer state for existing parameters...
Transferred optimizer state for 0 / 28 parameters
Re-compiling the model...

============================================================
             ARCHITECTURE CHANGE (at Iter 600)
============================================================
  n_layer                | 3
  n_head                 | 12
  n_embd                 | 768
  n_hidden               | 768
  block_size             | 1024
  vocab_size             | 50304
  dropout                | 0.0
  bias                   | False
  embedding_mode         | lora
  attn_lora_rank         | 96
  embedding_rank         | 96
  lora_alpha             | 1.0
============================================================

Architectural operation completed successfully.
Re-evaluating validation loss after operation...
New val loss after operation: 7.2832
iter 600: loss 7.4333, lr 0.00014, time 21911.44ms, mfu 27.80%
iter 610: loss 7.2389, lr 0.00016, time 567.33ms, mfu 28.44%
iter 620: loss 7.3027, lr 0.00017, time 580.71ms, mfu 28.94%
iter 630: loss 7.1777, lr 0.00018, time 588.99ms, mfu 29.34%
iter 640: loss 7.1828, lr 0.00020, time 589.90ms, mfu 29.70%
iter 650: loss 7.2443, lr 0.00021, time 580.55ms, mfu 30.08%
iter 660: loss 7.1156, lr 0.00023, time 587.86ms, mfu 30.37%
iter 670: loss 7.3768, lr 0.00024, time 587.67ms, mfu 30.64%
iter 680: loss 7.1567, lr 0.00025, time 583.25ms, mfu 30.91%
iter 690: loss 7.1110, lr 0.00027, time 579.73ms, mfu 31.17%
step 700: train loss 7.2044, val loss 7.1861
saving checkpoint to out

=== SCALING OPERATION TRIGGERED (DDP SYNC) ===
Operation: change_batch_size
Trigger reason: Loss threshold
Current val loss: 7.1861, Trigger loss: 100.0000
Iterations since last op: 100, Max wait: 1
Executing operation: change_batch_size  with value: 0.25
Batch size: 32 -> 8
=== SCALING OPERATION COMPLETE ===


=== SCALING OPERATION TRIGGERED (DDP SYNC) ===
Operation: change_grad_accum
Trigger reason: Loss threshold
Current val loss: 7.1861, Trigger loss: 100.0000
Iterations since last op: 0, Max wait: 1
Executing operation: change_grad_accum  with value: 2.0
Grad accum steps: 2 -> 4
=== SCALING OPERATION COMPLETE ===


=== SCALING OPERATION TRIGGERED (DDP SYNC) ===
Operation: change_lr
Trigger reason: Loss threshold
Current val loss: 7.1861, Trigger loss: 100.0000
Iterations since last op: 0, Max wait: 1
Executing operation: change_lr  with value: 0.7
LR multiplier: 1.4000 -> 0.9800
=== SCALING OPERATION COMPLETE ===


=== SCALING OPERATION TRIGGERED (DDP SYNC) ===
Operation: reset_lr_schedule
Trigger reason: Loss threshold
Current val loss: 7.1861, Trigger loss: 100.0000
Iterations since last op: 0, Max wait: 1
Executing operation: reset_lr_schedule  with value: None
LR schedule offset: 500 -> 700
=== SCALING OPERATION COMPLETE ===


=== SCALING OPERATION TRIGGERED (DDP SYNC) ===
Operation: stack_layers
Trigger reason: Loss threshold
Current val loss: 7.1861, Trigger loss: 100.0000
Iterations since last op: 0, Max wait: 1
Executing operation: stack_layers first resize with value: 2
Performing architectural operation: stack_layers
Stacking layers: current depth 3, creating 6 total layers.
Model now has 6 layers.

Detailed parameter count:
  total                  | Total:   66,549,504 | Trainable:   17,299,200
  token_embeddings       | Total:    4,902,912 | Trainable:    4,902,912
  position_embeddings    | Total:            0 | Trainable:            0
  attention_layers       | Total:   15,925,248 | Trainable:    5,308,416
  feed_forward_layers    | Total:    7,077,888 | Trainable:    7,077,888
  layer_norms            | Total:        9,216 | Trainable:        9,216
  final_layer_norm       | Total:          768 | Trainable:          768
  language_model_head    | Total:   38,633,472 | Trainable:            0
------------------------------------------------------------
Re-configuring optimizer after architectural change...
num decayed parameter tensors: 32, with 17,289,216 parameters
num non-decayed parameter tensors: 13, with 9,984 parameters
using fused AdamW: True
Transferring optimizer state for existing parameters...
Transferred optimizer state for 0 / 52 parameters
Re-compiling the model...

============================================================
             ARCHITECTURE CHANGE (at Iter 700)
============================================================
  n_layer                | 6
  n_head                 | 12
  n_embd                 | 768
  n_hidden               | 768
  block_size             | 1024
  vocab_size             | 50304
  dropout                | 0.0
  bias                   | False
  embedding_mode         | lora
  attn_lora_rank         | 96
  embedding_rank         | 96
  lora_alpha             | 1.0
============================================================

Architectural operation completed successfully.
Re-evaluating validation loss after operation...
New val loss after operation: 7.7357
iter 700: loss 7.4466, lr 0.00028, time 41135.53ms, mfu 28.08%
iter 710: loss 7.7515, lr 0.00001, time 371.42ms, mfu 28.60%
iter 720: loss 7.3529, lr 0.00002, time 370.96ms, mfu 29.06%
iter 730: loss 7.4655, lr 0.00003, time 379.60ms, mfu 29.41%
iter 740: loss 7.2840, lr 0.00004, time 379.67ms, mfu 29.72%
iter 750: loss 7.1608, lr 0.00005, time 379.80ms, mfu 30.00%
iter 760: loss 7.1672, lr 0.00006, time 380.87ms, mfu 30.24%
iter 770: loss 7.3678, lr 0.00007, time 384.03ms, mfu 30.43%
iter 780: loss 7.3485, lr 0.00008, time 396.34ms, mfu 30.50%
iter 790: loss 7.5581, lr 0.00009, time 380.64ms, mfu 30.70%
step 800: train loss 7.3202, val loss 7.2248
saving checkpoint to out
merge_lora_weights 7.224844455718994 7.1 700
iter 800: loss 7.6553, lr 0.00010, time 2353.22ms, mfu 28.15%
iter 810: loss 7.3499, lr 0.00011, time 378.60ms, mfu 28.60%
iter 820: loss 7.5065, lr 0.00012, time 381.22ms, mfu 28.98%
iter 830: loss 6.9363, lr 0.00013, time 380.22ms, mfu 29.33%
iter 840: loss 7.0776, lr 0.00014, time 379.52ms, mfu 29.65%
iter 850: loss 7.2240, lr 0.00015, time 379.34ms, mfu 29.94%
iter 860: loss 7.4764, lr 0.00016, time 379.63ms, mfu 30.20%
iter 870: loss 7.2617, lr 0.00017, time 378.93ms, mfu 30.43%
iter 880: loss 7.3041, lr 0.00018, time 377.01ms, mfu 30.67%
iter 890: loss 6.8432, lr 0.00019, time 374.15ms, mfu 30.90%
step 900: train loss 7.2313, val loss 7.1474
saving checkpoint to out
merge_lora_weights 7.147387504577637 7.1 700
iter 900: loss 7.1037, lr 0.00020, time 2707.06ms, mfu 28.27%
iter 910: loss 7.1090, lr 0.00021, time 372.06ms, mfu 28.76%
iter 920: loss 7.0991, lr 0.00022, time 375.60ms, mfu 29.17%
iter 930: loss 7.0474, lr 0.00023, time 380.36ms, mfu 29.50%
iter 940: loss 7.2406, lr 0.00024, time 382.05ms, mfu 29.78%
iter 950: loss 7.2609, lr 0.00025, time 377.66ms, mfu 30.07%
iter 960: loss 7.2036, lr 0.00026, time 379.25ms, mfu 30.32%
iter 970: loss 7.0778, lr 0.00027, time 379.80ms, mfu 30.54%
iter 980: loss 7.0982, lr 0.00028, time 379.90ms, mfu 30.73%
iter 990: loss 6.9937, lr 0.00028, time 380.78ms, mfu 30.90%
step 1000: train loss 7.2770, val loss 7.2206
saving checkpoint to out
merge_lora_weights 7.220643043518066 7.1 700
iter 1000: loss 7.3348, lr 0.00029, time 2226.26ms, mfu 28.37%
iter 1010: loss 6.9800, lr 0.00030, time 375.47ms, mfu 28.82%
iter 1020: loss 7.0055, lr 0.00031, time 378.11ms, mfu 29.20%
iter 1030: loss 6.9808, lr 0.00032, time 381.35ms, mfu 29.52%
iter 1040: loss 7.0930, lr 0.00033, time 379.34ms, mfu 29.82%
iter 1050: loss 7.2609, lr 0.00034, time 380.20ms, mfu 30.09%
iter 1060: loss 7.3263, lr 0.00035, time 380.55ms, mfu 30.32%
iter 1070: loss 7.1203, lr 0.00036, time 379.48ms, mfu 30.55%
iter 1080: loss 7.3669, lr 0.00037, time 384.74ms, mfu 30.70%
iter 1090: loss 7.0428, lr 0.00038, time 381.00ms, mfu 30.87%
step 1100: train loss 7.0379, val loss 7.1647
saving checkpoint to out
merge_lora_weights 7.164736270904541 7.1 700
iter 1100: loss 7.2703, lr 0.00039, time 2294.80ms, mfu 28.32%
iter 1110: loss 7.1369, lr 0.00040, time 380.21ms, mfu 28.74%
iter 1120: loss 7.4008, lr 0.00041, time 378.10ms, mfu 29.13%
iter 1130: loss 7.1119, lr 0.00042, time 381.33ms, mfu 29.45%
iter 1140: loss 7.2318, lr 0.00043, time 382.23ms, mfu 29.74%
iter 1150: loss 7.3164, lr 0.00044, time 380.25ms, mfu 30.01%
iter 1160: loss 6.7369, lr 0.00045, time 380.46ms, mfu 30.26%
iter 1170: loss 7.4057, lr 0.00046, time 378.55ms, mfu 30.49%
iter 1180: loss 6.9753, lr 0.00047, time 380.18ms, mfu 30.69%
iter 1190: loss 7.2864, lr 0.00048, time 380.13ms, mfu 30.87%
step 1200: train loss 7.1033, val loss 7.1482
saving checkpoint to out
merge_lora_weights 7.14819860458374 7.1 700
iter 1200: loss 7.5465, lr 0.00049, time 2728.75ms, mfu 28.23%
iter 1210: loss 7.1334, lr 0.00050, time 373.19ms, mfu 28.72%
iter 1220: loss 7.2364, lr 0.00051, time 377.58ms, mfu 29.12%
iter 1230: loss 6.9094, lr 0.00052, time 379.73ms, mfu 29.46%
iter 1240: loss 7.3164, lr 0.00053, time 380.03ms, mfu 29.76%
iter 1250: loss 7.0895, lr 0.00054, time 380.27ms, mfu 30.03%
iter 1260: loss 6.9751, lr 0.00055, time 378.77ms, mfu 30.29%
iter 1270: loss 6.8155, lr 0.00056, time 379.74ms, mfu 30.51%
iter 1280: loss 7.0156, lr 0.00057, time 379.54ms, mfu 30.71%
iter 1290: loss 7.2382, lr 0.00058, time 380.52ms, mfu 30.89%
step 1300: train loss 7.0600, val loss 7.1592
saving checkpoint to out
merge_lora_weights 7.159173011779785 7.1 700
iter 1300: loss 7.1801, lr 0.00059, time 2646.71ms, mfu 28.26%
iter 1310: loss 6.9677, lr 0.00060, time 376.99ms, mfu 28.71%
iter 1320: loss 7.1398, lr 0.00061, time 380.14ms, mfu 29.09%
iter 1330: loss 6.7166, lr 0.00062, time 379.54ms, mfu 29.43%
iter 1340: loss 7.0528, lr 0.00063, time 380.54ms, mfu 29.73%
iter 1350: loss 7.1083, lr 0.00064, time 381.69ms, mfu 30.00%
iter 1360: loss 6.8548, lr 0.00065, time 377.91ms, mfu 30.26%
iter 1370: loss 6.9467, lr 0.00066, time 379.95ms, mfu 30.49%
iter 1380: loss 6.9450, lr 0.00067, time 379.22ms, mfu 30.69%
iter 1390: loss 7.3716, lr 0.00068, time 383.18ms, mfu 30.85%
step 1400: train loss 7.1257, val loss 7.0455
saving checkpoint to out

=== SCALING OPERATION TRIGGERED (DDP SYNC) ===
Operation: merge_lora_weights
Trigger reason: Loss threshold
Current val loss: 7.0455, Trigger loss: 7.1000
Iterations since last op: 700, Max wait: 700
Executing operation: merge_lora_weights third burn with value: None
Performing architectural operation: merge_lora_weights
Merging LoRA weights into main weights...
LoRA weights merged and reset.

Detailed parameter count:
  total                  | Total:   66,549,504 | Trainable:   17,299,200
  token_embeddings       | Total:    4,902,912 | Trainable:    4,902,912
  position_embeddings    | Total:            0 | Trainable:            0
  attention_layers       | Total:   15,925,248 | Trainable:    5,308,416
  feed_forward_layers    | Total:    7,077,888 | Trainable:    7,077,888
  layer_norms            | Total:        9,216 | Trainable:        9,216
  final_layer_norm       | Total:          768 | Trainable:          768
  language_model_head    | Total:   38,633,472 | Trainable:            0
------------------------------------------------------------
Re-configuring optimizer after architectural change...
num decayed parameter tensors: 32, with 17,289,216 parameters
num non-decayed parameter tensors: 13, with 9,984 parameters
using fused AdamW: True
Transferring optimizer state for existing parameters...
Transferred optimizer state for 0 / 52 parameters
Re-compiling the model...

============================================================
             ARCHITECTURE CHANGE (at Iter 1400)
============================================================
  n_layer                | 6
  n_head                 | 12
  n_embd                 | 768
  n_hidden               | 768
  block_size             | 1024
  vocab_size             | 50304
  dropout                | 0.0
  bias                   | False
  embedding_mode         | lora
  attn_lora_rank         | 96
  embedding_rank         | 96
  lora_alpha             | 1.0
============================================================

Architectural operation completed successfully.
=== SCALING OPERATION COMPLETE ===


=== SCALING OPERATION TRIGGERED (DDP SYNC) ===
Operation: change_batch_size
Trigger reason: Loss threshold
Current val loss: 7.0455, Trigger loss: 100.0000
Iterations since last op: 0, Max wait: 1
Executing operation: change_batch_size  with value: 0.5
Batch size: 8 -> 4
=== SCALING OPERATION COMPLETE ===


=== SCALING OPERATION TRIGGERED (DDP SYNC) ===
Operation: change_grad_accum
Trigger reason: Loss threshold
Current val loss: 7.0455, Trigger loss: 100.0000
Iterations since last op: 0, Max wait: 1
Executing operation: change_grad_accum  with value: 2.0
Grad accum steps: 4 -> 8
=== SCALING OPERATION COMPLETE ===


=== SCALING OPERATION TRIGGERED (DDP SYNC) ===
Operation: widen_mlp
Trigger reason: Loss threshold
Current val loss: 7.0455, Trigger loss: 100.0000
Iterations since last op: 0, Max wait: 1
Executing operation: widen_mlp  with value: 2
Performing architectural operation: widen_mlp
Widening MLP layers by a factor of 2.
MLP hidden dimension widened to 1536.

Detailed parameter count:
  total                  | Total:   73,627,392 | Trainable:   24,377,088
  token_embeddings       | Total:    4,902,912 | Trainable:    4,902,912
  position_embeddings    | Total:            0 | Trainable:            0
  attention_layers       | Total:   15,925,248 | Trainable:    5,308,416
  feed_forward_layers    | Total:   14,155,776 | Trainable:   14,155,776
  layer_norms            | Total:        9,216 | Trainable:        9,216
  final_layer_norm       | Total:          768 | Trainable:          768
  language_model_head    | Total:   38,633,472 | Trainable:            0
------------------------------------------------------------
Re-configuring optimizer after architectural change...
num decayed parameter tensors: 32, with 24,367,104 parameters
num non-decayed parameter tensors: 13, with 9,984 parameters
using fused AdamW: True
Transferring optimizer state for existing parameters...
Transferred optimizer state for 0 / 52 parameters
Re-compiling the model...

============================================================
             ARCHITECTURE CHANGE (at Iter 1400)
============================================================
  n_layer                | 6
  n_head                 | 12
  n_embd                 | 768
  n_hidden               | 1536
  block_size             | 1024
  vocab_size             | 50304
  dropout                | 0.0
  bias                   | False
  embedding_mode         | lora
  attn_lora_rank         | 96
  embedding_rank         | 96
  lora_alpha             | 1.0
============================================================

Architectural operation completed successfully.
Re-evaluating validation loss after operation...
New val loss after operation: 7.0887
iter 1400: loss 6.8821, lr 0.00069, time 36662.45ms, mfu 27.80%
iter 1410: loss 7.0862, lr 0.00070, time 413.39ms, mfu 28.28%
iter 1420: loss 6.8729, lr 0.00071, time 417.49ms, mfu 28.69%
iter 1430: loss 7.2409, lr 0.00072, time 422.78ms, mfu 29.01%
iter 1440: loss 7.4594, lr 0.00073, time 427.51ms, mfu 29.27%
iter 1450: loss 6.6804, lr 0.00074, time 428.80ms, mfu 29.49%
iter 1460: loss 6.9255, lr 0.00075, time 430.08ms, mfu 29.68%
iter 1470: loss 6.6430, lr 0.00075, time 429.01ms, mfu 29.86%
iter 1480: loss 7.1462, lr 0.00076, time 432.57ms, mfu 29.99%
iter 1490: loss 7.6172, lr 0.00077, time 426.68ms, mfu 30.15%
step 1500: train loss 7.1014, val loss 7.1555
saving checkpoint to out

=== SCALING OPERATION TRIGGERED (DDP SYNC) ===
Operation: change_lr
Trigger reason: Loss threshold
Current val loss: 7.1555, Trigger loss: 100.0000
Iterations since last op: 100, Max wait: 1
Executing operation: change_lr  with value: 0.7
LR multiplier: 0.9800 -> 0.6860
=== SCALING OPERATION COMPLETE ===


=== SCALING OPERATION TRIGGERED (DDP SYNC) ===
Operation: reset_lr_schedule
Trigger reason: Loss threshold
Current val loss: 7.1555, Trigger loss: 100.0000
Iterations since last op: 0, Max wait: 1
Executing operation: reset_lr_schedule  with value: None
LR schedule offset: 700 -> 1500
=== SCALING OPERATION COMPLETE ===


=== SCALING OPERATION TRIGGERED (DDP SYNC) ===
Operation: stack_layers
Trigger reason: Loss threshold
Current val loss: 7.1555, Trigger loss: 100.0000
Iterations since last op: 0, Max wait: 1
Executing operation: stack_layers third resize with value: 2
Performing architectural operation: stack_layers
Stacking layers: current depth 6, creating 12 total layers.
Model now has 12 layers.

Detailed parameter count:
  total                  | Total:  103,717,632 | Trainable:   43,850,496
  token_embeddings       | Total:    4,902,912 | Trainable:    4,902,912
  position_embeddings    | Total:            0 | Trainable:            0
  attention_layers       | Total:   31,850,496 | Trainable:   10,616,832
  feed_forward_layers    | Total:   28,311,552 | Trainable:   28,311,552
  layer_norms            | Total:       18,432 | Trainable:       18,432
  final_layer_norm       | Total:          768 | Trainable:          768
  language_model_head    | Total:   38,633,472 | Trainable:            0
------------------------------------------------------------
Re-configuring optimizer after architectural change...
num decayed parameter tensors: 62, with 43,831,296 parameters
num non-decayed parameter tensors: 25, with 19,200 parameters
using fused AdamW: True
Transferring optimizer state for existing parameters...
Transferred optimizer state for 0 / 100 parameters
Re-compiling the model...

============================================================
             ARCHITECTURE CHANGE (at Iter 1500)
============================================================
  n_layer                | 12
  n_head                 | 12
  n_embd                 | 768
  n_hidden               | 1536
  block_size             | 1024
  vocab_size             | 50304
  dropout                | 0.0
  bias                   | False
  embedding_mode         | lora
  attn_lora_rank         | 96
  embedding_rank         | 96
  lora_alpha             | 1.0
============================================================

Architectural operation completed successfully.
Re-evaluating validation loss after operation...
New val loss after operation: 7.2535
iter 1500: loss 7.0888, lr 0.00078, time 64413.58ms, mfu 27.17%
iter 1510: loss 7.0383, lr 0.00001, time 654.81ms, mfu 27.50%
iter 1520: loss 7.1992, lr 0.00001, time 659.54ms, mfu 27.77%
iter 1530: loss 7.3890, lr 0.00002, time 658.94ms, mfu 28.01%
iter 1540: loss 6.7476, lr 0.00003, time 667.54ms, mfu 28.20%
iter 1550: loss 7.3478, lr 0.00003, time 659.15ms, mfu 28.40%
iter 1560: loss 7.2760, lr 0.00004, time 659.72ms, mfu 28.58%
iter 1570: loss 6.9859, lr 0.00005, time 660.84ms, mfu 28.73%
iter 1580: loss 6.9570, lr 0.00006, time 659.52ms, mfu 28.88%
iter 1590: loss 7.0410, lr 0.00006, time 659.45ms, mfu 29.01%
step 1600: train loss 7.1218, val loss 7.1769
saving checkpoint to out
merge_lora_weights 7.17691707611084 6.0 600
iter 1600: loss 7.0128, lr 0.00007, time 3931.30ms, mfu 26.62%
iter 1610: loss 7.0953, lr 0.00008, time 640.26ms, mfu 27.07%
iter 1620: loss 6.8351, lr 0.00008, time 656.35ms, mfu 27.40%
iter 1630: loss 7.6988, lr 0.00009, time 651.21ms, mfu 27.72%
iter 1640: loss 7.1517, lr 0.00010, time 644.88ms, mfu 28.03%
iter 1650: loss 7.0721, lr 0.00010, time 658.33ms, mfu 28.26%
iter 1660: loss 6.8897, lr 0.00011, time 657.01ms, mfu 28.46%
iter 1670: loss 7.0345, lr 0.00012, time 660.06ms, mfu 28.63%
iter 1680: loss 6.8211, lr 0.00012, time 659.74ms, mfu 28.79%
iter 1690: loss 7.0619, lr 0.00013, time 660.36ms, mfu 28.93%
step 1700: train loss 6.9717, val loss 6.9181
saving checkpoint to out
merge_lora_weights 6.918108940124512 6.0 600
iter 1700: loss 7.0271, lr 0.00014, time 4053.89ms, mfu 26.53%
iter 1710: loss 7.5642, lr 0.00014, time 659.29ms, mfu 26.89%
iter 1720: loss 6.8566, lr 0.00015, time 660.70ms, mfu 27.22%
iter 1730: loss 7.4398, lr 0.00016, time 661.55ms, mfu 27.51%
iter 1740: loss 6.5911, lr 0.00017, time 661.83ms, mfu 27.77%
iter 1750: loss 7.1867, lr 0.00017, time 659.53ms, mfu 28.01%
iter 1760: loss 6.9863, lr 0.00018, time 660.27ms, mfu 28.23%
iter 1770: loss 7.0618, lr 0.00019, time 659.87ms, mfu 28.42%
iter 1780: loss 6.8323, lr 0.00019, time 660.79ms, mfu 28.60%
iter 1790: loss 6.4137, lr 0.00020, time 659.40ms, mfu 28.76%
step 1800: train loss 7.0894, val loss 7.0587
saving checkpoint to out
merge_lora_weights 7.058673858642578 6.0 600
iter 1800: loss 7.2937, lr 0.00021, time 4031.65ms, mfu 26.37%
iter 1810: loss 7.1445, lr 0.00021, time 641.73ms, mfu 26.84%
iter 1820: loss 6.7096, lr 0.00022, time 658.09ms, mfu 27.18%
iter 1830: loss 7.1595, lr 0.00023, time 658.25ms, mfu 27.49%
iter 1840: loss 7.0226, lr 0.00023, time 656.93ms, mfu 27.77%
iter 1850: loss 6.7651, lr 0.00024, time 659.91ms, mfu 28.02%
iter 1860: loss 7.0277, lr 0.00025, time 659.54ms, mfu 28.23%
iter 1870: loss 6.9107, lr 0.00025, time 659.40ms, mfu 28.43%
iter 1880: loss 6.7699, lr 0.00026, time 659.69ms, mfu 28.61%
iter 1890: loss 7.1509, lr 0.00027, time 660.34ms, mfu 28.76%
step 1900: train loss 7.1112, val loss 7.0786
saving checkpoint to out
merge_lora_weights 7.0785813331604 6.0 600
iter 1900: loss 6.7263, lr 0.00027, time 3931.92ms, mfu 26.39%
iter 1910: loss 7.1416, lr 0.00028, time 643.08ms, mfu 26.85%
iter 1920: loss 6.8406, lr 0.00029, time 661.57ms, mfu 27.18%
iter 1930: loss 7.2027, lr 0.00030, time 662.46ms, mfu 27.47%
iter 1940: loss 6.8953, lr 0.00030, time 660.37ms, mfu 27.74%
iter 1950: loss 7.3565, lr 0.00031, time 659.71ms, mfu 27.98%
iter 1960: loss 6.9962, lr 0.00032, time 659.76ms, mfu 28.20%
iter 1970: loss 7.0763, lr 0.00032, time 660.89ms, mfu 28.40%
iter 1980: loss 7.1190, lr 0.00033, time 660.62ms, mfu 28.57%
iter 1990: loss 7.0767, lr 0.00034, time 661.43ms, mfu 28.73%
step 2000: train loss 6.9042, val loss 6.9347
saving checkpoint to out
merge_lora_weights 6.934704780578613 6.0 600
iter 2000: loss 7.7276, lr 0.00034, time 4020.92ms, mfu 26.35%
iter 2010: loss 6.7272, lr 0.00035, time 646.32ms, mfu 26.80%
iter 2020: loss 7.1333, lr 0.00036, time 660.15ms, mfu 27.13%
iter 2030: loss 7.4065, lr 0.00036, time 659.00ms, mfu 27.44%
iter 2040: loss 7.0154, lr 0.00037, time 660.47ms, mfu 27.72%
iter 2050: loss 7.0509, lr 0.00038, time 660.46ms, mfu 27.96%
iter 2060: loss 7.0746, lr 0.00038, time 660.65ms, mfu 28.18%
iter 2070: loss 7.3547, lr 0.00039, time 659.70ms, mfu 28.38%
iter 2080: loss 6.9674, lr 0.00040, time 660.61ms, mfu 28.56%
iter 2090: loss 7.0424, lr 0.00041, time 659.40ms, mfu 28.72%
step 2100: train loss 6.8672, val loss 6.8780
saving checkpoint to out

=== SCALING OPERATION TRIGGERED (DDP SYNC) ===
Operation: merge_lora_weights
Trigger reason: Timeout
Current val loss: 6.8780, Trigger loss: 6.0000
Iterations since last op: 600, Max wait: 600
Executing operation: merge_lora_weights fourth burn with value: None
Performing architectural operation: merge_lora_weights
Merging LoRA weights into main weights...
LoRA weights merged and reset.

Detailed parameter count:
  total                  | Total:  103,717,632 | Trainable:   43,850,496
  token_embeddings       | Total:    4,902,912 | Trainable:    4,902,912
  position_embeddings    | Total:            0 | Trainable:            0
  attention_layers       | Total:   31,850,496 | Trainable:   10,616,832
  feed_forward_layers    | Total:   28,311,552 | Trainable:   28,311,552
  layer_norms            | Total:       18,432 | Trainable:       18,432
  final_layer_norm       | Total:          768 | Trainable:          768
  language_model_head    | Total:   38,633,472 | Trainable:            0
------------------------------------------------------------
Re-configuring optimizer after architectural change...
num decayed parameter tensors: 62, with 43,831,296 parameters
num non-decayed parameter tensors: 25, with 19,200 parameters
using fused AdamW: True
Transferring optimizer state for existing parameters...
Transferred optimizer state for 0 / 100 parameters
Re-compiling the model...

============================================================
             ARCHITECTURE CHANGE (at Iter 2100)
============================================================
  n_layer                | 12
  n_head                 | 12
  n_embd                 | 768
  n_hidden               | 1536
  block_size             | 1024
  vocab_size             | 50304
  dropout                | 0.0
  bias                   | False
  embedding_mode         | lora
  attn_lora_rank         | 96
  embedding_rank         | 96
  lora_alpha             | 1.0
============================================================

Architectural operation completed successfully.
=== SCALING OPERATION COMPLETE ===


=== SCALING OPERATION TRIGGERED (DDP SYNC) ===
Operation: widen_mlp
Trigger reason: Loss threshold
Current val loss: 6.8780, Trigger loss: 100.0000
Iterations since last op: 0, Max wait: 1
Executing operation: widen_mlp second resize with value: 2
Performing architectural operation: widen_mlp
Widening MLP layers by a factor of 2.
MLP hidden dimension widened to 3072.

Detailed parameter count:
  total                  | Total:  132,029,184 | Trainable:   72,162,048
  token_embeddings       | Total:    4,902,912 | Trainable:    4,902,912
  position_embeddings    | Total:            0 | Trainable:            0
  attention_layers       | Total:   31,850,496 | Trainable:   10,616,832
  feed_forward_layers    | Total:   56,623,104 | Trainable:   56,623,104
  layer_norms            | Total:       18,432 | Trainable:       18,432
  final_layer_norm       | Total:          768 | Trainable:          768
  language_model_head    | Total:   38,633,472 | Trainable:            0
------------------------------------------------------------
Re-configuring optimizer after architectural change...
num decayed parameter tensors: 62, with 72,142,848 parameters
num non-decayed parameter tensors: 25, with 19,200 parameters
using fused AdamW: True
Transferring optimizer state for existing parameters...
Transferred optimizer state for 0 / 100 parameters
Re-compiling the model...

============================================================
             ARCHITECTURE CHANGE (at Iter 2100)
============================================================
  n_layer                | 12
  n_head                 | 12
  n_embd                 | 768
  n_hidden               | 3072
  block_size             | 1024
  vocab_size             | 50304
  dropout                | 0.0
  bias                   | False
  embedding_mode         | lora
  attn_lora_rank         | 96
  embedding_rank         | 96
  lora_alpha             | 1.0
============================================================

Architectural operation completed successfully.
Re-evaluating validation loss after operation...
New val loss after operation: 6.9862
iter 2100: loss 6.9777, lr 0.00041, time 68018.98ms, mfu 25.89%
iter 2110: loss 7.4577, lr 0.00042, time 820.44ms, mfu 26.29%
iter 2120: loss 7.2642, lr 0.00043, time 820.17ms, mfu 26.65%
iter 2130: loss 6.7844, lr 0.00043, time 819.65ms, mfu 26.97%
iter 2140: loss 6.6310, lr 0.00044, time 821.46ms, mfu 27.26%
iter 2150: loss 6.9908, lr 0.00045, time 818.95ms, mfu 27.53%
iter 2160: loss 7.2161, lr 0.00045, time 821.33ms, mfu 27.76%
iter 2170: loss 6.8097, lr 0.00046, time 819.18ms, mfu 27.98%
iter 2180: loss 7.0509, lr 0.00047, time 818.04ms, mfu 28.18%
iter 2190: loss 7.0393, lr 0.00047, time 818.23ms, mfu 28.36%
step 2200: train loss 6.8530, val loss 6.9993
saving checkpoint to out

=== SCALING OPERATION TRIGGERED (DDP SYNC) ===
Operation: change_batch_size
Trigger reason: Loss threshold
Current val loss: 6.9993, Trigger loss: 100.0000
Iterations since last op: 100, Max wait: 1
Executing operation: change_batch_size  with value: 0.5
Batch size: 4 -> 2
=== SCALING OPERATION COMPLETE ===


=== SCALING OPERATION TRIGGERED (DDP SYNC) ===
Operation: change_grad_accum
Trigger reason: Loss threshold
Current val loss: 6.9993, Trigger loss: 100.0000
Iterations since last op: 0, Max wait: 1
Executing operation: change_grad_accum  with value: 2.0
Grad accum steps: 8 -> 16
=== SCALING OPERATION COMPLETE ===


=== SCALING OPERATION TRIGGERED (DDP SYNC) ===
Operation: stack_layers
Trigger reason: Loss threshold
Current val loss: 6.9993, Trigger loss: 100.0000
Iterations since last op: 0, Max wait: 1
Executing operation: stack_layers fourth resize with value: 2
Performing architectural operation: stack_layers
Stacking layers: current depth 12, creating 24 total layers.
Model now has 24 layers.

Detailed parameter count:
  total                  | Total:  220,521,216 | Trainable:  139,420,416
  token_embeddings       | Total:    4,902,912 | Trainable:    4,902,912
  position_embeddings    | Total:            0 | Trainable:            0
  attention_layers       | Total:   63,700,992 | Trainable:   21,233,664
  feed_forward_layers    | Total:  113,246,208 | Trainable:  113,246,208
  layer_norms            | Total:       36,864 | Trainable:       36,864
  final_layer_norm       | Total:          768 | Trainable:          768
  language_model_head    | Total:   38,633,472 | Trainable:            0
------------------------------------------------------------
Re-configuring optimizer after architectural change...
num decayed parameter tensors: 122, with 139,382,784 parameters
num non-decayed parameter tensors: 49, with 37,632 parameters
using fused AdamW: True
Transferring optimizer state for existing parameters...
Transferred optimizer state for 0 / 196 parameters
Re-compiling the model...

============================================================
             ARCHITECTURE CHANGE (at Iter 2200)
============================================================
  n_layer                | 24
  n_head                 | 12
  n_embd                 | 768
  n_hidden               | 3072
  block_size             | 1024
  vocab_size             | 50304
  dropout                | 0.0
  bias                   | False
  embedding_mode         | lora
  attn_lora_rank         | 96
  embedding_rank         | 96
  lora_alpha             | 1.0
============================================================

Architectural operation completed successfully.
Re-evaluating validation loss after operation...
New val loss after operation: 7.0290
Traceback (most recent call last):
  File "/teamspace/studios/this_studio/nanoGPT/train.py", line 782, in <module>
    logits, loss = model(X, Y)
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py", line 655, in _fn
    return fn(*args, **kwargs)
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py", line 1432, in __call__
    return self._torchdynamo_orig_callable(
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py", line 1213, in __call__
    result = self._inner_convert(
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py", line 598, in __call__
    return _compile(
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py", line 1059, in _compile
    guarded_code = compile_inner(code, one_graph, hooks, transform)
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/_utils_internal.py", line 97, in wrapper_function
    return function(*args, **kwargs)
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py", line 761, in compile_inner
    return _compile_inner(code, one_graph, hooks, transform)
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py", line 797, in _compile_inner
    out_code = transform_code_object(code, transform)
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/_dynamo/bytecode_transformation.py", line 1422, in transform_code_object
    transformations(instructions, code_options)
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py", line 257, in _fn
    return fn(*args, **kwargs)
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py", line 715, in transform
    tracer.run()
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py", line 3500, in run
    super().run()
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py", line 1337, in run
    while self.step():
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py", line 1246, in step
    self.dispatch_table[inst.opcode](self, inst)
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py", line 3701, in RETURN_VALUE
    self._return(inst)
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py", line 3686, in _return
    self.output.compile_subgraph(
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/_dynamo/output_graph.py", line 1179, in compile_subgraph
    self.compile_and_call_fx_graph(
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/_dynamo/output_graph.py", line 1437, in compile_and_call_fx_graph
    compiled_fn = self.call_user_compiler(gm)
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/_dynamo/output_graph.py", line 1487, in call_user_compiler
    return self._call_user_compiler(gm)
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/_dynamo/output_graph.py", line 1519, in _call_user_compiler
    compiled_fn = compiler_fn(gm, self.example_inputs())
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/_dynamo/repro/after_dynamo.py", line 150, in __call__
    compiled_gm = compiler_fn(gm, example_inputs)
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/__init__.py", line 2347, in __call__
    return compile_fx(model_, inputs_, config_patches=self.config)
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/_inductor/compile_fx.py", line 2089, in compile_fx
    return aot_autograd(
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/_dynamo/backends/common.py", line 101, in __call__
    cg = aot_module_simplified(gm, example_inputs, **self.kwargs)
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py", line 1160, in aot_module_simplified
    compiled_fn = AOTAutogradCache.load(
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/_functorch/_aot_autograd/autograd_cache.py", line 775, in load
    compiled_fn = dispatch_and_compile()
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py", line 1145, in dispatch_and_compile
    compiled_fn, _ = create_aot_dispatcher_function(
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py", line 570, in create_aot_dispatcher_function
    return _create_aot_dispatcher_function(
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py", line 820, in _create_aot_dispatcher_function
    compiled_fn, fw_metadata = compiler_fn(
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/_functorch/_aot_autograd/jit_compile_runtime_wrappers.py", line 1187, in aot_dispatch_autograd
    compiled_bw_func = aot_config.bw_compiler(
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py", line 479, in __call__
    return self.compiler_fn(gm, example_inputs)
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/_dynamo/backends/common.py", line 72, in _wrapped_bw_compiler
    return disable(disable(bw_compiler_fn)(*args, **kwargs))
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py", line 838, in _fn
    return fn(*args, **kwargs)
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/_utils_internal.py", line 97, in wrapper_function
    return function(*args, **kwargs)
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/_inductor/compile_fx.py", line 2014, in bw_compiler
    return inner_compile(
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/_inductor/compile_fx.py", line 628, in compile_fx_inner
    return wrap_compiler_debug(_compile_fx_inner, compiler_name="inductor")(
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/_dynamo/repro/after_aot.py", line 124, in debug_wrapper
    inner_compiled_fn = compiler_fn(gm, example_inputs)
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/_inductor/compile_fx.py", line 745, in _compile_fx_inner
    mb_compiled_graph = fx_codegen_and_compile(
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/_inductor/compile_fx.py", line 1295, in fx_codegen_and_compile
    return scheme.codegen_and_compile(gm, example_inputs, inputs_to_check, graph_kwargs)
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/_inductor/compile_fx.py", line 1119, in codegen_and_compile
    graph.run(*example_inputs)
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/_inductor/graph.py", line 877, in run
    return super().run(*args)
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/fx/interpreter.py", line 171, in run
    self.env[node] = self.run_node(node)
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/_inductor/graph.py", line 1527, in run_node
    result = super().run_node(n)
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/fx/interpreter.py", line 240, in run_node
    return getattr(self, n.op)(n.target, args, kwargs)
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/_inductor/graph.py", line 1188, in call_function
    out = lowerings[target](*args, **kwargs)  # type: ignore[index]
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/_inductor/lowering.py", line 459, in wrapped
    args, kwargs = transform_args(
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/_inductor/lowering.py", line 373, in transform_args
    broadcasted = broadcast_tensors(
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/_inductor/lowering.py", line 466, in wrapped
    out = decomp_fn(*args, **kwargs)
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/_inductor/lowering.py", line 964, in broadcast_tensors
    target: list[sympy.Expr] = functools.reduce(
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/_inductor/lowering.py", line 515, in broadcast_symbolic_shapes
    V.graph.sizevars.guard_equals(x, y)
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/_inductor/sizevars.py", line 411, in guard_equals
    right = sympy_subs(right, self.inv_precomputed_replacements)  # type: ignore[arg-type]
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/_inductor/utils.py", line 782, in sympy_subs
    return sympy.sympify(expr).xreplace(
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/sympy/core/sympify.py", line 124, in sympify
    def sympify(a, locals=None, convert_xor=True, strict=False, rational=False,
KeyboardInterrupt
Traceback (most recent call last):
  File "/teamspace/studios/this_studio/nanoGPT/train.py", line 782, in <module>
    logits, loss = model(X, Y)
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py", line 655, in _fn
    return fn(*args, **kwargs)
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py", line 1432, in __call__
    return self._torchdynamo_orig_callable(
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py", line 1213, in __call__
    result = self._inner_convert(
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py", line 598, in __call__
    return _compile(
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py", line 1059, in _compile
    guarded_code = compile_inner(code, one_graph, hooks, transform)
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/_utils_internal.py", line 97, in wrapper_function
    return function(*args, **kwargs)
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py", line 761, in compile_inner
    return _compile_inner(code, one_graph, hooks, transform)
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py", line 797, in _compile_inner
    out_code = transform_code_object(code, transform)
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/_dynamo/bytecode_transformation.py", line 1422, in transform_code_object
    transformations(instructions, code_options)
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py", line 257, in _fn
    return fn(*args, **kwargs)
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py", line 715, in transform
    tracer.run()
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py", line 3500, in run
    super().run()
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py", line 1337, in run
    while self.step():
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py", line 1246, in step
    self.dispatch_table[inst.opcode](self, inst)
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py", line 3701, in RETURN_VALUE
    self._return(inst)
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py", line 3686, in _return
    self.output.compile_subgraph(
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/_dynamo/output_graph.py", line 1179, in compile_subgraph
    self.compile_and_call_fx_graph(
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/_dynamo/output_graph.py", line 1437, in compile_and_call_fx_graph
    compiled_fn = self.call_user_compiler(gm)
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/_dynamo/output_graph.py", line 1487, in call_user_compiler
    return self._call_user_compiler(gm)
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/_dynamo/output_graph.py", line 1519, in _call_user_compiler
    compiled_fn = compiler_fn(gm, self.example_inputs())
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/_dynamo/repro/after_dynamo.py", line 150, in __call__
    compiled_gm = compiler_fn(gm, example_inputs)
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/__init__.py", line 2347, in __call__
    return compile_fx(model_, inputs_, config_patches=self.config)
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/_inductor/compile_fx.py", line 2089, in compile_fx
    return aot_autograd(
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/_dynamo/backends/common.py", line 101, in __call__
    cg = aot_module_simplified(gm, example_inputs, **self.kwargs)
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py", line 1160, in aot_module_simplified
    compiled_fn = AOTAutogradCache.load(
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/_functorch/_aot_autograd/autograd_cache.py", line 775, in load
    compiled_fn = dispatch_and_compile()
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py", line 1145, in dispatch_and_compile
    compiled_fn, _ = create_aot_dispatcher_function(
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py", line 570, in create_aot_dispatcher_function
    return _create_aot_dispatcher_function(
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py", line 820, in _create_aot_dispatcher_function
    compiled_fn, fw_metadata = compiler_fn(
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/_functorch/_aot_autograd/jit_compile_runtime_wrappers.py", line 1187, in aot_dispatch_autograd
    compiled_bw_func = aot_config.bw_compiler(
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py", line 479, in __call__
    return self.compiler_fn(gm, example_inputs)
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/_dynamo/backends/common.py", line 72, in _wrapped_bw_compiler
    return disable(disable(bw_compiler_fn)(*args, **kwargs))
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py", line 838, in _fn
    return fn(*args, **kwargs)
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/_utils_internal.py", line 97, in wrapper_function
    return function(*args, **kwargs)
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/_inductor/compile_fx.py", line 2014, in bw_compiler
    return inner_compile(
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/_inductor/compile_fx.py", line 628, in compile_fx_inner
    return wrap_compiler_debug(_compile_fx_inner, compiler_name="inductor")(
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/_dynamo/repro/after_aot.py", line 124, in debug_wrapper
    inner_compiled_fn = compiler_fn(gm, example_inputs)
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/_inductor/compile_fx.py", line 745, in _compile_fx_inner
    mb_compiled_graph = fx_codegen_and_compile(
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/_inductor/compile_fx.py", line 1295, in fx_codegen_and_compile
    return scheme.codegen_and_compile(gm, example_inputs, inputs_to_check, graph_kwargs)
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/_inductor/compile_fx.py", line 1119, in codegen_and_compile
    graph.run(*example_inputs)
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/_inductor/graph.py", line 877, in run
    return super().run(*args)
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/fx/interpreter.py", line 171, in run
    self.env[node] = self.run_node(node)
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/_inductor/graph.py", line 1527, in run_node
    result = super().run_node(n)
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/fx/interpreter.py", line 240, in run_node
    return getattr(self, n.op)(n.target, args, kwargs)
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/_inductor/graph.py", line 1188, in call_function
    out = lowerings[target](*args, **kwargs)  # type: ignore[index]
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/_inductor/lowering.py", line 459, in wrapped
    args, kwargs = transform_args(
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/_inductor/lowering.py", line 373, in transform_args
    broadcasted = broadcast_tensors(
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/_inductor/lowering.py", line 466, in wrapped
    out = decomp_fn(*args, **kwargs)
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/_inductor/lowering.py", line 964, in broadcast_tensors
    target: list[sympy.Expr] = functools.reduce(
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/_inductor/lowering.py", line 515, in broadcast_symbolic_shapes
    V.graph.sizevars.guard_equals(x, y)
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/_inductor/sizevars.py", line 411, in guard_equals
    right = sympy_subs(right, self.inv_precomputed_replacements)  # type: ignore[arg-type]
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/_inductor/utils.py", line 782, in sympy_subs
    return sympy.sympify(expr).xreplace(
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/sympy/core/sympify.py", line 124, in sympify
    def sympify(a, locals=None, convert_xor=True, strict=False, rational=False,
KeyboardInterrupt
Exception ignored in atexit callback: <function _start_and_connect_service.<locals>.teardown_atexit at 0x7450418c1120>
Traceback (most recent call last):
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/wandb/sdk/lib/service/service_connection.py", line 54, in teardown_atexit
    conn.teardown(hooks.exit_code)
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/wandb/sdk/lib/service/service_connection.py", line 182, in teardown
    self._router.join()
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/wandb/sdk/interface/router.py", line 75, in join
    self._thread.join()
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/threading.py", line 1096, in join
    self._wait_for_tstate_lock()
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/threading.py", line 1116, in _wait_for_tstate_lock
    if lock.acquire(block, timeout):
KeyboardInterrupt:
Exception ignored in atexit callback: <function shutdown_compile_workers at 0x74505eb88ee0>
Traceback (most recent call last):
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/_inductor/async_compile.py", line 113, in shutdown_compile_workers
    pool.shutdown()
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/_inductor/compile_worker/subproc_pool.py", line 239, in shutdown
    self.process.wait(300)
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/subprocess.py", line 1222, in wait
    self._wait(timeout=sigint_timeout)
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/subprocess.py", line 1937, in _wait
    time.sleep(delay)
KeyboardInterrupt:
