# Char Diffusion Streaming Dataset

This directory implements the streaming dataset generator used by the `char_diffusion` experiment. It turns a character-level corpus into masked-language-modeling (MLM) training batches that are continuously written to the filesystem-backed queue consumed by `DatasetConsumer`.

## Directory contents

| File | Purpose |
| --- | --- |
| `prepare_streaming.py` | Streaming provider that subclasses `DataProviderBase` and emits masked batches for training and validation. |
| `masking_utils.py` | Masking strategies (`random`, `sticky`, and `span`) and helpers shared by the provider. |
| `file_utils.py` | Atomic write helpers used when running the provider standalone for debugging. |
| `config/*.py` | Optional stage-composition presets. `complex.py` demonstrates multi-stage unmasking schedules. |
| `input.txt` | Shakespeare corpus used to build the character vocabulary and sample training sequences. |

Running `prepare.py config/<your_config>.py` will drop queue files under `data/char_diffusion/queue/{train,val}` and a `meta.pkl` descriptor that `train.py` and other consumers can read.

## Provider workflow

1. **Initialization** – `CharDiffusionProvider` loads the `input.txt` corpus, constructs a character vocabulary, reserves an extra `[MASK]` token, and prepares 90/10 train/validation splits that are stored as integer ID lists.【F:data/char_diffusion/prepare_streaming.py†L40-L75】
2. **Metadata contract** – The provider advertises an MLM training type, vocabulary information, and the `(x, y)` batch schema so downstream trainers can validate shape and masking expectations.【F:data/char_diffusion/prepare_streaming.py†L108-L125】
3. **Sampling loop** – `DataProviderBase` repeatedly calls `sample_batch(split, rng)` to build up `batches_per_file` batches, concatenates them along the batch dimension, and writes each file atomically to the queue while respecting a configurable backlog limit.【F:data/common/provider_base.py†L22-L111】

### Default masking (single-stage)

If no composition config is provided, each batch is generated by sampling `batch_size` random subsequences of length `block_size` from the split-specific token list and applying BERT-style corruption at `mask_probability`. Labels keep the original token where a mask was applied and `ignore_index` everywhere else, matching the standard MLM loss expectation.【F:data/char_diffusion/prepare_streaming.py†L127-L162】 The corruption follows the 80/10/10 BERT rule: mask tokens, random replacements, or untouched originals.【F:data/char_diffusion/prepare_streaming.py†L16-L48】

### Stage-based masking (multi-stage diffusion)

When `use_all_stages_for_training=True` is supplied—either through `CharDiffusionProvider` kwargs or a config module such as `config/complex.py`—the provider mixes batches drawn from multiple masking stages within each written file.【F:data/char_diffusion/prepare_streaming.py†L51-L105】【F:data/char_diffusion/config/complex.py†L1-L25】 Stage definitions specify a `type` (`random`, `sticky`, or `span`) plus tuning knobs:

- **Random masking** samples a per-example ratio up to `max_masked_ratio` before applying BERT-style corruption.【F:data/char_diffusion/masking_utils.py†L121-L147】
- **Sticky masking** grows masks around already-selected positions to hit a `target_masked_ratio`, creating clustered corruption for diffusion-style training.【F:data/char_diffusion/masking_utils.py†L52-L118】
- **Span masking** hides a fixed number of contiguous spans directly with the mask token.【F:data/char_diffusion/masking_utils.py†L151-L183】

For each stage, `_calculate_stage_distribution` ensures that the `batches_per_file` budget is spread as evenly as possible, with any remainder allocated to the earliest stages. `_sample_stage_based_batch` then samples the requested number of batches per stage, applies the corresponding masking utility, concatenates the results, and shuffles them before returning a single `batch_size` batch to the base class.【F:data/char_diffusion/prepare_streaming.py†L83-L146】【F:data/char_diffusion/prepare_streaming.py†L164-L216】 Validation stages mirror this structure so evaluation batches reflect the same masking curriculum.【F:data/char_diffusion/prepare_streaming.py†L57-L82】

## Output files and metadata

Each produced `.pt` file contains:

- `tensors`: concatenated `x` (corrupted inputs) and `y` (targets) tensors of shape `[batches_per_file * batch_size, block_size]`.
- `metadata`: bookkeeping fields such as `file_idx`, `split`, and `num_batches`, useful for monitoring producer progress.【F:data/common/provider_base.py†L76-L104】

The provider writes files atomically via a temporary name and rotates sequence numbers to avoid overwriting. Backlog control prevents the queue from growing beyond `max_backlog_files` by pausing production when the consumer falls behind.【F:data/common/provider_base.py†L45-L74】【F:data/common/provider_base.py†L92-L111】 Running `prepare_streaming.py` directly (for debugging) uses the same helpers exposed in `file_utils.py` to guarantee durable writes.【F:data/char_diffusion/file_utils.py†L1-L58】

Metadata saved to `meta.pkl` includes vocabulary statistics, mask settings, and schema information in addition to the batch and block sizes, enabling `DatasetConsumer` to validate compatibility before training starts.【F:data/char_diffusion/prepare_streaming.py†L108-L125】【F:data/common/provider_base.py†L55-L74】 Delete stale queue files when you change schema-level settings such as the vocabulary or masking regime; the consumer will otherwise continue reading previously generated batches.

## Using the dataset

1. **Create or adapt a config** (e.g., `config/train_char_diffusion.py`) that sets `dataset = 'char_diffusion'`, `batch_size`, `block_size`, `batches_per_file`, and any stage kwargs. Both `prepare.py` and `train.py` load the same config file, so define hyperparameters once.【F:docs/datasets_guide.md†L29-L56】
2. **Start the producer** with `python prepare.py config/train_char_diffusion.py`. The provider will generate queue files until interrupted, logging progress when `verbose=True`.【F:docs/datasets_guide.md†L58-L70】
3. **Run the trainer** via `python train.py config/train_char_diffusion.py`. The consumer blocks until batches are available, consuming and deleting files to signal demand.【F:docs/datasets_guide.md†L3-L27】【F:data/common/provider_base.py†L92-L111】

Because `DataProviderBase` seeds each file deterministically based on `(split, seq, seed)`, re-running the producer with the same parameters regenerates identical data, which is especially useful for debugging and reproducibility.【F:data/common/provider_base.py†L76-L96】 Customize the dataset by editing `input.txt`, adjusting `mask_probability`, or pointing to an alternative stage config module.

## Debugging tips

- Use the standalone CLI in `prepare_streaming.py` (`python data/char_diffusion/prepare_streaming.py --help`) to experiment with batch sizes, stage mixes, or seeds without invoking the full streaming harness.【F:data/char_diffusion/prepare_streaming.py†L218-L273】
- Inspect produced files with `torch.load(path)` to confirm shapes and masks.
- If you change the vocabulary or schema, clear `data/char_diffusion/queue` and re-run the producer to avoid consumer errors caused by stale files.【F:docs/datasets_guide.md†L84-L96】

For deeper context on how this streaming pipeline integrates with the rest of the project, see `docs/datasets_guide.md`.
