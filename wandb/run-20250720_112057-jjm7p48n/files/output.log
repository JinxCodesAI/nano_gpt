
Detailed parameter count:
  total                  | Total:   52,149,504 | Trainable:    8,207,616
  token_embeddings       | Total:    2,451,456 | Trainable:    2,451,456
  position_embeddings    | Total:            0 | Trainable:            0
  attention_layers       | Total:    7,520,256 | Trainable:    2,211,840
  feed_forward_layers    | Total:    3,538,944 | Trainable:    3,538,944
  layer_norms            | Total:        4,608 | Trainable:        4,608
  final_layer_norm       | Total:          768 | Trainable:          768
  language_model_head    | Total:   38,633,472 | Trainable:            0
------------------------------------------------------------
eval every:100
step 0: train loss 10.9669, val loss 10.9694
merge_lora_weights 10.969396591186523 8.0 500
iter 0: loss 10.9466, lr 0.00005, time 6976.12ms, mfu -100.00%
iter 10: loss 9.5773, lr 0.00055, time 560.43ms, mfu 32.98%
iter 20: loss 9.4172, lr 0.00104, time 557.18ms, mfu 33.00%
iter 30: loss 9.3344, lr 0.00154, time 575.28ms, mfu 32.91%
iter 40: loss 9.1363, lr 0.00204, time 573.48ms, mfu 32.84%
iter 50: loss 9.0493, lr 0.00254, time 581.31ms, mfu 32.73%
iter 60: loss 8.9208, lr 0.00303, time 582.38ms, mfu 32.63%
iter 70: loss 8.8747, lr 0.00353, time 582.71ms, mfu 32.54%
iter 80: loss 8.6429, lr 0.00403, time 581.20ms, mfu 32.47%
iter 90: loss 8.5869, lr 0.00453, time 573.20ms, mfu 32.45%
step 100: train loss 8.5510, val loss 8.5789
saving checkpoint to out
merge_lora_weights 8.57887077331543 8.0 500
iter 100: loss 8.5925, lr 0.00502, time 4840.60ms, mfu 29.58%
iter 110: loss 8.4266, lr 0.00552, time 571.43ms, mfu 29.86%
iter 120: loss 8.2791, lr 0.00602, time 575.51ms, mfu 30.08%
iter 130: loss 8.3550, lr 0.00652, time 568.77ms, mfu 30.32%
iter 140: loss 8.2103, lr 0.00701, time 576.18ms, mfu 30.50%
iter 150: loss 8.2674, lr 0.00751, time 573.77ms, mfu 30.67%
iter 160: loss 8.2033, lr 0.00801, time 577.31ms, mfu 30.80%
iter 170: loss 8.1886, lr 0.00851, time 575.01ms, mfu 30.94%
iter 180: loss 8.0045, lr 0.00900, time 576.70ms, mfu 31.05%
iter 190: loss 7.9470, lr 0.00950, time 576.04ms, mfu 31.15%
step 200: train loss 7.9389, val loss 7.9800
saving checkpoint to out

=== SCALING OPERATION TRIGGERED (DDP SYNC) ===
Operation: merge_lora_weights
Trigger reason: Loss threshold
Current val loss: 7.9800, Trigger loss: 8.0000
Iterations since last op: 200, Max wait: 500
Executing operation: merge_lora_weights first burn with value: None
Performing architectural operation: merge_lora_weights
Merging LoRA weights into main weights...
LoRA weights merged and reset.

Detailed parameter count:
  total                  | Total:   52,149,504 | Trainable:    8,207,616
  token_embeddings       | Total:    2,451,456 | Trainable:    2,451,456
  position_embeddings    | Total:            0 | Trainable:            0
  attention_layers       | Total:    7,520,256 | Trainable:    2,211,840
  feed_forward_layers    | Total:    3,538,944 | Trainable:    3,538,944
  layer_norms            | Total:        4,608 | Trainable:        4,608
  final_layer_norm       | Total:          768 | Trainable:          768
  language_model_head    | Total:   38,633,472 | Trainable:            0
------------------------------------------------------------
Re-configuring optimizer after architectural change...
num decayed parameter tensors: 17, with 8,202,240 parameters
num non-decayed parameter tensors: 7, with 5,376 parameters
using fused AdamW: True
Transferring optimizer state for existing parameters...
Transferred optimizer state for 0 / 28 parameters
Re-compiling the model...
Architectural operation completed successfully.
=== SCALING OPERATION COMPLETE ===


=== SCALING OPERATION TRIGGERED (DDP SYNC) ===
Operation: change_lr
Trigger reason: Loss threshold
Current val loss: 7.9800, Trigger loss: 100.0000
Iterations since last op: 0, Max wait: 1
Executing operation: change_lr  with value: 0.7
LR multiplier: 10.0000 -> 7.0000
=== SCALING OPERATION COMPLETE ===


=== SCALING OPERATION TRIGGERED (DDP SYNC) ===
Operation: reset_lr_schedule
Trigger reason: Loss threshold
Current val loss: 7.9800, Trigger loss: 100.0000
Iterations since last op: 0, Max wait: 1
Executing operation: reset_lr_schedule  with value: None
LR schedule offset: 0 -> 200
=== SCALING OPERATION COMPLETE ===

merge_lora_weights 7.980018615722656 7.4 300
iter 200: loss 10.9286, lr 0.01000, time 4700.08ms, mfu 28.43%
iter 210: loss 8.3835, lr 0.00038, time 579.22ms, mfu 28.78%
iter 220: loss 8.2643, lr 0.00073, time 577.83ms, mfu 29.10%
iter 230: loss 7.9929, lr 0.00108, time 579.42ms, mfu 29.38%
iter 240: loss 7.9682, lr 0.00143, time 584.91ms, mfu 29.60%
iter 250: loss 7.8561, lr 0.00178, time 582.42ms, mfu 29.81%
iter 260: loss 7.9363, lr 0.00212, time 579.05ms, mfu 30.02%
iter 270: loss 7.8350, lr 0.00247, time 581.13ms, mfu 30.20%
iter 280: loss 7.7900, lr 0.00282, time 579.02ms, mfu 30.37%
iter 290: loss 7.7529, lr 0.00317, time 577.46ms, mfu 30.54%
step 300: train loss 7.7415, val loss 7.7684
saving checkpoint to out
merge_lora_weights 7.768369197845459 7.4 300
iter 300: loss 7.8101, lr 0.00352, time 5014.36ms, mfu 27.85%
iter 310: loss 7.8136, lr 0.00387, time 576.50ms, mfu 28.27%
iter 320: loss 7.7450, lr 0.00421, time 578.09ms, mfu 28.64%
iter 330: loss 7.7330, lr 0.00456, time 573.38ms, mfu 29.00%
iter 340: loss 7.4945, lr 0.00491, time 577.93ms, mfu 29.30%
iter 350: loss 7.6664, lr 0.00526, time 578.99ms, mfu 29.56%
iter 360: loss 7.7874, lr 0.00561, time 579.20ms, mfu 29.79%
iter 370: loss 7.7547, lr 0.00596, time 579.89ms, mfu 30.00%
iter 380: loss 7.5364, lr 0.00630, time 578.91ms, mfu 30.19%
iter 390: loss 7.5268, lr 0.00665, time 581.75ms, mfu 30.35%
step 400: train loss 7.6064, val loss 7.5332
saving checkpoint to out
merge_lora_weights 7.533212184906006 7.4 300
iter 400: loss 7.8049, lr 0.00700, time 4988.91ms, mfu 27.69%
iter 410: loss 7.4389, lr 0.00700, time 576.05ms, mfu 28.13%
iter 420: loss 7.5808, lr 0.00700, time 578.54ms, mfu 28.51%
iter 430: loss 7.5134, lr 0.00700, time 581.68ms, mfu 28.83%
iter 440: loss 7.4372, lr 0.00700, time 579.92ms, mfu 29.14%
iter 450: loss 7.5139, lr 0.00700, time 579.27ms, mfu 29.41%
iter 460: loss 7.5118, lr 0.00700, time 579.88ms, mfu 29.66%
iter 470: loss 7.4851, lr 0.00700, time 576.66ms, mfu 29.90%
iter 480: loss 7.3234, lr 0.00700, time 581.15ms, mfu 30.09%
iter 490: loss 7.5058, lr 0.00700, time 580.16ms, mfu 30.27%
step 500: train loss 7.4016, val loss 7.3865
saving checkpoint to out

=== SCALING OPERATION TRIGGERED (DDP SYNC) ===
Operation: merge_lora_weights
Trigger reason: Loss threshold
Current val loss: 7.3865, Trigger loss: 7.4000
Iterations since last op: 300, Max wait: 300
Executing operation: merge_lora_weights second burn with value: None
Performing architectural operation: merge_lora_weights
Merging LoRA weights into main weights...
LoRA weights merged and reset.

Detailed parameter count:
  total                  | Total:   52,149,504 | Trainable:    8,207,616
  token_embeddings       | Total:    2,451,456 | Trainable:    2,451,456
  position_embeddings    | Total:            0 | Trainable:            0
  attention_layers       | Total:    7,520,256 | Trainable:    2,211,840
  feed_forward_layers    | Total:    3,538,944 | Trainable:    3,538,944
  layer_norms            | Total:        4,608 | Trainable:        4,608
  final_layer_norm       | Total:          768 | Trainable:          768
  language_model_head    | Total:   38,633,472 | Trainable:            0
------------------------------------------------------------
Re-configuring optimizer after architectural change...
num decayed parameter tensors: 17, with 8,202,240 parameters
num non-decayed parameter tensors: 7, with 5,376 parameters
using fused AdamW: True
Transferring optimizer state for existing parameters...
Transferred optimizer state for 0 / 28 parameters
Re-compiling the model...
Architectural operation completed successfully.
=== SCALING OPERATION COMPLETE ===


=== SCALING OPERATION TRIGGERED (DDP SYNC) ===
Operation: change_lr
Trigger reason: Loss threshold
Current val loss: 7.3865, Trigger loss: 100.0000
Iterations since last op: 0, Max wait: 1
Executing operation: change_lr  with value: 0.2
LR multiplier: 7.0000 -> 1.4000
=== SCALING OPERATION COMPLETE ===


=== SCALING OPERATION TRIGGERED (DDP SYNC) ===
Operation: change_warmup_iters
Trigger reason: Loss threshold
Current val loss: 7.3865, Trigger loss: 100.0000
Iterations since last op: 0, Max wait: 1
Executing operation: change_warmup_iters  with value: 5
Warmup iters multiplier: 1.0000 -> 5.0000
=== SCALING OPERATION COMPLETE ===


=== SCALING OPERATION TRIGGERED (DDP SYNC) ===
Operation: reset_lr_schedule
Trigger reason: Loss threshold
Current val loss: 7.3865, Trigger loss: 100.0000
Iterations since last op: 0, Max wait: 1
Executing operation: reset_lr_schedule  with value: None
LR schedule offset: 200 -> 500
=== SCALING OPERATION COMPLETE ===


=== SCALING OPERATION TRIGGERED (DDP SYNC) ===
Operation: change_batch_size
Trigger reason: Loss threshold
Current val loss: 7.3865, Trigger loss: 100.0000
Iterations since last op: 0, Max wait: 1
Executing operation: change_batch_size  with value: 0.5
Batch size: 32 -> 16
=== SCALING OPERATION COMPLETE ===


=== SCALING OPERATION TRIGGERED (DDP SYNC) ===
Operation: change_grad_accum
Trigger reason: Loss threshold
Current val loss: 7.3865, Trigger loss: 100.0000
Iterations since last op: 0, Max wait: 1
Executing operation: change_grad_accum  with value: 2.0
Grad accum steps: 2 -> 4
=== SCALING OPERATION COMPLETE ===


=== SCALING OPERATION TRIGGERED (DDP SYNC) ===
Operation: change_lr
Trigger reason: Loss threshold
Current val loss: 7.3865, Trigger loss: 100.0000
Iterations since last op: 0, Max wait: 1
Executing operation: change_lr  with value: 0.7
LR multiplier: 1.4000 -> 0.9800
=== SCALING OPERATION COMPLETE ===


=== SCALING OPERATION TRIGGERED (DDP SYNC) ===
Operation: reset_lr_schedule
Trigger reason: Loss threshold
Current val loss: 7.3865, Trigger loss: 100.0000
Iterations since last op: 0, Max wait: 1
Executing operation: reset_lr_schedule  with value: None
LR schedule offset: 500 -> 500
=== SCALING OPERATION COMPLETE ===


=== SCALING OPERATION TRIGGERED (DDP SYNC) ===
Operation: stack_layers
Trigger reason: Loss threshold
Current val loss: 7.3865, Trigger loss: 100.0000
Iterations since last op: 0, Max wait: 1
Executing operation: stack_layers first resize with value: 2
Performing architectural operation: stack_layers
Stacking layers: current depth 3, creating 6 total layers.
Model now has 6 layers.

Detailed parameter count:
  total                  | Total:   63,213,312 | Trainable:   13,963,008
  token_embeddings       | Total:    2,451,456 | Trainable:    2,451,456
  position_embeddings    | Total:            0 | Trainable:            0
  attention_layers       | Total:   15,040,512 | Trainable:    4,423,680
  feed_forward_layers    | Total:    7,077,888 | Trainable:    7,077,888
  layer_norms            | Total:        9,216 | Trainable:        9,216
  final_layer_norm       | Total:          768 | Trainable:          768
  language_model_head    | Total:   38,633,472 | Trainable:            0
------------------------------------------------------------
Re-configuring optimizer after architectural change...
num decayed parameter tensors: 32, with 13,953,024 parameters
num non-decayed parameter tensors: 13, with 9,984 parameters
using fused AdamW: True
Transferring optimizer state for existing parameters...
Transferred optimizer state for 0 / 52 parameters
Re-compiling the model...
Architectural operation completed successfully.
Re-evaluating validation loss after operation...
New val loss after operation: 8.3341
iter 500: loss 8.3008, lr 0.00700, time 20312.65ms, mfu 27.35%
iter 510: loss 7.7522, lr 0.00001, time 754.71ms, mfu 27.75%
iter 520: loss 7.7180, lr 0.00002, time 764.17ms, mfu 28.06%
iter 530: loss 7.4366, lr 0.00003, time 773.00ms, mfu 28.31%
iter 540: loss 7.6239, lr 0.00004, time 776.55ms, mfu 28.52%
iter 550: loss 7.6117, lr 0.00005, time 771.82ms, mfu 28.73%
iter 560: loss 7.4068, lr 0.00006, time 764.31ms, mfu 28.94%
iter 570: loss 7.2600, lr 0.00007, time 761.66ms, mfu 29.15%
iter 580: loss 7.3459, lr 0.00008, time 765.92ms, mfu 29.32%
iter 590: loss 7.2606, lr 0.00009, time 762.99ms, mfu 29.48%
step 600: train loss 7.3227, val loss 7.2950
saving checkpoint to out
merge_lora_weights 7.294989109039307 7.0 700
iter 600: loss 7.2384, lr 0.00010, time 3547.75ms, mfu 27.20%
iter 610: loss 7.3601, lr 0.00011, time 756.05ms, mfu 27.60%
iter 620: loss 7.0590, lr 0.00012, time 761.01ms, mfu 27.94%
iter 630: loss 7.2303, lr 0.00013, time 762.17ms, mfu 28.25%
iter 640: loss 7.1883, lr 0.00014, time 762.83ms, mfu 28.52%
iter 650: loss 7.0612, lr 0.00015, time 770.77ms, mfu 28.73%
iter 660: loss 7.3343, lr 0.00016, time 764.74ms, mfu 28.94%
iter 670: loss 7.4503, lr 0.00017, time 768.46ms, mfu 29.12%
iter 680: loss 7.1299, lr 0.00018, time 767.93ms, mfu 29.28%
iter 690: loss 7.1205, lr 0.00019, time 767.47ms, mfu 29.43%
step 700: train loss 7.1765, val loss 7.2385
saving checkpoint to out
merge_lora_weights 7.238474369049072 7.0 700
iter 700: loss 7.3441, lr 0.00020, time 3399.96ms, mfu 27.18%
iter 710: loss 7.0345, lr 0.00021, time 766.69ms, mfu 27.54%
iter 720: loss 7.2386, lr 0.00022, time 768.18ms, mfu 27.86%
iter 730: loss 7.3353, lr 0.00023, time 763.74ms, mfu 28.17%
iter 740: loss 7.2182, lr 0.00024, time 761.11ms, mfu 28.45%
iter 750: loss 7.1611, lr 0.00025, time 761.62ms, mfu 28.71%
iter 760: loss 7.2739, lr 0.00026, time 759.84ms, mfu 28.94%
iter 770: loss 6.9653, lr 0.00027, time 763.24ms, mfu 29.14%
iter 780: loss 7.3067, lr 0.00028, time 760.41ms, mfu 29.33%
iter 790: loss 6.9740, lr 0.00028, time 764.54ms, mfu 29.49%
step 800: train loss 7.1305, val loss 7.1114
saving checkpoint to out
merge_lora_weights 7.111371040344238 7.0 700
iter 800: loss 7.3572, lr 0.00029, time 3667.65ms, mfu 27.18%
iter 810: loss 7.1442, lr 0.00030, time 765.40ms, mfu 27.55%
iter 820: loss 6.9787, lr 0.00031, time 767.26ms, mfu 27.87%
iter 830: loss 7.1539, lr 0.00032, time 766.07ms, mfu 28.17%
iter 840: loss 7.0911, lr 0.00033, time 762.96ms, mfu 28.44%
iter 850: loss 7.2228, lr 0.00034, time 763.34ms, mfu 28.69%
iter 860: loss 7.1046, lr 0.00035, time 766.90ms, mfu 28.90%
iter 870: loss 7.1100, lr 0.00036, time 765.38ms, mfu 29.10%
iter 880: loss 7.2919, lr 0.00037, time 766.96ms, mfu 29.26%
iter 890: loss 7.1949, lr 0.00038, time 767.24ms, mfu 29.42%
step 900: train loss 7.0922, val loss 7.1317
saving checkpoint to out
merge_lora_weights 7.131697177886963 7.0 700
iter 900: loss 7.0765, lr 0.00039, time 3486.69ms, mfu 27.15%
iter 910: loss 6.9632, lr 0.00040, time 766.63ms, mfu 27.52%
iter 920: loss 7.2567, lr 0.00041, time 761.59ms, mfu 27.86%
iter 930: loss 7.2516, lr 0.00042, time 761.33ms, mfu 28.18%
iter 940: loss 7.1069, lr 0.00043, time 762.24ms, mfu 28.46%
iter 950: loss 6.9644, lr 0.00044, time 767.68ms, mfu 28.69%
iter 960: loss 6.9879, lr 0.00045, time 765.47ms, mfu 28.90%
iter 970: loss 6.8488, lr 0.00046, time 765.58ms, mfu 29.10%
iter 980: loss 7.4322, lr 0.00047, time 767.32ms, mfu 29.26%
iter 990: loss 6.9476, lr 0.00048, time 766.13ms, mfu 29.42%
step 1000: train loss 7.0269, val loss 7.0791
saving checkpoint to out
merge_lora_weights 7.07910680770874 7.0 700
iter 1000: loss 6.8182, lr 0.00049, time 3669.49ms, mfu 27.12%
iter 1010: loss 6.8866, lr 0.00050, time 762.89ms, mfu 27.50%
iter 1020: loss 7.1552, lr 0.00051, time 767.54ms, mfu 27.83%
iter 1030: loss 6.8298, lr 0.00052, time 763.90ms, mfu 28.14%
iter 1040: loss 7.2218, lr 0.00053, time 763.81ms, mfu 28.41%
iter 1050: loss 7.0234, lr 0.00054, time 763.26ms, mfu 28.67%
iter 1060: loss 7.0004, lr 0.00055, time 762.26ms, mfu 28.90%
iter 1070: loss 7.1597, lr 0.00056, time 763.37ms, mfu 29.10%
iter 1080: loss 7.1015, lr 0.00057, time 765.52ms, mfu 29.27%
iter 1090: loss 7.2820, lr 0.00058, time 765.62ms, mfu 29.43%
step 1100: train loss 7.0407, val loss 7.0454
saving checkpoint to out
merge_lora_weights 7.045402526855469 7.0 700
iter 1100: loss 7.1905, lr 0.00059, time 3564.05ms, mfu 27.15%
iter 1110: loss 6.9452, lr 0.00060, time 765.24ms, mfu 27.52%
iter 1120: loss 6.7876, lr 0.00061, time 761.11ms, mfu 27.87%
iter 1130: loss 6.7998, lr 0.00062, time 765.70ms, mfu 28.17%
iter 1140: loss 7.1905, lr 0.00063, time 769.47ms, mfu 28.42%
iter 1150: loss 7.1696, lr 0.00064, time 767.31ms, mfu 28.65%
iter 1160: loss 7.0175, lr 0.00065, time 768.17ms, mfu 28.86%
iter 1170: loss 7.0419, lr 0.00066, time 766.38ms, mfu 29.06%
iter 1180: loss 7.0662, lr 0.00067, time 764.94ms, mfu 29.24%
iter 1190: loss 6.9521, lr 0.00068, time 768.95ms, mfu 29.38%
step 1200: train loss 7.0868, val loss 6.9408
saving checkpoint to out

=== SCALING OPERATION TRIGGERED (DDP SYNC) ===
Operation: merge_lora_weights
Trigger reason: Loss threshold
Current val loss: 6.9408, Trigger loss: 7.0000
Iterations since last op: 700, Max wait: 700
Executing operation: merge_lora_weights third burn with value: None
Performing architectural operation: merge_lora_weights
Merging LoRA weights into main weights...
LoRA weights merged and reset.

Detailed parameter count:
  total                  | Total:   63,213,312 | Trainable:   13,963,008
  token_embeddings       | Total:    2,451,456 | Trainable:    2,451,456
  position_embeddings    | Total:            0 | Trainable:            0
  attention_layers       | Total:   15,040,512 | Trainable:    4,423,680
  feed_forward_layers    | Total:    7,077,888 | Trainable:    7,077,888
  layer_norms            | Total:        9,216 | Trainable:        9,216
  final_layer_norm       | Total:          768 | Trainable:          768
  language_model_head    | Total:   38,633,472 | Trainable:            0
------------------------------------------------------------
Re-configuring optimizer after architectural change...
num decayed parameter tensors: 32, with 13,953,024 parameters
num non-decayed parameter tensors: 13, with 9,984 parameters
using fused AdamW: True
Transferring optimizer state for existing parameters...
Transferred optimizer state for 0 / 52 parameters
Re-compiling the model...
Architectural operation completed successfully.
=== SCALING OPERATION COMPLETE ===


=== SCALING OPERATION TRIGGERED (DDP SYNC) ===
Operation: change_batch_size
Trigger reason: Loss threshold
Current val loss: 6.9408, Trigger loss: 100.0000
Iterations since last op: 0, Max wait: 1
Executing operation: change_batch_size  with value: 0.5
Batch size: 16 -> 8
=== SCALING OPERATION COMPLETE ===


=== SCALING OPERATION TRIGGERED (DDP SYNC) ===
Operation: change_grad_accum
Trigger reason: Loss threshold
Current val loss: 6.9408, Trigger loss: 100.0000
Iterations since last op: 0, Max wait: 1
Executing operation: change_grad_accum  with value: 2.0
Grad accum steps: 4 -> 8
=== SCALING OPERATION COMPLETE ===


=== SCALING OPERATION TRIGGERED (DDP SYNC) ===
Operation: decrease_vocab_lora_scaling
Trigger reason: Loss threshold
Current val loss: 6.9408, Trigger loss: 100.0000
Iterations since last op: 0, Max wait: 1
Executing operation: decrease_vocab_lora_scaling  with value: 0.5
Performing architectural operation: decrease_vocab_lora_scaling
Resizing embedding LoRA rank to 24.

Detailed parameter count:
  total                  | Total:   61,987,584 | Trainable:   12,737,280
  token_embeddings       | Total:    1,225,728 | Trainable:    1,225,728
  position_embeddings    | Total:            0 | Trainable:            0
  attention_layers       | Total:   15,040,512 | Trainable:    4,423,680
  feed_forward_layers    | Total:    7,077,888 | Trainable:    7,077,888
  layer_norms            | Total:        9,216 | Trainable:        9,216
  final_layer_norm       | Total:          768 | Trainable:          768
  language_model_head    | Total:   38,633,472 | Trainable:            0
------------------------------------------------------------
Re-configuring optimizer after architectural change...
num decayed parameter tensors: 32, with 12,727,296 parameters
num non-decayed parameter tensors: 13, with 9,984 parameters
using fused AdamW: True
Transferring optimizer state for existing parameters...
Transferred optimizer state for 0 / 52 parameters
Re-compiling the model...
Architectural operation completed successfully.
Re-evaluating validation loss after operation...
W0720 11:35:46.962000 254287 /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/_inductor/utils.py:1250] [0/4] Not enough SMs to use max_autotune_gemm mode
New val loss after operation: 6.8900
iter 1200: loss 7.0371, lr 0.00069, time 41994.60ms, mfu 26.50%
iter 1210: loss 6.7320, lr 0.00070, time 749.54ms, mfu 26.95%
iter 1220: loss 7.0144, lr 0.00071, time 759.69ms, mfu 27.31%
iter 1230: loss 7.0100, lr 0.00072, time 778.23ms, mfu 27.56%
iter 1240: loss 6.9288, lr 0.00073, time 760.53ms, mfu 27.86%
iter 1250: loss 6.8254, lr 0.00074, time 756.84ms, mfu 28.14%
iter 1260: loss 7.1684, lr 0.00075, time 758.63ms, mfu 28.38%
iter 1270: loss 7.4094, lr 0.00075, time 750.22ms, mfu 28.64%
iter 1280: loss 6.8935, lr 0.00076, time 750.80ms, mfu 28.87%
iter 1290: loss 6.9689, lr 0.00077, time 752.17ms, mfu 29.07%
step 1300: train loss 6.8929, val loss 6.9014
saving checkpoint to out

=== SCALING OPERATION TRIGGERED (DDP SYNC) ===
Operation: change_lr
Trigger reason: Loss threshold
Current val loss: 6.9014, Trigger loss: 100.0000
Iterations since last op: 100, Max wait: 1
Executing operation: change_lr  with value: 0.7
LR multiplier: 0.9800 -> 0.6860
=== SCALING OPERATION COMPLETE ===


=== SCALING OPERATION TRIGGERED (DDP SYNC) ===
Operation: reset_lr_schedule
Trigger reason: Loss threshold
Current val loss: 6.9014, Trigger loss: 100.0000
Iterations since last op: 0, Max wait: 1
Executing operation: reset_lr_schedule  with value: None
LR schedule offset: 500 -> 1300
=== SCALING OPERATION COMPLETE ===


=== SCALING OPERATION TRIGGERED (DDP SYNC) ===
Operation: stack_layers
Trigger reason: Loss threshold
Current val loss: 6.9014, Trigger loss: 100.0000
Iterations since last op: 0, Max wait: 1
Executing operation: stack_layers third resize with value: 2
Performing architectural operation: stack_layers
Stacking layers: current depth 6, creating 12 total layers.
Model now has 12 layers.

Detailed parameter count:
  total                  | Total:   84,115,200 | Trainable:   24,248,064
  token_embeddings       | Total:    1,225,728 | Trainable:    1,225,728
  position_embeddings    | Total:            0 | Trainable:            0
  attention_layers       | Total:   30,081,024 | Trainable:    8,847,360
  feed_forward_layers    | Total:   14,155,776 | Trainable:   14,155,776
  layer_norms            | Total:       18,432 | Trainable:       18,432
  final_layer_norm       | Total:          768 | Trainable:          768
  language_model_head    | Total:   38,633,472 | Trainable:            0
------------------------------------------------------------
Re-configuring optimizer after architectural change...
num decayed parameter tensors: 62, with 24,228,864 parameters
num non-decayed parameter tensors: 25, with 19,200 parameters
using fused AdamW: True
Transferring optimizer state for existing parameters...
Transferred optimizer state for 0 / 100 parameters
Re-compiling the model...
Architectural operation completed successfully.
Re-evaluating validation loss after operation...
New val loss after operation: 7.3038
iter 1300: loss 7.4829, lr 0.00078, time 65320.56ms, mfu 26.21%
iter 1310: loss 7.2745, lr 0.00001, time 1122.36ms, mfu 26.57%
iter 1320: loss 6.9715, lr 0.00001, time 1146.08ms, mfu 26.83%
iter 1330: loss 7.1527, lr 0.00002, time 1175.89ms, mfu 27.00%
iter 1340: loss 7.0283, lr 0.00003, time 1158.46ms, mfu 27.19%
iter 1350: loss 6.9288, lr 0.00003, time 1144.82ms, mfu 27.39%
iter 1360: loss 6.8759, lr 0.00004, time 1117.59ms, mfu 27.65%
iter 1370: loss 6.8325, lr 0.00005, time 1138.57ms, mfu 27.82%
iter 1380: loss 7.1561, lr 0.00006, time 1138.15ms, mfu 27.98%
iter 1390: loss 7.3432, lr 0.00006, time 1123.13ms, mfu 28.16%
step 1400: train loss 7.0317, val loss 7.0458
saving checkpoint to out
merge_lora_weights 7.045823574066162 6.0 600
iter 1400: loss 6.7129, lr 0.00007, time 4197.45ms, mfu 26.14%
iter 1410: loss 7.0216, lr 0.00008, time 1149.97ms, mfu 26.44%
iter 1420: loss 7.1274, lr 0.00008, time 1158.71ms, mfu 26.68%
iter 1430: loss 7.0706, lr 0.00009, time 1159.48ms, mfu 26.90%
iter 1440: loss 7.1342, lr 0.00010, time 1156.76ms, mfu 27.11%
iter 1450: loss 6.9976, lr 0.00010, time 1149.52ms, mfu 27.31%
iter 1460: loss 7.0366, lr 0.00011, time 1139.44ms, mfu 27.51%
iter 1470: loss 6.7628, lr 0.00012, time 1140.02ms, mfu 27.70%
iter 1480: loss 6.7920, lr 0.00012, time 1140.77ms, mfu 27.86%
iter 1490: loss 6.9567, lr 0.00013, time 1140.52ms, mfu 28.01%
step 1500: train loss 6.9709, val loss 6.9719
saving checkpoint to out
merge_lora_weights 6.9718756675720215 6.0 600
iter 1500: loss 7.0460, lr 0.00014, time 4028.49ms, mfu 26.04%
iter 1510: loss 6.6686, lr 0.00014, time 1153.22ms, mfu 26.34%
iter 1520: loss 7.0838, lr 0.00015, time 1151.06ms, mfu 26.61%
iter 1530: loss 7.2640, lr 0.00016, time 1146.68ms, mfu 26.87%
iter 1540: loss 7.1853, lr 0.00017, time 1158.68ms, mfu 27.07%
iter 1550: loss 6.6599, lr 0.00017, time 1142.35ms, mfu 27.29%
iter 1560: loss 6.8392, lr 0.00018, time 1140.76ms, mfu 27.50%
iter 1570: loss 6.6703, lr 0.00019, time 1138.07ms, mfu 27.69%
iter 1580: loss 7.1831, lr 0.00019, time 1135.37ms, mfu 27.87%
iter 1590: loss 6.5950, lr 0.00020, time 1146.45ms, mfu 28.00%
step 1600: train loss 6.9232, val loss 6.9250
saving checkpoint to out
merge_lora_weights 6.924999237060547 6.0 600
iter 1600: loss 7.0020, lr 0.00021, time 3794.08ms, mfu 26.08%
iter 1610: loss 6.8497, lr 0.00021, time 1137.51ms, mfu 26.42%
iter 1620: loss 6.8020, lr 0.00022, time 1150.67ms, mfu 26.68%
iter 1630: loss 7.0151, lr 0.00023, time 1154.05ms, mfu 26.92%
iter 1640: loss 7.2558, lr 0.00023, time 1143.67ms, mfu 27.15%
iter 1650: loss 7.3568, lr 0.00024, time 1145.26ms, mfu 27.36%
iter 1660: loss 6.8905, lr 0.00025, time 1143.38ms, mfu 27.55%
iter 1670: loss 6.9619, lr 0.00025, time 1158.85ms, mfu 27.68%
iter 1680: loss 7.0853, lr 0.00026, time 1142.19ms, mfu 27.84%
iter 1690: loss 6.8794, lr 0.00027, time 1139.74ms, mfu 28.00%
step 1700: train loss 6.9893, val loss 6.9584
saving checkpoint to out
merge_lora_weights 6.958372592926025 6.0 600
iter 1700: loss 7.0397, lr 0.00027, time 4066.75ms, mfu 26.02%
iter 1710: loss 6.9833, lr 0.00028, time 1132.82ms, mfu 26.37%
iter 1720: loss 6.5951, lr 0.00029, time 1146.71ms, mfu 26.65%
iter 1730: loss 7.0689, lr 0.00030, time 1145.22ms, mfu 26.91%
iter 1740: loss 6.9170, lr 0.00030, time 1156.59ms, mfu 27.11%
iter 1750: loss 6.7031, lr 0.00031, time 1157.96ms, mfu 27.29%
iter 1760: loss 6.9820, lr 0.00032, time 1151.59ms, mfu 27.47%
iter 1770: loss 6.8193, lr 0.00032, time 1144.09ms, mfu 27.65%
iter 1780: loss 6.7743, lr 0.00033, time 1133.32ms, mfu 27.84%
iter 1790: loss 6.9128, lr 0.00034, time 1141.72ms, mfu 27.98%
step 1800: train loss 6.8507, val loss 7.0007
saving checkpoint to out
merge_lora_weights 7.0006561279296875 6.0 600
iter 1800: loss 7.0982, lr 0.00034, time 3120.25ms, mfu 26.26%
iter 1810: loss 6.9470, lr 0.00035, time 1149.58ms, mfu 26.54%
iter 1820: loss 6.9489, lr 0.00036, time 1155.47ms, mfu 26.79%
iter 1830: loss 7.0713, lr 0.00036, time 1155.75ms, mfu 27.00%
iter 1840: loss 6.9134, lr 0.00037, time 1144.71ms, mfu 27.23%
iter 1850: loss 6.6617, lr 0.00038, time 1153.46ms, mfu 27.41%
iter 1860: loss 6.9626, lr 0.00038, time 1127.15ms, mfu 27.63%
iter 1870: loss 7.0344, lr 0.00039, time 1142.99ms, mfu 27.80%
iter 1880: loss 6.4427, lr 0.00040, time 1144.46ms, mfu 27.94%
iter 1890: loss 7.1080, lr 0.00041, time 1145.95ms, mfu 28.07%
step 1900: train loss 6.9975, val loss 6.9238
saving checkpoint to out

=== SCALING OPERATION TRIGGERED (DDP SYNC) ===
Operation: merge_lora_weights
Trigger reason: Timeout
Current val loss: 6.9238, Trigger loss: 6.0000
Iterations since last op: 600, Max wait: 600
Executing operation: merge_lora_weights fourth burn with value: None
Performing architectural operation: merge_lora_weights
Merging LoRA weights into main weights...
LoRA weights merged and reset.

Detailed parameter count:
  total                  | Total:   84,115,200 | Trainable:   24,248,064
  token_embeddings       | Total:    1,225,728 | Trainable:    1,225,728
  position_embeddings    | Total:            0 | Trainable:            0
  attention_layers       | Total:   30,081,024 | Trainable:    8,847,360
  feed_forward_layers    | Total:   14,155,776 | Trainable:   14,155,776
  layer_norms            | Total:       18,432 | Trainable:       18,432
  final_layer_norm       | Total:          768 | Trainable:          768
  language_model_head    | Total:   38,633,472 | Trainable:            0
------------------------------------------------------------
Re-configuring optimizer after architectural change...
num decayed parameter tensors: 62, with 24,228,864 parameters
num non-decayed parameter tensors: 25, with 19,200 parameters
using fused AdamW: True
Transferring optimizer state for existing parameters...
Transferred optimizer state for 0 / 100 parameters
Re-compiling the model...
Architectural operation completed successfully.
=== SCALING OPERATION COMPLETE ===


=== SCALING OPERATION TRIGGERED (DDP SYNC) ===
Operation: widen_mlp
Trigger reason: Loss threshold
Current val loss: 6.9238, Trigger loss: 100.0000
Iterations since last op: 0, Max wait: 1
Executing operation: widen_mlp second resize with value: 2
Performing architectural operation: widen_mlp
Widening MLP layers by a factor of 2.
MLP hidden dimension widened to 1536.

Detailed parameter count:
  total                  | Total:   98,270,976 | Trainable:   38,403,840
  token_embeddings       | Total:    1,225,728 | Trainable:    1,225,728
  position_embeddings    | Total:            0 | Trainable:            0
  attention_layers       | Total:   30,081,024 | Trainable:    8,847,360
  feed_forward_layers    | Total:   28,311,552 | Trainable:   28,311,552
  layer_norms            | Total:       18,432 | Trainable:       18,432
  final_layer_norm       | Total:          768 | Trainable:          768
  language_model_head    | Total:   38,633,472 | Trainable:            0
------------------------------------------------------------
Re-configuring optimizer after architectural change...
num decayed parameter tensors: 62, with 38,384,640 parameters
num non-decayed parameter tensors: 25, with 19,200 parameters
using fused AdamW: True
Transferring optimizer state for existing parameters...
Transferred optimizer state for 0 / 100 parameters
Re-compiling the model...
Architectural operation completed successfully.
Re-evaluating validation loss after operation...
New val loss after operation: 6.8265
iter 1900: loss 7.0424, lr 0.00041, time 67676.26ms, mfu 25.32%
iter 1910: loss 6.6534, lr 0.00042, time 1282.43ms, mfu 25.76%
iter 1920: loss 6.9618, lr 0.00043, time 1307.66ms, mfu 26.09%
iter 1930: loss 6.8340, lr 0.00043, time 1308.47ms, mfu 26.39%
iter 1940: loss 6.7713, lr 0.00044, time 1298.40ms, mfu 26.68%
Traceback (most recent call last):
  File "/teamspace/studios/this_studio/nanoGPT/train.py", line 657, in <module>
    scaler.scale(loss).backward()
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/_tensor.py", line 648, in backward
    torch.autograd.backward(
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/autograd/__init__.py", line 353, in backward
    _engine_run_backward(
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/autograd/graph.py", line 824, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
KeyboardInterrupt
Traceback (most recent call last):
  File "/teamspace/studios/this_studio/nanoGPT/train.py", line 657, in <module>
    scaler.scale(loss).backward()
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/_tensor.py", line 648, in backward
    torch.autograd.backward(
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/autograd/__init__.py", line 353, in backward
    _engine_run_backward(
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/autograd/graph.py", line 824, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
KeyboardInterrupt
Exception ignored in atexit callback: <function _start_and_connect_service.<locals>.teardown_atexit at 0x7b79096fc8b0>
Traceback (most recent call last):
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/wandb/sdk/lib/service/service_connection.py", line 54, in teardown_atexit
    conn.teardown(hooks.exit_code)
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/wandb/sdk/lib/service/service_connection.py", line 182, in teardown
    self._router.join()
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/wandb/sdk/interface/router.py", line 75, in join
    self._thread.join()
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/threading.py", line 1096, in join
    self._wait_for_tstate_lock()
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/threading.py", line 1116, in _wait_for_tstate_lock
    if lock.acquire(block, timeout):
KeyboardInterrupt:
Exception ignored in atexit callback: <function shutdown_compile_workers at 0x7b793aab0670>
Traceback (most recent call last):
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/_inductor/async_compile.py", line 113, in shutdown_compile_workers
    pool.shutdown()
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/_inductor/compile_worker/subproc_pool.py", line 239, in shutdown
    self.process.wait(300)
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/subprocess.py", line 1209, in wait
    return self._wait(timeout=timeout)
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/subprocess.py", line 1937, in _wait

