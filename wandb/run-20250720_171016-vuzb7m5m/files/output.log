Calculating target architecture based on schedule...

============================================================
       TARGET MODEL ARCHITECTURE (at end of schedule)
============================================================
  n_layer                | 6
  n_head                 | 12
  n_embd                 | 768
  n_hidden               | 1536
  block_size             | 1024
  vocab_size             | 50304
  dropout                | 0.0
  bias                   | False
  embedding_mode         | standard
  attn_lora_rank         | 0
  embedding_rank         | 0
  lora_alpha             | 0.0
============================================================


============================================================
           INITIAL MODEL ARCHITECTURE (at Iter 0)
============================================================
  n_layer                | 3
  n_head                 | 12
  n_embd                 | 768
  n_hidden               | 768
  block_size             | 1024
  vocab_size             | 50304
  dropout                | 0.0
  bias                   | False
  embedding_mode         | lora
  attn_lora_rank         | 48
  embedding_rank         | 48
  lora_alpha             | 1.0
============================================================


Detailed parameter count:
  total                  | Total:   52,149,504 | Trainable:    8,207,616
  token_embeddings       | Total:    2,451,456 | Trainable:    2,451,456
  position_embeddings    | Total:            0 | Trainable:            0
  attention_layers       | Total:    7,520,256 | Trainable:    2,211,840
  feed_forward_layers    | Total:    3,538,944 | Trainable:    3,538,944
  layer_norms            | Total:        4,608 | Trainable:        4,608
  final_layer_norm       | Total:          768 | Trainable:          768
  language_model_head    | Total:   38,633,472 | Trainable:            0
------------------------------------------------------------
eval every:100
step 0: train loss 10.9663, val loss 10.9694
  MLP Rank Utilization (L0): 71.09% (546/768)
Attn LoRA    | 47/48           | 97.92%
  MLP Rank Utilization (L1): 71.09% (546/768)
Attn LoRA    | 47/48           | 97.92%
  MLP Rank Utilization (L2): 71.22% (547/768)
Attn LoRA    | 47/48           | 97.92%
--- Model Analysis ---
  Embedding Utilization (L2): 97.53% (749/768)
  Average Attention Entropy:  5.8975
----------------------
Embed LoRA   | 47/48           | 97.92%
merge_lora_weights 10.969396591186523 8.0 1500
iter 0: loss 10.9733, lr 0.00005, time 9862.33ms, mfu -100.00%
iter 10: loss 9.6177, lr 0.00055, time 562.40ms, mfu 32.86%
iter 20: loss 9.4793, lr 0.00104, time 562.99ms, mfu 32.86%
iter 30: loss 9.2850, lr 0.00154, time 572.09ms, mfu 32.80%
iter 40: loss 9.1316, lr 0.00204, time 574.45ms, mfu 32.74%
iter 50: loss 9.0282, lr 0.00254, time 579.72ms, mfu 32.65%
[34m[1mwandb[0m: [33mWARNING[0m Fatal error while uploading data. Some run data will not be synced, but it will still be written to disk. Use `wandb sync` at the end of the run to try uploading.
iter 60: loss 8.8160, lr 0.00303, time 582.50ms, mfu 32.56%
iter 70: loss 8.8574, lr 0.00353, time 589.32ms, mfu 32.44%
iter 80: loss 8.6071, lr 0.00403, time 586.02ms, mfu 32.35%
iter 90: loss 8.5780, lr 0.00453, time 581.96ms, mfu 32.29%
step 100: train loss 8.5895, val loss 8.5851
  MLP Rank Utilization (L0): 64.84% (498/768)
Attn LoRA    | 2/48            | 4.17%
  MLP Rank Utilization (L1): 66.80% (513/768)
Attn LoRA    | 2/48            | 4.17%
  MLP Rank Utilization (L2): 67.84% (521/768)
Attn LoRA    | 1/48            | 2.08%
--- Model Analysis ---
  Embedding Utilization (L2): 97.53% (749/768)
  Average Attention Entropy:  5.5171
----------------------
Embed LoRA   | 2/48            | 4.17%
saving checkpoint to out
merge_lora_weights 8.585118293762207 8.0 1500
iter 100: loss 8.5475, lr 0.00502, time 6172.68ms, mfu 29.36%
iter 110: loss 8.5692, lr 0.00552, time 568.31ms, mfu 29.68%
iter 120: loss 8.3751, lr 0.00602, time 577.21ms, mfu 29.91%
iter 130: loss 8.2824, lr 0.00652, time 573.49ms, mfu 30.14%
iter 140: loss 8.3121, lr 0.00701, time 569.12ms, mfu 30.37%
iter 150: loss 8.2481, lr 0.00751, time 566.46ms, mfu 30.60%
iter 160: loss 8.1948, lr 0.00801, time 568.64ms, mfu 30.79%
iter 170: loss 8.1211, lr 0.00851, time 567.83ms, mfu 30.97%
iter 180: loss 8.0794, lr 0.00900, time 575.24ms, mfu 31.08%
