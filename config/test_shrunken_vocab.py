# Test configuration for shrunken vocabulary training feature
# This config demonstrates the shrunken vocabulary feature with a small model for testing

import torch

# Basic training settings
wandb_log = False # Disable wandb for testing
eval_interval = 100 # Evaluate more frequently for testing
log_interval = 10
eval_iters = 10 # Fewer eval iterations for faster testing
eval_only = False
always_save_checkpoint = False # Don't save checkpoints during testing
init_from = 'scratch'

# Data settings
dataset = 'fineweb10B' # Use the dataset that has the token mapping
batch_size = 4 # Small batch size for testing
block_size = 256 # Smaller block size for testing
gradient_accumulation_steps = 2

# Model settings - small model for testing
n_layer = 4
n_head = 4
n_embd = 256
dropout = 0.0
bias = False
n_hidden = None # Will default to 4 * n_embd = 512

# Training settings
learning_rate = 1e-3 # Higher learning rate for faster testing
max_iters = 500 # Short training for testing
weight_decay = 1e-2
beta1 = 0.9
beta2 = 0.95
grad_clip = 1.0

# Learning rate decay settings
decay_lr = True
warmup_iters = 50 # Short warmup for testing
lr_decay_iters = 400 # Most of the training
min_lr = 1e-4

# System settings
device = 'cuda' if torch.cuda.is_available() else 'cpu'
dtype = 'bfloat16' if torch.cuda.is_available() and torch.cuda.is_bf16_supported() else 'float16'
compile = False # Disable compilation for testing to avoid overhead

# LoRA settings (disabled for this test)
embedding_mode = 'standard'
attn_lora_rank = 0
embedding_rank = 0
lora_alpha = 1.0

# Shrunken vocabulary settings - THE KEY FEATURE BEING TESTED
shrunken_vocab_size = 11199 # Size from the token mapping file
vocab_remapping_file = 'data/vocab_remapping.pt' # Generated by our script
RARE_TOKEN_ID = 11198 # The rare token ID from the mapping

# Output directory for test
out_dir = 'out-test-shrunken-vocab'
