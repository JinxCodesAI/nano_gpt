_wandb:
    value:
        cli_version: 0.21.0
        e:
            qflzw1d245iv2wybnuk19bkdwxlvtgoq:
                args:
                    - config/train_gpt2_fast.py
                codePath: train.py
                codePathLocal: train.py
                cpu_count: 4
                cpu_count_logical: 8
                cudaVersion: "12.2"
                disk:
                    /:
                        total: "395184570368"
                        used: "46117896192"
                email: adamskrodzki@gmail.com
                executable: /home/zeus/miniconda3/envs/cloudspace/bin/python
                git:
                    commit: d9b42705b8d5cf57261ec46f0edcd3529562a3f4
                    remote: https://github.com/JinxCodesAI/nano_gpt.git
                gpu: NVIDIA L4
                gpu_count: 1
                gpu_nvidia:
                    - architecture: Ada
                      cudaCores: 7424
                      memoryTotal: "24152899584"
                      name: NVIDIA L4
                      uuid: GPU-1ec5735b-7c52-3d5f-98ce-f0a5a13f2fb7
                host: cs-01k0hnsy8ny8br925q52a63mtv
                memory:
                    total: "33652187136"
                os: Linux-6.8.0-1032-gcp-x86_64-with-glibc2.31
                program: /teamspace/studios/this_studio/nanoGPT/train.py
                python: CPython 3.10.10
                root: /teamspace/studios/this_studio/nanoGPT
                startedAt: "2025-07-19T17:24:17.007228Z"
                writerId: qflzw1d245iv2wybnuk19bkdwxlvtgoq
        m: []
        python_version: 3.10.10
        t:
            "1":
                - 1
            "2":
                - 1
            "3":
                - 13
                - 16
            "4": 3.10.10
            "5": 0.21.0
            "12": 0.21.0
            "13": linux-x86_64
always_save_checkpoint:
    value: true
attn_lora_rank_divisor:
    value: 0
backend:
    value: nccl
batch_size:
    value: 12
batch_size_multiplier:
    value: 1
beta1:
    value: 0.9
beta2:
    value: 0.95
bias:
    value: false
block_size:
    value: 1024
compile:
    value: true
dataset:
    value: fineweb10B
decay_lr:
    value: true
device:
    value: cuda
dropout:
    value: 0
dtype:
    value: bfloat16
eval_interval:
    value: 1000
eval_interval_multiplier:
    value: 0.04
eval_iters:
    value: 200
eval_iters_multiplier:
    value: 0.05
eval_only:
    value: false
file_logging:
    value: true
grad_accum_multiplier:
    value: 0.01
grad_clip:
    value: 1
gradient_accumulation_steps:
    value: 5
init_from:
    value: scratch
learning_rate:
    value: 0.0006
log_dir:
    value: logs
log_interval:
    value: 10
lora_alpha_multiplier:
    value: 1
lr_decay_iters:
    value: 600000
lr_multiplier:
    value: 10
max_iters:
    value: 600000
min_lr:
    value: 6e-05
n_embd:
    value: 768
n_head:
    value: 12
n_hidden_divisor:
    value: 1
n_layer:
    value: 12
n_layer_divisor:
    value: 1
out_dir:
    value: out
rotary_base:
    value: 10000
rotary_max_position_embeddings:
    value: 2048
use_rotary_embeddings:
    value: true
vocab_lora_rank_divisor:
    value: 0
wandb_log:
    value: true
wandb_project:
    value: owt
wandb_run_name:
    value: gpt2-124M-fast
warmup_iters:
    value: 2000
warmup_iters_multiplier:
    value: 1
weight_decay:
    value: 0.1
