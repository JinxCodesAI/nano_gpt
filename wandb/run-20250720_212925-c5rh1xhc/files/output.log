Calculating target architecture based on schedule...

============================================================
       TARGET MODEL ARCHITECTURE (at end of schedule)
============================================================
  n_layer                | 2
  n_head                 | 12
  n_embd                 | 768
  n_hidden               | 768
  block_size             | 1024
  vocab_size             | 50304
  dropout                | 0.0
  bias                   | False
  embedding_mode         | standard
  attn_lora_rank         | 0
  embedding_rank         | 0
  lora_alpha             | 0.0
============================================================


============================================================
           INITIAL MODEL ARCHITECTURE (at Iter 0)
============================================================
  n_layer                | 1
  n_head                 | 12
  n_embd                 | 768
  n_hidden               | 768
  block_size             | 1024
  vocab_size             | 50304
  dropout                | 0.0
  bias                   | False
  embedding_mode         | lora
  attn_lora_rank         | 0
  embedding_rank         | 0
  lora_alpha             | 1.0
============================================================


Detailed parameter count:
  total                  | Total:   42,174,720 | Trainable:   42,174,720
  token_embeddings       | Total:            0 | Trainable:            0
  position_embeddings    | Total:            0 | Trainable:            0
  attention_layers       | Total:    2,359,296 | Trainable:    2,359,296
  feed_forward_layers    | Total:    1,179,648 | Trainable:    1,179,648
  layer_norms            | Total:        1,536 | Trainable:        1,536
  final_layer_norm       | Total:          768 | Trainable:          768
  language_model_head    | Total:   38,633,472 | Trainable:   38,633,472
------------------------------------------------------------
eval every:200
step 400: train loss 6.4513, val loss 6.3592
raw_model.config.n_layer=1
doing something 0
  MLP Rank Utilization (L0): 71.09% (546/768)
--- Model Analysis ---
  Embedding Utilization: 97.14% (746/768)
  Average Attention Entropy:  5.5033
----------------------

=== SCALING OPERATION TRIGGERED (DDP SYNC) ===
Operation: reset_lr_schedule
Trigger reason: Loss threshold
Current val loss: 6.3592, Trigger loss: 100.0000
Iterations since last op: 400, Max wait: 1
Executing operation: reset_lr_schedule  with value: None
LR schedule offset: 0 -> 400
=== SCALING OPERATION COMPLETE ===


=== SCALING OPERATION TRIGGERED (DDP SYNC) ===
Operation: change_batch_size
Trigger reason: Loss threshold
Current val loss: 6.3592, Trigger loss: 100.0000
Iterations since last op: 0, Max wait: 1
Executing operation: change_batch_size  with value: 0.5
Batch size: 32 -> 16
=== SCALING OPERATION COMPLETE ===


=== SCALING OPERATION TRIGGERED (DDP SYNC) ===
Operation: change_grad_accum
Trigger reason: Loss threshold
Current val loss: 6.3592, Trigger loss: 100.0000
Iterations since last op: 0, Max wait: 1
Executing operation: change_grad_accum  with value: 2.0
Grad accum steps: 2 -> 4
=== SCALING OPERATION COMPLETE ===


=== SCALING OPERATION TRIGGERED (DDP SYNC) ===
Operation: stack_layers
Trigger reason: Loss threshold
Current val loss: 6.3592, Trigger loss: 100.0000
Iterations since last op: 0, Max wait: 1
Executing operation: stack_layers first burn with value: 2
Performing architectural operation: stack_layers
Stacking layers: current depth 1, creating 2 total layers.
Model now has 2 layers.

Detailed parameter count:
  total                  | Total:   45,715,200 | Trainable:   45,715,200
  token_embeddings       | Total:            0 | Trainable:            0
  position_embeddings    | Total:            0 | Trainable:            0
  attention_layers       | Total:    4,718,592 | Trainable:    4,718,592
  feed_forward_layers    | Total:    2,359,296 | Trainable:    2,359,296
  layer_norms            | Total:        3,072 | Trainable:        3,072
  final_layer_norm       | Total:          768 | Trainable:          768
  language_model_head    | Total:   38,633,472 | Trainable:   38,633,472
------------------------------------------------------------
Re-configuring optimizer after architectural change...
num decayed parameter tensors: 9, with 45,711,360 parameters
num non-decayed parameter tensors: 5, with 3,840 parameters
using fused AdamW: True
Transferring optimizer state for existing parameters...
Transferred optimizer state for 0 / 14 parameters
Re-compiling the model...

============================================================
             ARCHITECTURE CHANGE (at Iter 400)
============================================================
  n_layer                | 2
  n_head                 | 12
  n_embd                 | 768
  n_hidden               | 768
  block_size             | 1024
  vocab_size             | 50304
  dropout                | 0.0
  bias                   | False
  embedding_mode         | lora
  attn_lora_rank         | 0
  embedding_rank         | 0
  lora_alpha             | 1.0
============================================================

Architectural operation completed successfully.
Re-evaluating validation loss after operation...
W0720 21:29:33.170000 82532 /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/_inductor/utils.py:1250] [0/2] Not enough SMs to use max_autotune_gemm mode
New val loss after operation: 6.4803
iter 400: loss 6.6801, lr 0.00010, time 23295.88ms, mfu -100.00%
iter 410: loss 6.5863, lr 0.00001, time 592.62ms, mfu 26.79%
iter 420: loss 6.2768, lr 0.00001, time 596.99ms, mfu 26.77%
iter 430: loss 6.4952, lr 0.00002, time 599.68ms, mfu 26.74%
iter 440: loss 6.5435, lr 0.00002, time 604.83ms, mfu 26.70%
iter 450: loss 6.3780, lr 0.00003, time 611.06ms, mfu 26.62%
iter 460: loss 6.2895, lr 0.00003, time 616.68ms, mfu 26.54%
iter 470: loss 6.3438, lr 0.00004, time 611.96ms, mfu 26.48%
iter 480: loss 6.3001, lr 0.00004, time 609.98ms, mfu 26.43%
iter 490: loss 6.4632, lr 0.00005, time 601.21ms, mfu 26.43%
iter 500: loss 6.3426, lr 0.00005, time 599.50ms, mfu 26.44%
iter 510: loss 6.0884, lr 0.00006, time 598.39ms, mfu 26.45%
iter 520: loss 6.3479, lr 0.00006, time 598.26ms, mfu 26.46%
iter 530: loss 6.1845, lr 0.00007, time 598.95ms, mfu 26.46%
iter 540: loss 6.3988, lr 0.00007, time 593.83ms, mfu 26.49%
iter 550: loss 6.1298, lr 0.00008, time 597.09ms, mfu 26.50%
iter 560: loss 6.4872, lr 0.00008, time 594.02ms, mfu 26.52%
iter 570: loss 6.1402, lr 0.00009, time 594.19ms, mfu 26.54%
iter 580: loss 6.1824, lr 0.00009, time 596.66ms, mfu 26.55%
iter 590: loss 6.2376, lr 0.00010, time 600.73ms, mfu 26.54%
step 600: train loss 6.1310, val loss 6.3114
raw_model.config.n_layer=2
doing something 0
  MLP Rank Utilization (L0): 70.96% (545/768)
doing something 1
  MLP Rank Utilization (L1): 70.96% (545/768)
--- Model Analysis ---
  Embedding Utilization: 97.01% (745/768)
  Average Attention Entropy:  4.1117
----------------------
saving checkpoint to out

=== SCALING OPERATION TRIGGERED (DDP SYNC) ===
Operation: change_lr
Trigger reason: Loss threshold
Current val loss: 6.3114, Trigger loss: 100.0000
Iterations since last op: 200, Max wait: 1
Executing operation: change_lr  with value: 0.5
LR multiplier: 1.0000 -> 0.5000
=== SCALING OPERATION COMPLETE ===


=== SCALING OPERATION TRIGGERED (DDP SYNC) ===
Operation: reset_lr_schedule
Trigger reason: Loss threshold
Current val loss: 6.3114, Trigger loss: 100.0000
Iterations since last op: 0, Max wait: 1
Executing operation: reset_lr_schedule  with value: None
LR schedule offset: 400 -> 600
=== SCALING OPERATION COMPLETE ===

merge_lora_weights 6.311356544494629 5.5 2500
iter 600: loss 6.0737, lr 0.00010, time 12296.80ms, mfu 24.01%
iter 610: loss 6.1871, lr 0.00000, time 600.23ms, mfu 24.26%
iter 620: loss 6.0769, lr 0.00001, time 604.50ms, mfu 24.46%
iter 630: loss 6.1994, lr 0.00001, time 604.44ms, mfu 24.64%
iter 640: loss 6.0358, lr 0.00001, time 608.88ms, mfu 24.78%
iter 650: loss 6.3401, lr 0.00001, time 607.08ms, mfu 24.92%
iter 660: loss 6.1134, lr 0.00002, time 609.79ms, mfu 25.03%
iter 670: loss 6.0625, lr 0.00002, time 603.74ms, mfu 25.16%
iter 680: loss 6.1695, lr 0.00002, time 603.28ms, mfu 25.28%
iter 690: loss 6.1536, lr 0.00002, time 603.49ms, mfu 25.38%
iter 700: loss 6.2343, lr 0.00003, time 600.89ms, mfu 25.48%
iter 710: loss 6.1573, lr 0.00003, time 598.29ms, mfu 25.59%
iter 720: loss 6.1643, lr 0.00003, time 598.30ms, mfu 25.68%
iter 730: loss 6.1040, lr 0.00003, time 598.46ms, mfu 25.77%
iter 740: loss 6.1328, lr 0.00004, time 598.59ms, mfu 25.84%
iter 750: loss 6.2782, lr 0.00004, time 597.63ms, mfu 25.92%
iter 760: loss 5.9817, lr 0.00004, time 596.03ms, mfu 25.99%
iter 770: loss 6.0941, lr 0.00004, time 600.82ms, mfu 26.03%
iter 780: loss 6.0693, lr 0.00005, time 600.45ms, mfu 26.07%
iter 790: loss 6.2352, lr 0.00005, time 599.90ms, mfu 26.11%
step 800: train loss 6.0283, val loss 5.9134
raw_model.config.n_layer=2
doing something 0
  MLP Rank Utilization (L0): 70.96% (545/768)
doing something 1
  MLP Rank Utilization (L1): 70.96% (545/768)
--- Model Analysis ---
  Embedding Utilization: 96.88% (744/768)
  Average Attention Entropy:  4.1385
----------------------
saving checkpoint to out
merge_lora_weights 5.913354396820068 5.5 2500
iter 800: loss 6.1316, lr 0.00005, time 4062.81ms, mfu 23.89%
iter 810: loss 6.0818, lr 0.00005, time 599.31ms, mfu 24.15%
iter 820: loss 6.1444, lr 0.00005, time 601.67ms, mfu 24.38%
iter 830: loss 6.0599, lr 0.00005, time 602.62ms, mfu 24.57%
iter 840: loss 6.1953, lr 0.00005, time 604.11ms, mfu 24.75%
iter 850: loss 6.1478, lr 0.00005, time 603.16ms, mfu 24.90%
iter 860: loss 5.9993, lr 0.00005, time 602.33ms, mfu 25.05%
iter 870: loss 6.0968, lr 0.00005, time 602.26ms, mfu 25.18%
iter 880: loss 6.0538, lr 0.00005, time 599.35ms, mfu 25.31%
iter 890: loss 6.1575, lr 0.00005, time 601.43ms, mfu 25.42%
iter 900: loss 5.9857, lr 0.00005, time 599.92ms, mfu 25.53%
iter 910: loss 6.0024, lr 0.00005, time 600.93ms, mfu 25.62%
iter 920: loss 6.1078, lr 0.00005, time 599.71ms, mfu 25.70%
iter 930: loss 5.9929, lr 0.00005, time 598.84ms, mfu 25.78%
iter 940: loss 6.1969, lr 0.00005, time 599.22ms, mfu 25.85%
iter 950: loss 5.8665, lr 0.00005, time 599.23ms, mfu 25.92%
iter 960: loss 6.1997, lr 0.00005, time 599.86ms, mfu 25.97%
iter 970: loss 5.8570, lr 0.00005, time 600.73ms, mfu 26.02%
iter 980: loss 6.0062, lr 0.00005, time 598.41ms, mfu 26.07%
iter 990: loss 5.8509, lr 0.00005, time 599.37ms, mfu 26.11%
step 1000: train loss 5.9325, val loss 5.9461
raw_model.config.n_layer=2
doing something 0
  MLP Rank Utilization (L0): 70.96% (545/768)
doing something 1
  MLP Rank Utilization (L1): 70.96% (545/768)
--- Model Analysis ---
  Embedding Utilization: 96.88% (744/768)
  Average Attention Entropy:  4.1227
----------------------
merge_lora_weights 5.946084499359131 5.5 2500
iter 1000: loss 5.7776, lr 0.00005, time 1881.38ms, mfu 24.35%
iter 1010: loss 5.8928, lr 0.00005, time 602.47ms, mfu 24.55%
iter 1020: loss 5.8867, lr 0.00005, time 599.76ms, mfu 24.74%
iter 1030: loss 5.8656, lr 0.00005, time 599.60ms, mfu 24.91%
Traceback (most recent call last):
  File "/teamspace/studios/this_studio/nanoGPT/train.py", line 855, in <module>
    scaler.scale(loss).backward()
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/_tensor.py", line 648, in backward
    torch.autograd.backward(
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/autograd/__init__.py", line 353, in backward
    _engine_run_backward(
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/autograd/graph.py", line 824, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
KeyboardInterrupt
Traceback (most recent call last):
  File "/teamspace/studios/this_studio/nanoGPT/train.py", line 855, in <module>
    scaler.scale(loss).backward()
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/_tensor.py", line 648, in backward
    torch.autograd.backward(
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/autograd/__init__.py", line 353, in backward
    _engine_run_backward(
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/autograd/graph.py", line 824, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
KeyboardInterrupt
Exception ignored in atexit callback: <function _start_and_connect_service.<locals>.teardown_atexit at 0x78f6a7712200>
Traceback (most recent call last):
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/wandb/sdk/lib/service/service_connection.py", line 54, in teardown_atexit
    conn.teardown(hooks.exit_code)
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/wandb/sdk/lib/service/service_connection.py", line 182, in teardown
    self._router.join()
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/wandb/sdk/interface/router.py", line 75, in join
    self._thread.join()
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/threading.py", line 1096, in join
    self._wait_for_tstate_lock()
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/threading.py", line 1116, in _wait_for_tstate_lock
    if lock.acquire(block, timeout):
KeyboardInterrupt:
Exception ignored in atexit callback: <function shutdown_compile_workers at 0x78f6c2e71d80>
Traceback (most recent call last):
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/_inductor/async_compile.py", line 113, in shutdown_compile_workers
    pool.shutdown()
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/_inductor/compile_worker/subproc_pool.py", line 239, in shutdown
    self.process.wait(300)
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/subprocess.py", line 1209, in wait
    return self._wait(timeout=timeout)
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/subprocess.py", line 1937, in _wait
    time.sleep(delay)
KeyboardInterrupt:
