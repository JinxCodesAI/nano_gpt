# Completion Variance Analysis Script

## Overview

The `completion_variance_analysis.py` script analyzes how much scoring model ratings vary for different completions generated from the same masked input. This helps understand the consistency and reliability of scoring models when evaluating text generated by language models.

## Purpose

This script addresses the key question: **"How much do ratings from a scoring model differ between different completions generated from the same input?"**

The analysis helps understand:
- Whether scoring models provide consistent ratings for similar quality text
- How masking ratio affects rating variance
- Whether language models generate truly diverse completions
- The relationship between completion diversity and rating variance

## How It Works

1. **Load Models**: Loads a language model (LANGUAGE_MODEL type) and a scoring model (SEQUENCE_SCORER type)
2. **Sample Data**: Reads samples from training data and applies random masking
3. **Generate Completions**: Creates multiple independent completions from the same masked input
4. **Rate Completions**: Uses the scoring model to rate all completions
5. **Analyze Variance**: Calculates statistical measures of rating variance and completion diversity

## Installation & Requirements

The script reuses existing project utilities and requires:
- PyTorch
- NumPy
- Trained language model checkpoint (LANGUAGE_MODEL mode)
- Trained scoring model checkpoint (SEQUENCE_SCORER mode)
- Training data (train.bin) and vocabulary (meta.pkl)

## Usage

### Basic Usage

```bash
python completion_variance_analysis.py \
    --language_model ckpt_unmasking_1000.pt \
    --scoring_model ckpt_sequence_scorer_1000_best.pt \
    --batch_size 8 \
    --num_samples 5 \
    --mask_ratio 0.3
```

### Advanced Usage

```bash
python completion_variance_analysis.py \
    --language_model ckpt_unmasking_1000.pt \
    --scoring_model ckpt_sequence_scorer_1000_best.pt \
    --dataset shakespeare_char \
    --batch_size 8 \
    --num_samples 10 \
    --sequence_length 256 \
    --mask_ratio 0.4 \
    --temperature 1.0 \
    --top_p 0.9 \
    --diffusion_iterations 5 \
    --verbose \
    --debug
```

## Command Line Arguments

### Required Arguments
- `--language_model`: Path to language model checkpoint (LANGUAGE_MODEL type)
- `--scoring_model`: Path to scoring model checkpoint (SEQUENCE_SCORER type)

### Data Configuration
- `--dataset`: Dataset name (default: shakespeare_char)
- `--out_dir`: Directory containing model checkpoints (default: out)

### Generation Parameters
- `--batch_size`: Number of independent completions per input (default: 8)
- `--num_samples`: Number of different masked inputs to test (default: 10)
- `--sequence_length`: Length of sequences to work with (default: 256)
- `--mask_ratio`: Fraction of tokens to mask (default: 0.3)

### Quality Parameters
- `--temperature`: Temperature for sampling (default: 1.0)
- `--top_p`: Nucleus sampling parameter (default: 1.0)
- `--diffusion_iterations`: Number of diffusion iterations (default: 10)

### Analysis Parameters
- `--seed`: Random seed for reproducibility (default: 42)
- `--verbose`: Enable verbose logging
- `--debug`: Enable debug output (shows completion texts)

### Device Configuration
- `--device`: Device to use (default: cuda)
- `--dtype`: Data type for inference (default: float16)
- `--compile`: Compile models with torch.compile

## Output Interpretation

### Per-Sample Results

For each sample, the script reports:

```
Sample 1 Results:
  Ratings: ['0.2372', '0.2203', '0.3240', '0.2015']
  Mean ± Std: 0.2458 ± 0.0469
  Range: [0.2015, 0.3240] (span: 0.1224)
  Coefficient of Variation: 0.1908
  Diversity: 4/4 unique (1.00)
  Token differences: mean=21.8, max=24
```

**Key Metrics:**
- **Ratings**: Individual scores from the scoring model (lower = better quality)
- **Mean ± Std**: Average rating and standard deviation
- **Range**: Minimum and maximum ratings with span
- **Coefficient of Variation**: Relative variability (std/mean)
- **Diversity**: Fraction of unique completions
- **Token differences**: Average and maximum token differences between completions

### Overall Analysis Results

```
=== Overall Analysis Results ===
Analyzed 5 samples with 8 completions each

Rating Variance Statistics:
  Standard deviation - Mean: 0.0518, Std: 0.0474
  Range - Mean: 0.1591, Std: 0.1412
  Coefficient of Variation - Mean: 0.2013, Std: 0.1751

Completion Diversity Statistics:
  Diversity ratio - Mean: 1.0000, Std: 0.0000
  Samples with all unique completions: 5/5
  Samples with no diversity (all identical): 0/5
```

**Interpretation:**
- **High variance** indicates inconsistent scoring model ratings
- **Low diversity** suggests language model generates similar completions
- **Correlation** between diversity and variance shows if diverse completions get more varied ratings

## Key Findings from Testing

### Mask Ratio Impact
Higher mask ratios lead to significantly higher rating variance:
- **2.7% masked** → std=0.0020, range=0.0066, CV=0.0141
- **25.4% masked** → std=0.0948, range=0.2681, CV=0.2980
- **39.8% masked** → std=0.1178, range=0.3636, CV=0.4746

### Completion Diversity
- All tested completions were unique (100% diversity)
- Language model successfully generates different outputs from same input
- Token differences correlate with masking ratio

### Rating Consistency
- Scoring model shows significant variance for high mask ratios
- Low mask ratios produce more consistent ratings
- Variance increases non-linearly with masking difficulty

## Performance Optimizations

The script includes several optimizations:
- **Batch Processing**: Rates all completions together for GPU efficiency
- **Memory Management**: Processes completions in sub-batches to manage memory
- **Memory Cleanup**: Clears GPU cache between samples
- **Efficient Storage**: Doesn't store large tensors in results

## Testing

Run the comprehensive test suite:

```bash
python test_completion_variance_analysis.py -v
```

Tests cover:
- Model loading functionality
- Vocabulary loading
- Data sampling and masking
- Completion generation
- Rating calculation
- Variance analysis
- Argument parsing

## Example Results

A typical analysis might show:

1. **High Masking (40%)**: Large variance (CV=0.47), indicating scoring model uncertainty
2. **Medium Masking (20%)**: Moderate variance (CV=0.18), reasonable consistency
3. **Low Masking (3%)**: Low variance (CV=0.01), high consistency

This suggests scoring models are most reliable when evaluating text with minimal modifications.

## Troubleshooting

### Common Issues

1. **Model Loading Errors**: Ensure checkpoint paths are correct and models have expected modes
2. **Memory Issues**: Reduce batch_size or sequence_length for large models
3. **CUDA Errors**: Use --device cpu for CPU-only inference
4. **Vocabulary Errors**: Ensure dataset directory contains meta.pkl

### Debug Mode

Use `--debug` flag to see:
- Masked input sequences
- Generated completion texts
- Individual completion ratings
- Detailed diversity statistics

## Future Enhancements

Potential improvements:
- Support for different masking strategies (span, sticky, etc.)
- Gradual unmasking instead of single-step completion
- Multiple scoring models comparison
- Statistical significance testing
- Visualization of results
- Export to CSV/JSON formats
