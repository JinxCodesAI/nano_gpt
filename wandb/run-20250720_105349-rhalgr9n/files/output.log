
Detailed parameter count:
  total                  | Total:   52,149,504 | Trainable:    8,207,616
  token_embeddings       | Total:    2,451,456 | Trainable:    2,451,456
  position_embeddings    | Total:            0 | Trainable:            0
  attention_layers       | Total:    7,520,256 | Trainable:    2,211,840
  feed_forward_layers    | Total:    3,538,944 | Trainable:    3,538,944
  layer_norms            | Total:        4,608 | Trainable:        4,608
  final_layer_norm       | Total:          768 | Trainable:          768
  language_model_head    | Total:   38,633,472 | Trainable:            0
------------------------------------------------------------
eval every:100
step 0: train loss 10.9675, val loss 10.9694
merge_lora_weights 10.969396591186523 8.0 500
iter 0: loss 10.9711, lr 0.00005, time 7187.15ms, mfu -100.00%
iter 10: loss 9.6136, lr 0.00055, time 562.10ms, mfu 32.88%
iter 20: loss 9.4373, lr 0.00104, time 566.07ms, mfu 32.85%
iter 30: loss 9.2876, lr 0.00154, time 571.47ms, mfu 32.80%
iter 40: loss 9.1289, lr 0.00204, time 580.93ms, mfu 32.70%
iter 50: loss 9.0753, lr 0.00254, time 579.81ms, mfu 32.62%
iter 60: loss 8.8965, lr 0.00303, time 582.61ms, mfu 32.53%
iter 70: loss 8.8195, lr 0.00353, time 583.55ms, mfu 32.44%
iter 80: loss 8.6907, lr 0.00403, time 578.56ms, mfu 32.39%
iter 90: loss 8.6056, lr 0.00453, time 575.27ms, mfu 32.37%
step 100: train loss 8.5486, val loss 8.5826
saving checkpoint to out
merge_lora_weights 8.58260440826416 8.0 500
iter 100: loss 8.4392, lr 0.00502, time 4974.84ms, mfu 29.50%
iter 110: loss 8.4637, lr 0.00552, time 572.94ms, mfu 29.78%
iter 120: loss 8.4610, lr 0.00602, time 587.12ms, mfu 29.95%
iter 130: loss 8.3575, lr 0.00652, time 565.49ms, mfu 30.22%
iter 140: loss 8.3498, lr 0.00701, time 572.43ms, mfu 30.43%
iter 150: loss 8.2097, lr 0.00751, time 567.60ms, mfu 30.64%
iter 160: loss 8.1460, lr 0.00801, time 574.79ms, mfu 30.79%
iter 170: loss 8.0367, lr 0.00851, time 569.28ms, mfu 30.96%
iter 180: loss 8.1219, lr 0.00900, time 575.90ms, mfu 31.07%
iter 190: loss 8.0077, lr 0.00950, time 575.90ms, mfu 31.17%
step 200: train loss 7.9634, val loss 7.9914
saving checkpoint to out

=== SCALING OPERATION TRIGGERED (DDP SYNC) ===
Operation: merge_lora_weights
Trigger reason: Loss threshold
Current val loss: 7.9914, Trigger loss: 8.0000
Iterations since last op: 200, Max wait: 500
Executing operation: merge_lora_weights first burn with value: None
Performing architectural operation: merge_lora_weights
Merging LoRA weights into main weights...
LoRA weights merged and reset.

Detailed parameter count:
  total                  | Total:   52,149,504 | Trainable:    8,207,616
  token_embeddings       | Total:    2,451,456 | Trainable:    2,451,456
  position_embeddings    | Total:            0 | Trainable:            0
  attention_layers       | Total:    7,520,256 | Trainable:    2,211,840
  feed_forward_layers    | Total:    3,538,944 | Trainable:    3,538,944
  layer_norms            | Total:        4,608 | Trainable:        4,608
  final_layer_norm       | Total:          768 | Trainable:          768
  language_model_head    | Total:   38,633,472 | Trainable:            0
------------------------------------------------------------
Re-configuring optimizer after architectural change...
num decayed parameter tensors: 17, with 8,202,240 parameters
num non-decayed parameter tensors: 7, with 5,376 parameters
using fused AdamW: True
Transferring optimizer state for existing parameters...
Transferred optimizer state for 0 / 28 parameters
Re-compiling the model...
Architectural operation completed successfully.
=== SCALING OPERATION COMPLETE ===


=== SCALING OPERATION TRIGGERED (DDP SYNC) ===
Operation: change_lr
Trigger reason: Loss threshold
Current val loss: 7.9914, Trigger loss: 100.0000
Iterations since last op: 0, Max wait: 1
Executing operation: change_lr  with value: 0.7
LR multiplier: 10.0000 -> 7.0000
=== SCALING OPERATION COMPLETE ===


=== SCALING OPERATION TRIGGERED (DDP SYNC) ===
Operation: reset_lr_schedule
Trigger reason: Loss threshold
Current val loss: 7.9914, Trigger loss: 100.0000
Iterations since last op: 0, Max wait: 1
Executing operation: reset_lr_schedule  with value: None
LR schedule offset: 0 -> 200
=== SCALING OPERATION COMPLETE ===

merge_lora_weights 7.991374969482422 7.2 300
iter 200: loss 11.9264, lr 0.01000, time 4965.13ms, mfu 28.43%
iter 210: loss 8.6204, lr 0.00038, time 576.60ms, mfu 28.79%
iter 220: loss 8.1896, lr 0.00073, time 580.45ms, mfu 29.10%
iter 230: loss 8.1555, lr 0.00108, time 582.46ms, mfu 29.36%
iter 240: loss 8.0720, lr 0.00143, time 578.60ms, mfu 29.62%
iter 250: loss 7.9470, lr 0.00178, time 580.39ms, mfu 29.84%
iter 260: loss 7.7951, lr 0.00212, time 580.58ms, mfu 30.04%
iter 270: loss 7.8978, lr 0.00247, time 578.88ms, mfu 30.23%
iter 280: loss 7.7596, lr 0.00282, time 581.58ms, mfu 30.38%
iter 290: loss 7.8063, lr 0.00317, time 578.84ms, mfu 30.54%
step 300: train loss 7.7865, val loss 7.7585
saving checkpoint to out
merge_lora_weights 7.758502960205078 7.2 300
iter 300: loss 7.8948, lr 0.00352, time 4974.52ms, mfu 27.85%
iter 310: loss 7.8579, lr 0.00387, time 575.38ms, mfu 28.28%
iter 320: loss 7.6055, lr 0.00421, time 579.92ms, mfu 28.64%
iter 330: loss 7.7589, lr 0.00456, time 579.23ms, mfu 28.97%
iter 340: loss 7.6671, lr 0.00491, time 580.11ms, mfu 29.26%
iter 350: loss 7.6280, lr 0.00526, time 582.08ms, mfu 29.50%
iter 360: loss 7.5433, lr 0.00561, time 579.29ms, mfu 29.74%
iter 370: loss 7.6231, lr 0.00596, time 580.70ms, mfu 29.95%
iter 380: loss 7.4627, lr 0.00630, time 580.10ms, mfu 30.14%
iter 390: loss 7.4645, lr 0.00665, time 578.96ms, mfu 30.32%
step 400: train loss 7.5949, val loss 7.5171
saving checkpoint to out
merge_lora_weights 7.517101287841797 7.2 300
iter 400: loss 7.5588, lr 0.00700, time 5009.44ms, mfu 27.66%
iter 410: loss 7.6025, lr 0.00700, time 577.61ms, mfu 28.09%
iter 420: loss 7.5273, lr 0.00700, time 580.63ms, mfu 28.46%
iter 430: loss 7.5213, lr 0.00700, time 580.35ms, mfu 28.80%
iter 440: loss 7.5615, lr 0.00700, time 579.73ms, mfu 29.11%
iter 450: loss 7.4526, lr 0.00700, time 580.48ms, mfu 29.38%
iter 460: loss 7.4711, lr 0.00700, time 579.55ms, mfu 29.63%
iter 470: loss 7.2331, lr 0.00700, time 581.59ms, mfu 29.85%
iter 480: loss 7.4897, lr 0.00700, time 581.42ms, mfu 30.04%
iter 490: loss 7.3560, lr 0.00700, time 580.13ms, mfu 30.22%
step 500: train loss 7.3680, val loss 7.3900
saving checkpoint to out

=== SCALING OPERATION TRIGGERED (DDP SYNC) ===
Operation: merge_lora_weights
Trigger reason: Timeout
Current val loss: 7.3900, Trigger loss: 7.2000
Iterations since last op: 300, Max wait: 300
Executing operation: merge_lora_weights second burn with value: None
Performing architectural operation: merge_lora_weights
Merging LoRA weights into main weights...
LoRA weights merged and reset.

Detailed parameter count:
  total                  | Total:   52,149,504 | Trainable:    8,207,616
  token_embeddings       | Total:    2,451,456 | Trainable:    2,451,456
  position_embeddings    | Total:            0 | Trainable:            0
  attention_layers       | Total:    7,520,256 | Trainable:    2,211,840
  feed_forward_layers    | Total:    3,538,944 | Trainable:    3,538,944
  layer_norms            | Total:        4,608 | Trainable:        4,608
  final_layer_norm       | Total:          768 | Trainable:          768
  language_model_head    | Total:   38,633,472 | Trainable:            0
------------------------------------------------------------
Re-configuring optimizer after architectural change...
num decayed parameter tensors: 17, with 8,202,240 parameters
num non-decayed parameter tensors: 7, with 5,376 parameters
using fused AdamW: True
Transferring optimizer state for existing parameters...
Transferred optimizer state for 0 / 28 parameters
Re-compiling the model...
Architectural operation completed successfully.
=== SCALING OPERATION COMPLETE ===


=== SCALING OPERATION TRIGGERED (DDP SYNC) ===
Operation: change_lr
Trigger reason: Loss threshold
Current val loss: 7.3900, Trigger loss: 100.0000
Iterations since last op: 0, Max wait: 1
Executing operation: change_lr  with value: 0.2
LR multiplier: 7.0000 -> 1.4000
=== SCALING OPERATION COMPLETE ===


=== SCALING OPERATION TRIGGERED (DDP SYNC) ===
Operation: change_warmup_iters
Trigger reason: Loss threshold
Current val loss: 7.3900, Trigger loss: 100.0000
Iterations since last op: 0, Max wait: 1
Executing operation: change_warmup_iters  with value: 2
Warmup iters multiplier: 1.0000 -> 2.0000
=== SCALING OPERATION COMPLETE ===


=== SCALING OPERATION TRIGGERED (DDP SYNC) ===
Operation: reset_lr_schedule
Trigger reason: Loss threshold
Current val loss: 7.3900, Trigger loss: 100.0000
Iterations since last op: 0, Max wait: 1
Executing operation: reset_lr_schedule  with value: None
LR schedule offset: 200 -> 500
=== SCALING OPERATION COMPLETE ===


=== SCALING OPERATION TRIGGERED (DDP SYNC) ===
Operation: change_batch_size
Trigger reason: Loss threshold
Current val loss: 7.3900, Trigger loss: 100.0000
Iterations since last op: 0, Max wait: 1
Executing operation: change_batch_size  with value: 0.5
Batch size: 32 -> 16
=== SCALING OPERATION COMPLETE ===


=== SCALING OPERATION TRIGGERED (DDP SYNC) ===
Operation: change_grad_accum
Trigger reason: Loss threshold
Current val loss: 7.3900, Trigger loss: 100.0000
Iterations since last op: 0, Max wait: 1
Executing operation: change_grad_accum  with value: 2.0
Grad accum steps: 2 -> 4
=== SCALING OPERATION COMPLETE ===


=== SCALING OPERATION TRIGGERED (DDP SYNC) ===
Operation: change_lr
Trigger reason: Loss threshold
Current val loss: 7.3900, Trigger loss: 100.0000
Iterations since last op: 0, Max wait: 1
Executing operation: change_lr  with value: 0.7
LR multiplier: 1.4000 -> 0.9800
=== SCALING OPERATION COMPLETE ===


=== SCALING OPERATION TRIGGERED (DDP SYNC) ===
Operation: reset_lr_schedule
Trigger reason: Loss threshold
Current val loss: 7.3900, Trigger loss: 100.0000
Iterations since last op: 0, Max wait: 1
Executing operation: reset_lr_schedule  with value: None
LR schedule offset: 500 -> 500
=== SCALING OPERATION COMPLETE ===


=== SCALING OPERATION TRIGGERED (DDP SYNC) ===
Operation: stack_layers
Trigger reason: Loss threshold
Current val loss: 7.3900, Trigger loss: 100.0000
Iterations since last op: 0, Max wait: 1
Executing operation: stack_layers first resize with value: 2
Performing architectural operation: stack_layers
Stacking layers: current depth 3, creating 6 total layers.
Model now has 6 layers.

Detailed parameter count:
  total                  | Total:   63,213,312 | Trainable:   13,963,008
  token_embeddings       | Total:    2,451,456 | Trainable:    2,451,456
  position_embeddings    | Total:            0 | Trainable:            0
  attention_layers       | Total:   15,040,512 | Trainable:    4,423,680
  feed_forward_layers    | Total:    7,077,888 | Trainable:    7,077,888
  layer_norms            | Total:        9,216 | Trainable:        9,216
  final_layer_norm       | Total:          768 | Trainable:          768
  language_model_head    | Total:   38,633,472 | Trainable:            0
------------------------------------------------------------
Re-configuring optimizer after architectural change...
num decayed parameter tensors: 32, with 13,953,024 parameters
num non-decayed parameter tensors: 13, with 9,984 parameters
using fused AdamW: True
Transferring optimizer state for existing parameters...
Transferred optimizer state for 0 / 52 parameters
Re-compiling the model...
Architectural operation completed successfully.
Re-evaluating validation loss after operation...
New val loss after operation: 8.4811
iter 500: loss 8.5036, lr 0.00700, time 20050.56ms, mfu 27.32%
iter 510: loss 7.8507, lr 0.00003, time 752.22ms, mfu 27.73%
iter 520: loss 7.6641, lr 0.00005, time 763.34ms, mfu 28.05%
iter 530: loss 7.5698, lr 0.00008, time 770.00ms, mfu 28.31%
iter 540: loss 7.5103, lr 0.00010, time 770.90ms, mfu 28.54%
iter 550: loss 7.5287, lr 0.00012, time 774.91ms, mfu 28.73%
iter 560: loss 7.4578, lr 0.00015, time 769.78ms, mfu 28.93%
iter 570: loss 7.4635, lr 0.00017, time 760.04ms, mfu 29.14%
iter 580: loss 7.4566, lr 0.00020, time 758.46ms, mfu 29.34%
iter 590: loss 7.3299, lr 0.00022, time 759.27ms, mfu 29.51%
step 600: train loss 7.2183, val loss 7.2091
saving checkpoint to out
merge_lora_weights 7.2091169357299805 6.8 700
iter 600: loss 6.9970, lr 0.00025, time 3634.32ms, mfu 27.21%
iter 610: loss 7.1027, lr 0.00027, time 755.02ms, mfu 27.62%
iter 620: loss 7.2149, lr 0.00030, time 755.70ms, mfu 27.98%
iter 630: loss 7.1352, lr 0.00032, time 758.76ms, mfu 28.29%
iter 640: loss 7.2776, lr 0.00034, time 763.94ms, mfu 28.55%
iter 650: loss 7.2026, lr 0.00037, time 765.73ms, mfu 28.78%
iter 660: loss 7.1829, lr 0.00039, time 769.48ms, mfu 28.97%
iter 670: loss 7.1770, lr 0.00042, time 767.02ms, mfu 29.15%
iter 680: loss 7.2490, lr 0.00044, time 769.26ms, mfu 29.31%
iter 690: loss 7.0595, lr 0.00047, time 771.33ms, mfu 29.44%
step 700: train loss 7.1840, val loss 7.1721
saving checkpoint to out
merge_lora_weights 7.17209005355835 6.8 700
iter 700: loss 7.2586, lr 0.00049, time 3374.51ms, mfu 27.19%
iter 710: loss 7.1911, lr 0.00052, time 761.40ms, mfu 27.57%
iter 720: loss 6.9836, lr 0.00054, time 760.81ms, mfu 27.92%
iter 730: loss 7.0350, lr 0.00056, time 763.07ms, mfu 28.22%
iter 740: loss 7.4272, lr 0.00059, time 761.15ms, mfu 28.50%
iter 750: loss 7.0649, lr 0.00061, time 766.04ms, mfu 28.73%
iter 760: loss 7.0218, lr 0.00064, time 761.98ms, mfu 28.96%
iter 770: loss 7.2147, lr 0.00066, time 764.75ms, mfu 29.15%
iter 780: loss 6.9440, lr 0.00069, time 761.74ms, mfu 29.33%
iter 790: loss 7.0958, lr 0.00071, time 763.32ms, mfu 29.49%
step 800: train loss 7.0415, val loss 7.0491
saving checkpoint to out
merge_lora_weights 7.049079895019531 6.8 700
iter 800: loss 6.9724, lr 0.00074, time 3536.71ms, mfu 27.21%
iter 810: loss 6.9181, lr 0.00076, time 765.05ms, mfu 27.58%
iter 820: loss 6.8895, lr 0.00078, time 768.23ms, mfu 27.89%
iter 830: loss 6.8629, lr 0.00081, time 765.18ms, mfu 28.19%
iter 840: loss 7.0733, lr 0.00083, time 764.89ms, mfu 28.46%
iter 850: loss 7.1065, lr 0.00086, time 762.21ms, mfu 28.71%
iter 860: loss 6.8622, lr 0.00088, time 765.28ms, mfu 28.92%
iter 870: loss 7.0442, lr 0.00091, time 766.99ms, mfu 29.11%
iter 880: loss 6.9184, lr 0.00093, time 763.27ms, mfu 29.29%
iter 890: loss 6.7478, lr 0.00096, time 764.50ms, mfu 29.45%
step 900: train loss 6.9858, val loss 7.0579
saving checkpoint to out
merge_lora_weights 7.05786657333374 6.8 700
iter 900: loss 7.1556, lr 0.00098, time 3285.54ms, mfu 27.22%
iter 910: loss 6.9387, lr 0.00098, time 762.87ms, mfu 27.60%
iter 920: loss 7.1639, lr 0.00098, time 769.53ms, mfu 27.90%
iter 930: loss 6.9223, lr 0.00098, time 766.50ms, mfu 28.19%
iter 940: loss 7.0686, lr 0.00098, time 763.08ms, mfu 28.47%
iter 950: loss 6.9822, lr 0.00098, time 762.02ms, mfu 28.72%
iter 960: loss 7.1040, lr 0.00098, time 761.53ms, mfu 28.95%
iter 970: loss 6.9092, lr 0.00098, time 761.93ms, mfu 29.15%
iter 980: loss 6.9107, lr 0.00098, time 760.62ms, mfu 29.34%
iter 990: loss 6.9208, lr 0.00098, time 762.21ms, mfu 29.50%
step 1000: train loss 6.9582, val loss 6.9843
saving checkpoint to out
merge_lora_weights 6.984304904937744 6.8 700
iter 1000: loss 7.0949, lr 0.00098, time 3557.79ms, mfu 27.22%
iter 1010: loss 7.0050, lr 0.00098, time 761.38ms, mfu 27.60%
iter 1020: loss 6.8908, lr 0.00098, time 766.55ms, mfu 27.92%
iter 1030: loss 6.9383, lr 0.00098, time 767.63ms, mfu 28.20%
iter 1040: loss 6.9974, lr 0.00098, time 768.63ms, mfu 28.45%
iter 1050: loss 6.9234, lr 0.00098, time 762.87ms, mfu 28.70%
iter 1060: loss 6.9303, lr 0.00098, time 763.37ms, mfu 28.92%
iter 1070: loss 6.9310, lr 0.00098, time 761.56ms, mfu 29.13%
iter 1080: loss 6.9823, lr 0.00098, time 763.95ms, mfu 29.31%
iter 1090: loss 6.9048, lr 0.00098, time 759.56ms, mfu 29.49%
step 1100: train loss 6.9637, val loss 6.9476
saving checkpoint to out
merge_lora_weights 6.947593688964844 6.8 700
iter 1100: loss 6.7181, lr 0.00098, time 3560.91ms, mfu 27.20%
iter 1110: loss 6.8634, lr 0.00098, time 764.07ms, mfu 27.57%
iter 1120: loss 6.8542, lr 0.00098, time 769.14ms, mfu 27.88%
iter 1130: loss 6.6384, lr 0.00098, time 768.14ms, mfu 28.17%
iter 1140: loss 6.9136, lr 0.00098, time 772.77ms, mfu 28.41%
iter 1150: loss 6.8928, lr 0.00098, time 767.68ms, mfu 28.64%
iter 1160: loss 7.0246, lr 0.00098, time 766.83ms, mfu 28.86%
iter 1170: loss 6.9365, lr 0.00098, time 765.70ms, mfu 29.05%
iter 1180: loss 6.9835, lr 0.00098, time 762.93ms, mfu 29.24%
iter 1190: loss 6.8546, lr 0.00098, time 763.24ms, mfu 29.41%
step 1200: train loss 6.9126, val loss 6.8344
saving checkpoint to out

=== SCALING OPERATION TRIGGERED (DDP SYNC) ===
Operation: merge_lora_weights
Trigger reason: Timeout
Current val loss: 6.8344, Trigger loss: 6.8000
Iterations since last op: 700, Max wait: 700
Executing operation: merge_lora_weights third burn with value: None
Performing architectural operation: merge_lora_weights
Merging LoRA weights into main weights...
LoRA weights merged and reset.

Detailed parameter count:
  total                  | Total:   63,213,312 | Trainable:   13,963,008
  token_embeddings       | Total:    2,451,456 | Trainable:    2,451,456
  position_embeddings    | Total:            0 | Trainable:            0
  attention_layers       | Total:   15,040,512 | Trainable:    4,423,680
  feed_forward_layers    | Total:    7,077,888 | Trainable:    7,077,888
  layer_norms            | Total:        9,216 | Trainable:        9,216
  final_layer_norm       | Total:          768 | Trainable:          768
  language_model_head    | Total:   38,633,472 | Trainable:            0
------------------------------------------------------------
Re-configuring optimizer after architectural change...
num decayed parameter tensors: 32, with 13,953,024 parameters
num non-decayed parameter tensors: 13, with 9,984 parameters
using fused AdamW: True
Transferring optimizer state for existing parameters...
Transferred optimizer state for 0 / 52 parameters
Re-compiling the model...
Architectural operation completed successfully.
=== SCALING OPERATION COMPLETE ===


=== SCALING OPERATION TRIGGERED (DDP SYNC) ===
Operation: change_batch_size
Trigger reason: Loss threshold
Current val loss: 6.8344, Trigger loss: 100.0000
Iterations since last op: 0, Max wait: 1
Executing operation: change_batch_size  with value: 0.5
Batch size: 16 -> 8
=== SCALING OPERATION COMPLETE ===


=== SCALING OPERATION TRIGGERED (DDP SYNC) ===
Operation: change_grad_accum
Trigger reason: Loss threshold
Current val loss: 6.8344, Trigger loss: 100.0000
Iterations since last op: 0, Max wait: 1
Executing operation: change_grad_accum  with value: 2.0
Grad accum steps: 4 -> 8
=== SCALING OPERATION COMPLETE ===


=== SCALING OPERATION TRIGGERED (DDP SYNC) ===
Operation: widen_mlp
Trigger reason: Loss threshold
Current val loss: 6.8344, Trigger loss: 100.0000
Iterations since last op: 0, Max wait: 1
Executing operation: widen_mlp second resize with value: 2
Performing architectural operation: widen_mlp
Widening MLP layers by a factor of 2.
MLP hidden dimension widened to 1536.

Detailed parameter count:
  total                  | Total:   70,291,200 | Trainable:   21,040,896
  token_embeddings       | Total:    2,451,456 | Trainable:    2,451,456
  position_embeddings    | Total:            0 | Trainable:            0
  attention_layers       | Total:   15,040,512 | Trainable:    4,423,680
  feed_forward_layers    | Total:   14,155,776 | Trainable:   14,155,776
  layer_norms            | Total:        9,216 | Trainable:        9,216
  final_layer_norm       | Total:          768 | Trainable:          768
  language_model_head    | Total:   38,633,472 | Trainable:            0
------------------------------------------------------------
Re-configuring optimizer after architectural change...
num decayed parameter tensors: 32, with 21,030,912 parameters
num non-decayed parameter tensors: 13, with 9,984 parameters
using fused AdamW: True
Transferring optimizer state for existing parameters...
Transferred optimizer state for 0 / 52 parameters
Re-compiling the model...
Architectural operation completed successfully.
Re-evaluating validation loss after operation...
W0720 11:08:38.373000 223670 /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/_inductor/utils.py:1250] [0/4] Not enough SMs to use max_autotune_gemm mode
New val loss after operation: 6.8374
iter 1200: loss 7.5149, lr 0.00098, time 37381.99ms, mfu 26.54%
iter 1210: loss 6.7168, lr 0.00098, time 809.63ms, mfu 27.09%
iter 1220: loss 7.0605, lr 0.00098, time 820.26ms, mfu 27.54%
iter 1230: loss 6.8100, lr 0.00098, time 832.69ms, mfu 27.89%
iter 1240: loss 7.3053, lr 0.00098, time 828.69ms, mfu 28.23%
iter 1250: loss 6.8448, lr 0.00098, time 823.09ms, mfu 28.56%
iter 1260: loss 6.5402, lr 0.00098, time 819.29ms, mfu 28.86%
iter 1270: loss 7.2660, lr 0.00098, time 808.01ms, mfu 29.18%
iter 1280: loss 6.6352, lr 0.00098, time 818.06ms, mfu 29.43%
iter 1290: loss 6.9425, lr 0.00098, time 802.83ms, mfu 29.72%
step 1300: train loss 6.8001, val loss 6.8569
saving checkpoint to out

=== SCALING OPERATION TRIGGERED (DDP SYNC) ===
Operation: change_lr
Trigger reason: Loss threshold
Current val loss: 6.8569, Trigger loss: 100.0000
Iterations since last op: 100, Max wait: 1
Executing operation: change_lr  with value: 0.7
LR multiplier: 0.9800 -> 0.6860
=== SCALING OPERATION COMPLETE ===


=== SCALING OPERATION TRIGGERED (DDP SYNC) ===
Operation: reset_lr_schedule
Trigger reason: Loss threshold
Current val loss: 6.8569, Trigger loss: 100.0000
Iterations since last op: 0, Max wait: 1
Executing operation: reset_lr_schedule  with value: None
LR schedule offset: 500 -> 1300
=== SCALING OPERATION COMPLETE ===


=== SCALING OPERATION TRIGGERED (DDP SYNC) ===
Operation: stack_layers
Trigger reason: Loss threshold
Current val loss: 6.8569, Trigger loss: 100.0000
Iterations since last op: 0, Max wait: 1
Executing operation: stack_layers third resize with value: 2
Performing architectural operation: stack_layers
Stacking layers: current depth 6, creating 12 total layers.
Model now has 12 layers.

Detailed parameter count:
  total                  | Total:   99,496,704 | Trainable:   39,629,568
  token_embeddings       | Total:    2,451,456 | Trainable:    2,451,456
  position_embeddings    | Total:            0 | Trainable:            0
  attention_layers       | Total:   30,081,024 | Trainable:    8,847,360
  feed_forward_layers    | Total:   28,311,552 | Trainable:   28,311,552
  layer_norms            | Total:       18,432 | Trainable:       18,432
  final_layer_norm       | Total:          768 | Trainable:          768
  language_model_head    | Total:   38,633,472 | Trainable:            0
------------------------------------------------------------
Re-configuring optimizer after architectural change...
num decayed parameter tensors: 62, with 39,610,368 parameters
num non-decayed parameter tensors: 25, with 19,200 parameters
using fused AdamW: True
Transferring optimizer state for existing parameters...
Transferred optimizer state for 0 / 100 parameters
Re-compiling the model...
Architectural operation completed successfully.
Re-evaluating validation loss after operation...
New val loss after operation: 7.1794
iter 1300: loss 7.0702, lr 0.00098, time 29412.84ms, mfu 26.88%
iter 1310: loss 7.3004, lr 0.00002, time 1281.47ms, mfu 27.19%
iter 1320: loss 7.0287, lr 0.00004, time 1312.07ms, mfu 27.40%
iter 1330: loss 7.1824, lr 0.00005, time 1313.77ms, mfu 27.59%
iter 1340: loss 6.6330, lr 0.00007, time 1299.16ms, mfu 27.79%
iter 1350: loss 6.7985, lr 0.00009, time 1279.23ms, mfu 28.02%
iter 1360: loss 6.7487, lr 0.00010, time 1299.61ms, mfu 28.18%
iter 1370: loss 6.6841, lr 0.00012, time 1279.11ms, mfu 28.37%
iter 1380: loss 6.6332, lr 0.00014, time 1279.83ms, mfu 28.54%
iter 1390: loss 6.9535, lr 0.00016, time 1280.06ms, mfu 28.69%
step 1400: train loss 6.8381, val loss 6.7648
saving checkpoint to out
merge_lora_weights 6.764803409576416 6.0 600
iter 1400: loss 6.9061, lr 0.00017, time 5047.29ms, mfu 26.58%
iter 1410: loss 6.7678, lr 0.00019, time 1300.27ms, mfu 26.88%
iter 1420: loss 6.9238, lr 0.00021, time 1307.70ms, mfu 27.13%
iter 1430: loss 6.8037, lr 0.00022, time 1300.78ms, mfu 27.38%
iter 1440: loss 6.7951, lr 0.00024, time 1299.77ms, mfu 27.60%
iter 1450: loss 7.0312, lr 0.00026, time 1287.49ms, mfu 27.83%
iter 1460: loss 6.6687, lr 0.00028, time 1289.79ms, mfu 28.03%
iter 1470: loss 6.6462, lr 0.00029, time 1298.20ms, mfu 28.19%
iter 1480: loss 6.7874, lr 0.00031, time 1300.52ms, mfu 28.33%
iter 1490: loss 6.5165, lr 0.00033, time 1296.89ms, mfu 28.46%
step 1500: train loss 6.8233, val loss 6.7984
saving checkpoint to out
merge_lora_weights 6.798384666442871 6.0 600
iter 1500: loss 6.8484, lr 0.00034, time 4633.22ms, mfu 26.44%
iter 1510: loss 7.0317, lr 0.00036, time 1299.77ms, mfu 26.76%
iter 1520: loss 6.7850, lr 0.00038, time 1298.75ms, mfu 27.05%
iter 1530: loss 6.6773, lr 0.00040, time 1300.31ms, mfu 27.30%
iter 1540: loss 6.7715, lr 0.00041, time 1299.25ms, mfu 27.53%
iter 1550: loss 7.0082, lr 0.00043, time 1297.56ms, mfu 27.74%
iter 1560: loss 7.1706, lr 0.00045, time 1300.16ms, mfu 27.93%
iter 1570: loss 7.2054, lr 0.00046, time 1298.26ms, mfu 28.10%
iter 1580: loss 6.7968, lr 0.00048, time 1299.67ms, mfu 28.25%
iter 1590: loss 6.5395, lr 0.00050, time 1300.50ms, mfu 28.38%
step 1600: train loss 6.7362, val loss 6.8911
saving checkpoint to out
merge_lora_weights 6.891066074371338 6.0 600
iter 1600: loss 6.6362, lr 0.00051, time 4909.03ms, mfu 26.33%
iter 1610: loss 7.2492, lr 0.00053, time 1301.76ms, mfu 26.65%
iter 1620: loss 6.4047, lr 0.00055, time 1299.59ms, mfu 26.94%
iter 1630: loss 6.6368, lr 0.00057, time 1299.01ms, mfu 27.21%
iter 1640: loss 6.7280, lr 0.00058, time 1299.53ms, mfu 27.45%
iter 1650: loss 6.7484, lr 0.00060, time 1298.48ms, mfu 27.67%
iter 1660: loss 6.6879, lr 0.00062, time 1300.02ms, mfu 27.86%
iter 1670: loss 7.0272, lr 0.00063, time 1299.11ms, mfu 28.03%
iter 1680: loss 6.7711, lr 0.00065, time 1298.17ms, mfu 28.19%
iter 1690: loss 6.8537, lr 0.00067, time 1299.79ms, mfu 28.33%
step 1700: train loss 6.7244, val loss 6.7487
saving checkpoint to out
merge_lora_weights 6.748686790466309 6.0 600
iter 1700: loss 6.5384, lr 0.00069, time 4726.58ms, mfu 26.31%
iter 1710: loss 6.5578, lr 0.00069, time 1300.50ms, mfu 26.64%
iter 1720: loss 6.5694, lr 0.00069, time 1296.45ms, mfu 26.94%
iter 1730: loss 6.8176, lr 0.00069, time 1302.10ms, mfu 27.20%
iter 1740: loss 7.0621, lr 0.00069, time 1298.68ms, mfu 27.45%
Traceback (most recent call last):
  File "/teamspace/studios/this_studio/nanoGPT/train.py", line 657, in <module>
    scaler.scale(loss).backward()
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/_tensor.py", line 648, in backward
    torch.autograd.backward(
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/autograd/__init__.py", line 353, in backward
    _engine_run_backward(
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/autograd/graph.py", line 824, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
KeyboardInterrupt
Traceback (most recent call last):
  File "/teamspace/studios/this_studio/nanoGPT/train.py", line 657, in <module>
    scaler.scale(loss).backward()
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/_tensor.py", line 648, in backward
    torch.autograd.backward(
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/autograd/__init__.py", line 353, in backward
    _engine_run_backward(
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/autograd/graph.py", line 824, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
KeyboardInterrupt
Exception ignored in atexit callback: <function _start_and_connect_service.<locals>.teardown_atexit at 0x75a42e55c8b0>
Traceback (most recent call last):
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/wandb/sdk/lib/service/service_connection.py", line 54, in teardown_atexit
    conn.teardown(hooks.exit_code)
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/wandb/sdk/lib/service/service_connection.py", line 182, in teardown
    self._router.join()
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/wandb/sdk/interface/router.py", line 75, in join
    self._thread.join()
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/threading.py", line 1096, in join
    self._wait_for_tstate_lock()
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/threading.py", line 1116, in _wait_for_tstate_lock
    if lock.acquire(block, timeout):
KeyboardInterrupt:
Exception ignored in atexit callback: <function shutdown_compile_workers at 0x75a4500f0670>
Traceback (most recent call last):
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/_inductor/async_compile.py", line 113, in shutdown_compile_workers
    pool.shutdown()
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/_inductor/compile_worker/subproc_pool.py", line 239, in shutdown
    self.process.wait(300)
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/subprocess.py", line 1209, in wait
    return self._wait(timeout=timeout)
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/subprocess.py", line 1937, in _wait
    time.sleep(delay)
KeyboardInterrupt:
