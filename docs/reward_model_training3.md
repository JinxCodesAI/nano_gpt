# Guide: Training a Language Model with GRPO and a Self-Supervised Reward Model

This document outlines the step-by-step process for implementing a Generative Reward Policy Optimization (GRPO) training loop. This approach is designed to solve a common challenge in reinforcement learning for language models: the absence of an explicit reward function for unsupervised pre-training data.

The core of this strategy is a novel, self-supervised reward mechanism. Instead of relying on human feedback (RLHF) or a separate, pre-trained preference model, we create our own reward signal through an adversarial process between two models:

1. **The Base Model (Generator):** The language model we aim to improve.
2. **The Reward Model (Discriminator/Evaluator):** A specialized version of the Base Model trained for a unique task.

The Reward Model is not a simple binary classifier for "real" vs. "fake" text. Instead, it is trained to solve a more nuanced problem: **given a sequence of text, predict the proportion of tokens that came from the original ("natural") dataset versus those that were synthetically generated by the Base Model.**

To achieve this, the Reward Model is trained on a custom-generated dataset where each sample is a mix of natural and synthetic data. For a sequence of length N, a random crossover point K is chosen. The first K tokens are taken from the real dataset, and the remaining N-K tokens are generated by the current Base Model. The model is then trained to predict the target probability vector [K/N, (N-K)/N].

The process becomes cyclical and adversarial. The Base Model is then fine-tuned using GRPO, where its "reward" is the P(natural) score assigned by the frozen Reward Model. This incentivizes the Base Model to generate text that so closely mimics the true data distribution that it can consistently "fool" the Reward Model into believing the generated text is entirely natural. This dynamic pushes the generator's quality far beyond what standard supervised training alone can achieve.

## Part 1: Requirements & System Setup

### 1.1. Python Libraries

Ensure you have the necessary libraries installed.

```bash
pip install torch numpy transformers datasets tiktoken tqdm
```

### 1.2. File Structure

Your project directory should be organized as follows. We will be modifying model.py and creating a new script, prepare_reward_data.py.

```
/your_project/
|
|-- model.py                    # (Will be modified)
|-- train.py                    # Main script for standard LM training
|-- train_reward_model.py       # New script for training the reward model
|-- train_grpo.py               # New script for the GRPO reinforcement learning step
|
|-- prepare_reward_data.py      # Enhanced script to generate the reward dataset
|-- tokenization_manager.py     # New: Unified tokenization interface
|-- data_loader.py              # New: Unified data loading interface
|-- reward_data_config.py       # New: Configuration and validation system
|-- reward_dataset_loader.py    # Enhanced: Dataset loading with tokenization validation
|
|-- data/
|   |-- shakespeare/            # BPE tokenization data
|   |   |-- input.txt
|   |   |-- prepare.py
|   |   |-- train.bin
|   |   |-- val.bin
|   |
|   |-- shakespeare_char/       # Character tokenization data
|   |   |-- input.txt
|   |   |-- prepare.py
|   |   |-- meta.pkl           # Character tokenization metadata
|   |   |-- train.bin
|   |   |-- val.bin
|   |
|   |-- reward_dataset/         # Directory for the reward model's data
|       |-- train_x.bin         # Input sequences
|       |-- train_y.bin         # Target probabilities
|       |-- train_metadata.txt  # Training metadata with tokenization info
|       |-- val_x.bin           # Validation sequences
|       |-- val_y.bin           # Validation probabilities
|       |-- val_metadata.txt    # Validation metadata with tokenization info
|
|-- tests/                      # Comprehensive test suite
|   |-- test_tokenization_manager.py
|   |-- test_data_loader.py
|   |-- test_integration.py
|   |-- test_reward_dataset_loader.py
|   |-- test_end_to_end.py
|
|-- docs/                       # Documentation
    |-- reward_model_training_infrastructure.md
    |-- reward_model_training3.md
```

## Part 2: Model Architecture Modifications (model.py)

To avoid maintaining two separate large models, we will modify the GPT class to operate in one of two modes: generator or reward. Both modes will share the vast majority of their weights (the "trunk"), but will have different final layers (the "head") specific to their task.

### 2.1. Update GPTConfig

Add a mode parameter to the configuration class to specify which model to build.

```python
# In model.py

from dataclasses import dataclass

@dataclass
class GPTConfig:
    block_size: int = 1024
    vocab_size: int = 50304
    n_layer: int = 12
    n_head: int = 12
    n_embd: int = 768
    dropout: float = 0.0
    bias: bool = True
    # --- ADD THESE LINES ---
    mode: str = 'generator' # Can be 'generator' or 'reward'
    reward_head_hidden_dim: int = 256  # Configurable reward head size
```

### 2.2. Update GPT Class

The __init__ and forward methods will be updated to handle the two modes.

**Key Changes:**

1. **Shared Trunk:** The transformer module dictionary (containing embeddings, dropout, transformer blocks, and final layer norm) is created for both modes. This is the core of the model that will be shared.
2. **Conditional Heads:**
   * If config.mode == 'generator', we create the standard lm_head for predicting the next token.
   * If config.mode == 'reward', we create a new reward_head. This head is a simple MLP that takes the final transformer output, pools it, and classifies it.
3. **Pooling + MLP + Softmax:** The reward_head takes the hidden state of the **last token** in the sequence as a summary of the entire sequence (Pooling). It then passes this summary through a small MLP which outputs two values. A Softmax layer converts these values into a probability distribution [P(natural), P(synthetic)].
4. **Forward Pass Logic:** The forward method first computes the output of the shared trunk (x). Then, it branches based on the mode, passing x to the appropriate head to get either logits (for the generator) or probabilities (for the reward model).

Here is the implementation within the GPT class:

```python
# In model.py, inside the GPT class

class GPT(nn.Module):
    def __init__(self, config):
        super().__init__()
        assert config.vocab_size is not None
        assert config.block_size is not None
        assert config.mode in ['generator', 'reward'], "mode must be 'generator' or 'reward'"
        self.config = config

        # Shared "trunk" of the model
        self.transformer = nn.ModuleDict(dict(
            wte = nn.Embedding(config.vocab_size, config.n_embd),
            drop = nn.Dropout(config.dropout),
            h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]),
            ln_f = LayerNorm(config.n_embd, bias=config.bias),
        ))

        # Task-specific "heads"
        if self.config.mode == 'generator':
            self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)
            self.transformer.wte.weight = self.lm_head.weight
        elif self.config.mode == 'reward':
            self.reward_head = nn.Sequential(
                nn.Linear(config.n_embd, 256),
                nn.ReLU(),
                nn.Linear(256, 2), # Outputs 2 raw scores for [natural, synthetic]
                nn.Softmax(dim=-1)  # Converts scores to probabilities
            )

        # ... (rest of __init__, including weight initialization)

    def forward(self, idx, targets=None):
        # ... (forward pass through the shared trunk to get `x`)
        tok_emb = self.transformer.wte(idx)
        x = self.transformer.drop(tok_emb)
        for block in self.transformer.h:
            x = block(x)
        x = self.transformer.ln_f(x)

        # Branching logic for the specific head
        if self.config.mode == 'generator':
            if targets is not None:
                logits = self.lm_head(x)
                loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-1)
            else:
                logits = self.lm_head(x[:, [-1], :])
                loss = None
            return logits, loss

        elif self.config.mode == 'reward':
            # Pool by taking the hidden state of the last token
            pooled_output = x[:, -1, :] # Shape: (batch_size, n_embd)
            # Get probabilities from the reward head
            probabilities = self.reward_head(pooled_output) # Shape: (batch_size, 2)
            
            # For reward model training, loss is calculated in the training script
            loss = None
            if targets is not None:
                # Example loss: Mean Squared Error
                loss = F.mse_loss(probabilities, targets)
            
            return probabilities, loss
```

## Part 3: Preparing Data for the Reward Model

This step is crucial. We will create a script prepare_reward_data.py that generates a labeled dataset for training the Reward Model. **To prevent data contamination, the reward model's training and validation sets must be generated from the base model's corresponding data splits.**

### prepare_reward_data.py Algorithm:

1. **Inputs:**
   * Path to the raw text data (input.txt).
   * Path to a pre-trained Base Model checkpoint (.pt file).
   * The block_size to use for samples.
   * The train/validation split ratio (e.g., 0.9).
2. **Process:**
   * Load the tokenizer (e.g., tiktoken).
   * Load the pre-trained Base Model and set it to evaluation mode (model.eval()).
   * Read the entire raw text dataset and tokenize it.
   * **Crucially, split the tokenized data into a training set and a validation set *before* generating any samples.** This split must be identical to the one used for the Base Model's training to ensure the datasets are perfectly aligned and independent.
   * **Generate Training Data:**
     * Iterate through the *training split* of the tokenized data.
     * For each sequence, perform the K crossover generation process to create (X_train, Y_train) pairs.
   * **Generate Validation Data:**
     * Iterate through the *validation split* of the tokenized data.
     * For each sequence, perform the K crossover generation process to create (X_val, Y_val) pairs.
3. **Output:**
   * Save the training pairs to a train.bin file in the reward_dataset directory.
   * Save the validation pairs to a val.bin file in the reward_dataset directory. A simple approach is to write the block_size integers for X, followed by the 2 floats for Y, and repeat for all samples.

## Part 4: The Iterative Training Cycle

This is the high-level orchestration of the entire training process.

### Cycle 0: Initial Base Model Training

* **Goal:** Train an initial, competent Base Model.
* **Action:** Run your standard train.py script on the original dataset (e.g., Shakespeare).
* **Stop Condition:** Employ an early stopping strategy based on validation loss dynamics to prevent overfitting and save computational resources. A robust set of criteria is:
  * **Early Stopping with Patience:** Don't stop at the first sign of an increase in validation loss. Instead, track the best validation loss achieved so far. Stop training only if the validation loss has failed to improve (i.e., decrease) for a specified number of evaluation steps, known as "patience" (e.g., 5-10 evaluations).
  * **Minimum Improvement Threshold (min_delta):** To avoid stopping due to insignificant fluctuations, consider an improvement valid only if the validation loss decreases by at least a small amount, or min_delta (e.g., 0.001).
  * **Practical Rule:** A good combined rule is: "Stop training if the validation loss has not decreased by at least min_delta over the last patience evaluation steps." This ensures the model has truly plateaued before training is halted.
* **Output:** A checkpoint file, base_model_v1.pt.

### The Main Loop: Cycles 1, 2, 3...

This loop iterates through four distinct steps. Let's assume we just finished Cycle N and have base_model_v(N+1).pt. We now begin Cycle N+1.

#### Step 1: Generate Reward Dataset

* **Goal:** Create a new dataset that challenges the *current* Base Model.
* **Action:**
  1. Take the latest Base Model checkpoint: base_model_v(N+1).pt.
  2. Run the prepare_reward_data.py script. It will use this model to generate the "synthetic" portions of the data, ensuring the train/val split of the source text is respected.
* **Output:** A new reward dataset (reward_data_v(N+1)/).

#### Step 2: Train the Reward Model

* **Goal:** Train a new Reward Model to get good at discriminating the outputs of the latest Base Model.
* **Action:**
  1. Write a new script train_reward_model.py.
  2. In this script, instantiate the GPT model with config.mode = 'reward'.
  3. Load the weights from base_model_v(N+1).pt. PyTorch will load the shared trunk weights and randomly initialize the new reward_head.
  4. **Freeze the shared trunk weights.** This is critical. We want to use the powerful, pre-trained language features from the Base Model as a feature extractor without altering them. Only the new reward_head should be trained. This is faster, more stable, and prevents overfitting.
     ```python
     # In train_reward_model.py, after loading the state dict
     for param in model.transformer.parameters():
         param.requires_grad = False
     ```
  5. Train this model on the dataset from Step 1. The optimizer will now only update the weights of the reward_head. The loss function should compare the model's output probabilities with the ground-truth ratios (e.g., using Mean Squared Error).
* **Stop Condition:** Train until validation loss on the reward dataset stops improving, using the same "patience" and "min_delta" criteria from Cycle 0.
* **Output:** A new Reward Model checkpoint, reward_model_v(N+1).pt.

#### Step 3: GRPO Training of the Base Model

* **Goal:** Use the new Reward Model to improve the Base Model via reinforcement learning.
* **Action:**
  1. Write a new script train_grpo.py.
  2. Load two models:
     * The **Base Model** (base_model_v(N+1).pt) in 'generator' mode. This is the policy we are updating.
     * The **Reward Model** (reward_model_v(N+1).pt) in 'reward' mode. Freeze its weights (.eval()).
  3. The GRPO loop:
     * **Sample a variable-length prompt:**
       1. First, sample a random prompt length, P_len, from a distribution (e.g., uniformly from [1, block_size / 2]). This ensures the model is trained on a variety of prompt lengths.
       2. Then, sample a prompt of P_len tokens from the original dataset.
     * Generate a full-length completion (up to block_size) using the Base Model, conditioned on the sampled prompt.
     * Pass the full completion to the frozen Reward Model to get a probability distribution. The reward is the first value: P(natural).
     * Use this reward signal to update the Base Model's weights using a policy gradient algorithm (GRPO/PPO). The objective is to maximize the reward.
* **When to Stop This Step?** This is a critical decision. You stop GRPO training and start a new cycle when the Base Model has effectively "solved" the current Reward Model.
  * **Primary Metric:** The average reward. When the average P(natural) score that the Base Model achieves from the Reward Model **plateaus or exceeds a high threshold (e.g., 0.9-0.95)**, it means the generator can consistently fool this version of the discriminator. It's time to build a better discriminator.
  * **Secondary Metric:** KL divergence between the policy (Base Model) before and after an update batch. If this divergence grows too large, the policy is changing too drastically, which can lead to instability. Capping it is a core part of PPO/GRPO.
* **Output:** An improved Base Model, base_model_v(N+2).pt.

#### Step 4: Loop or Terminate?

* **Go to Step 1:** You now have a new, improved base model. Repeat the cycle by using it to generate a new, harder reward dataset.
* **When to Break Execution?** The entire process can be stopped when:
  1. You complete a pre-defined number of cycles (e.g., 5-10 cycles).
  2. The overall improvement between cycles diminishes. You can measure this by tracking the perplexity of the Base Model on a held-out, global validation set. If perplexity stops improving cycle-over-cycle, the model has likely converged.
  3. Human evaluation. Ultimately, the goal is better text. Periodically generate samples and have humans rate them. If the quality is no longer improving, stop the process.

##### How to Measure Perplexity

Perplexity is a core metric for evaluating language models. It measures how "surprised" a model is by a sequence of text. A lower score is better, indicating the model was more confident in its predictions.

**The Mathematical Connection:** Perplexity is simply the exponentiation of the cross-entropy loss you already use for training.

Perplexity = e^(cross-entropy_loss)

**Calculation Steps:**

1. **Load Model and Data:** Load your Base Model in 'generator' mode and put it in evaluation mode (model.eval()). Use a data loader for your global validation set (the one held out from the very beginning).
2. **Calculate Average Loss:** Iterate through the entire validation set without updating gradients (with torch.no_grad():). For each batch, calculate the cross-entropy loss and keep a running average.
3. **Exponentiate:** Once you have the final average loss over the entire validation set, calculate math.exp(average_loss) to get the perplexity score.

Here is a sample implementation:

```python
import torch
import math

@torch.no_grad()
def calculate_perplexity(model, data_loader, device):
    """
    Calculates the perplexity of a model on a given dataset.
    """
    model.eval()  # Set the model to evaluation mode
    total_loss = 0
    num_batches = 0

    for batch in data_loader:
        inputs, targets = batch
        inputs = inputs.to(device)
        targets = targets.to(device)

        # Forward pass to get the loss
        _, loss = model(inputs, targets)

        if loss is not None:
            total_loss += loss.item()
            num_batches += 1

    model.train()  # Set the model back to training mode

    if num_batches == 0:
        return float('inf')

    # Calculate the average loss and then perplexity
    average_loss = total_loss / num_batches
    perplexity = math.exp(average_loss)
    
    return perplexity
```

This iterative, adversarial process provides a robust path toward improving your language model's generation quality without needing a predefined, explicit reward function.