
Detailed parameter count:
  total                  | Total:   52,149,504 | Trainable:    8,207,616
  token_embeddings       | Total:    2,451,456 | Trainable:    2,451,456
  position_embeddings    | Total:            0 | Trainable:            0
  attention_layers       | Total:    7,520,256 | Trainable:    2,211,840
  feed_forward_layers    | Total:    3,538,944 | Trainable:    3,538,944
  layer_norms            | Total:        4,608 | Trainable:        4,608
  final_layer_norm       | Total:          768 | Trainable:          768
  language_model_head    | Total:   38,633,472 | Trainable:            0
------------------------------------------------------------
step 0: train loss 10.9693, val loss 10.9694
iter 0: loss 10.9702, lr 0.00005, time 7199.87ms, mfu -100.00%
iter 10: loss 9.5993, lr 0.00055, time 565.23ms, mfu 32.70%
iter 20: loss 9.4339, lr 0.00104, time 572.18ms, mfu 32.66%
iter 30: loss 9.2281, lr 0.00154, time 580.78ms, mfu 32.57%
iter 40: loss 9.1852, lr 0.00204, time 578.90ms, mfu 32.51%
iter 50: loss 9.0106, lr 0.00254, time 587.95ms, mfu 32.40%
iter 60: loss 9.0446, lr 0.00303, time 591.06ms, mfu 32.29%
iter 70: loss 8.7243, lr 0.00353, time 588.14ms, mfu 32.20%
iter 80: loss 8.7382, lr 0.00403, time 583.24ms, mfu 32.15%
iter 90: loss 8.5918, lr 0.00453, time 584.37ms, mfu 32.10%
step 100: train loss 8.6006, val loss 8.6016
saving checkpoint to out

=== SCALING OPERATION TRIGGERED (DDP SYNC) ===
Operation: merge_lora_weights
Trigger reason: Timeout
Current val loss: 8.6016, Trigger loss: 1.0000
Iterations since last op: 100, Max wait: 100
Executing operation: merge_lora_weights with value: None
Performing architectural operation: merge_lora_weights
Merging LoRA weights into main weights...
LoRA weights merged and reset.

Detailed parameter count:
  total                  | Total:   52,149,504 | Trainable:    8,207,616
  token_embeddings       | Total:    2,451,456 | Trainable:    2,451,456
  position_embeddings    | Total:            0 | Trainable:            0
  attention_layers       | Total:    7,520,256 | Trainable:    2,211,840
  feed_forward_layers    | Total:    3,538,944 | Trainable:    3,538,944
  layer_norms            | Total:        4,608 | Trainable:        4,608
  final_layer_norm       | Total:          768 | Trainable:          768
  language_model_head    | Total:   38,633,472 | Trainable:            0
------------------------------------------------------------
Re-configuring optimizer after architectural change...
num decayed parameter tensors: 17, with 8,202,240 parameters
num non-decayed parameter tensors: 7, with 5,376 parameters
using fused AdamW: True
Transferring optimizer state for existing parameters...
Transferred optimizer state for 0 / 28 parameters
Re-compiling the model...
Architectural operation completed successfully.
=== SCALING OPERATION COMPLETE ===


=== SCALING OPERATION TRIGGERED (DDP SYNC) ===
Operation: change_lr
Trigger reason: Loss threshold
Current val loss: 8.6016, Trigger loss: 100.0000
Iterations since last op: 0, Max wait: 1
Executing operation: change_lr with value: 0.5
LR multiplier: 10.0000 -> 5.0000
=== SCALING OPERATION COMPLETE ===


=== SCALING OPERATION TRIGGERED (DDP SYNC) ===
Operation: reset_lr_schedule
Trigger reason: Loss threshold
Current val loss: 8.6016, Trigger loss: 100.0000
Iterations since last op: 0, Max wait: 1
Executing operation: reset_lr_schedule with value: None
LR schedule offset: 0 -> 100
=== SCALING OPERATION COMPLETE ===

iter 100: loss 8.5358, lr 0.00502, time 4973.18ms, mfu 29.26%
iter 110: loss 8.4667, lr 0.00027, time 571.47ms, mfu 29.57%
iter 120: loss 8.5395, lr 0.00052, time 577.18ms, mfu 29.81%
iter 130: loss 8.4527, lr 0.00077, time 579.08ms, mfu 30.02%
iter 140: loss 8.3773, lr 0.00102, time 579.15ms, mfu 30.21%
iter 150: loss 8.3683, lr 0.00127, time 579.41ms, mfu 30.38%
iter 160: loss 8.3230, lr 0.00152, time 579.89ms, mfu 30.53%
iter 170: loss 8.2196, lr 0.00177, time 582.98ms, mfu 30.65%
iter 180: loss 8.1573, lr 0.00201, time 583.06ms, mfu 30.75%
iter 190: loss 8.1706, lr 0.00226, time 585.09ms, mfu 30.83%
step 200: train loss 8.1212, val loss 8.1555
saving checkpoint to out

=== SCALING OPERATION TRIGGERED (DDP SYNC) ===
Operation: merge_lora_weights
Trigger reason: Timeout
Current val loss: 8.1555, Trigger loss: 1.0000
Iterations since last op: 100, Max wait: 100
Executing operation: merge_lora_weights with value: None
Performing architectural operation: merge_lora_weights
Merging LoRA weights into main weights...
LoRA weights merged and reset.

Detailed parameter count:
  total                  | Total:   52,149,504 | Trainable:    8,207,616
  token_embeddings       | Total:    2,451,456 | Trainable:    2,451,456
  position_embeddings    | Total:            0 | Trainable:            0
  attention_layers       | Total:    7,520,256 | Trainable:    2,211,840
  feed_forward_layers    | Total:    3,538,944 | Trainable:    3,538,944
  layer_norms            | Total:        4,608 | Trainable:        4,608
  final_layer_norm       | Total:          768 | Trainable:          768
  language_model_head    | Total:   38,633,472 | Trainable:            0
------------------------------------------------------------
Re-configuring optimizer after architectural change...
num decayed parameter tensors: 17, with 8,202,240 parameters
num non-decayed parameter tensors: 7, with 5,376 parameters
using fused AdamW: True
Transferring optimizer state for existing parameters...
Transferred optimizer state for 0 / 28 parameters
Re-compiling the model...
Architectural operation completed successfully.
Re-evaluating validation loss after operation...
New val loss after operation: 8.1678
iter 200: loss 8.2958, lr 0.00251, time 7032.01ms, mfu 28.01%
iter 210: loss 8.1602, lr 0.00276, time 587.81ms, mfu 28.36%
iter 220: loss 8.1695, lr 0.00301, time 588.87ms, mfu 28.66%
iter 230: loss 8.0821, lr 0.00326, time 586.93ms, mfu 28.94%
iter 240: loss 8.1005, lr 0.00351, time 591.62ms, mfu 29.17%
iter 250: loss 8.0150, lr 0.00376, time 584.23ms, mfu 29.42%
iter 260: loss 8.0613, lr 0.00400, time 585.52ms, mfu 29.63%
iter 270: loss 7.9513, lr 0.00425, time 587.80ms, mfu 29.81%
iter 280: loss 7.8380, lr 0.00450, time 584.70ms, mfu 29.99%
iter 290: loss 7.7504, lr 0.00475, time 586.44ms, mfu 30.14%
step 300: train loss 7.7577, val loss 7.8001
saving checkpoint to out

=== SCALING OPERATION TRIGGERED (DDP SYNC) ===
Operation: change_lr
Trigger reason: Loss threshold
Current val loss: 7.8001, Trigger loss: 100.0000
Iterations since last op: 100, Max wait: 1
Executing operation: change_lr with value: 0.5
LR multiplier: 5.0000 -> 2.5000
=== SCALING OPERATION COMPLETE ===


=== SCALING OPERATION TRIGGERED (DDP SYNC) ===
Operation: reset_lr_schedule
Trigger reason: Loss threshold
Current val loss: 7.8001, Trigger loss: 100.0000
Iterations since last op: 0, Max wait: 1
Executing operation: reset_lr_schedule with value: None
LR schedule offset: 100 -> 300
=== SCALING OPERATION COMPLETE ===


=== SCALING OPERATION TRIGGERED (DDP SYNC) ===
Operation: change_warmup_iters
Trigger reason: Loss threshold
Current val loss: 7.8001, Trigger loss: 100.0000
Iterations since last op: 0, Max wait: 1
Executing operation: change_warmup_iters with value: 2.0
Warmup iters multiplier: 1.0000 -> 2.0000
=== SCALING OPERATION COMPLETE ===

iter 300: loss 7.7803, lr 0.00500, time 4592.49ms, mfu 27.53%
iter 310: loss 7.8683, lr 0.00007, time 576.84ms, mfu 27.98%
iter 320: loss 7.6478, lr 0.00013, time 585.09ms, mfu 28.34%
iter 330: loss 7.7884, lr 0.00019, time 586.19ms, mfu 28.66%
iter 340: loss 7.8266, lr 0.00026, time 589.97ms, mfu 28.93%
iter 350: loss 7.7393, lr 0.00032, time 593.54ms, mfu 29.15%
iter 360: loss 7.7167, lr 0.00038, time 593.24ms, mfu 29.35%
iter 370: loss 7.6068, lr 0.00044, time 592.43ms, mfu 29.53%
iter 380: loss 7.6580, lr 0.00050, time 592.52ms, mfu 29.70%
iter 390: loss 7.5988, lr 0.00057, time 590.89ms, mfu 29.86%
step 400: train loss 7.6416, val loss 7.6300
saving checkpoint to out

=== SCALING OPERATION TRIGGERED (DDP SYNC) ===
Operation: merge_lora_weights
Trigger reason: Timeout
Current val loss: 7.6300, Trigger loss: 1.0000
Iterations since last op: 100, Max wait: 100
Executing operation: merge_lora_weights with value: None
Performing architectural operation: merge_lora_weights
Merging LoRA weights into main weights...
LoRA weights merged and reset.

Detailed parameter count:
  total                  | Total:   52,149,504 | Trainable:    8,207,616
  token_embeddings       | Total:    2,451,456 | Trainable:    2,451,456
  position_embeddings    | Total:            0 | Trainable:            0
  attention_layers       | Total:    7,520,256 | Trainable:    2,211,840
  feed_forward_layers    | Total:    3,538,944 | Trainable:    3,538,944
  layer_norms            | Total:        4,608 | Trainable:        4,608
  final_layer_norm       | Total:          768 | Trainable:          768
  language_model_head    | Total:   38,633,472 | Trainable:            0
------------------------------------------------------------
Re-configuring optimizer after architectural change...
num decayed parameter tensors: 17, with 8,202,240 parameters
num non-decayed parameter tensors: 7, with 5,376 parameters
using fused AdamW: True
Transferring optimizer state for existing parameters...
Transferred optimizer state for 0 / 28 parameters
Re-compiling the model...
Architectural operation completed successfully.
Re-evaluating validation loss after operation...
New val loss after operation: 7.7191
iter 400: loss 7.6448, lr 0.00063, time 7150.45ms, mfu 27.13%
iter 410: loss 7.7396, lr 0.00069, time 587.45ms, mfu 27.56%
iter 420: loss 7.5785, lr 0.00075, time 586.29ms, mfu 27.96%
iter 430: loss 7.6425, lr 0.00082, time 584.97ms, mfu 28.32%
iter 440: loss 7.6894, lr 0.00088, time 582.49ms, mfu 28.66%
iter 450: loss 7.7393, lr 0.00094, time 583.91ms, mfu 28.96%
iter 460: loss 7.4768, lr 0.00100, time 584.61ms, mfu 29.23%
iter 470: loss 7.4663, lr 0.00107, time 589.86ms, mfu 29.44%
iter 480: loss 7.5646, lr 0.00113, time 588.53ms, mfu 29.63%
iter 490: loss 7.5482, lr 0.00119, time 592.17ms, mfu 29.79%
step 500: train loss 7.5371, val loss 7.5310
saving checkpoint to out

=== SCALING OPERATION TRIGGERED (DDP SYNC) ===
Operation: stack_layers
Trigger reason: Loss threshold
Current val loss: 7.5310, Trigger loss: 100.0000
Iterations since last op: 100, Max wait: 1
Executing operation: stack_layers with value: 2
Performing architectural operation: stack_layers
Stacking layers: current depth 3, creating 6 total layers.
Model now has 6 layers.

Detailed parameter count:
  total                  | Total:   63,213,312 | Trainable:   13,963,008
  token_embeddings       | Total:    2,451,456 | Trainable:    2,451,456
  position_embeddings    | Total:            0 | Trainable:            0
  attention_layers       | Total:   15,040,512 | Trainable:    4,423,680
  feed_forward_layers    | Total:    7,077,888 | Trainable:    7,077,888
  layer_norms            | Total:        9,216 | Trainable:        9,216
  final_layer_norm       | Total:          768 | Trainable:          768
  language_model_head    | Total:   38,633,472 | Trainable:            0
------------------------------------------------------------
Re-configuring optimizer after architectural change...
num decayed parameter tensors: 32, with 13,953,024 parameters
num non-decayed parameter tensors: 13, with 9,984 parameters
using fused AdamW: True
Transferring optimizer state for existing parameters...
Transferred optimizer state for 0 / 52 parameters
Re-compiling the model...
Architectural operation completed successfully.
Re-evaluating validation loss after operation...
New val loss after operation: 8.0791
iter 500: loss 7.9938, lr 0.00125, time 10429.26ms, mfu 27.04%
iter 510: loss 7.6565, lr 0.00132, time 798.94ms, mfu 27.29%
iter 520: loss 7.5339, lr 0.00138, time 803.00ms, mfu 27.50%
iter 530: loss 7.5933, lr 0.00144, time 800.17ms, mfu 27.70%
iter 540: loss 7.4362, lr 0.00150, time 799.39ms, mfu 27.88%
iter 550: loss 7.5005, lr 0.00156, time 796.18ms, mfu 28.06%
iter 560: loss 7.5735, lr 0.00163, time 799.34ms, mfu 28.21%
iter 570: loss 7.3773, lr 0.00169, time 800.95ms, mfu 28.34%
iter 580: loss 7.4884, lr 0.00175, time 798.34ms, mfu 28.46%
Traceback (most recent call last):
  File "/teamspace/studios/this_studio/nanoGPT/train.py", line 632, in <module>
    logits, loss = model(X, Y)
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py", line 655, in _fn
    return fn(*args, **kwargs)
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
  File "/teamspace/studios/this_studio/nanoGPT/model.py", line 420, in forward
    def forward(self, idx, targets=None):
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py", line 838, in _fn
    return fn(*args, **kwargs)
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py", line 1201, in forward
    return compiled_fn(full_args)
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 315, in runtime_wrapper
    all_outs = call_func_at_runtime_with_args(
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/_functorch/_aot_autograd/utils.py", line 126, in call_func_at_runtime_with_args
    out = normalize_as_list(f(args))
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/_functorch/_aot_autograd/utils.py", line 100, in g
    return f(*args)
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/autograd/function.py", line 575, in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 1937, in forward
    fw_outs = call_func_at_runtime_with_args(
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/_functorch/_aot_autograd/utils.py", line 126, in call_func_at_runtime_with_args
    out = normalize_as_list(f(args))
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 495, in wrapper
    return compiled_fn(runtime_args)
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/_inductor/output_code.py", line 460, in __call__
    return self.current_callable(inputs)
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/_inductor/utils.py", line 2404, in run
    return model(new_inputs)
  File "/tmp/torchinductor_adamskrodzki/oy/coyddne3ev4fsq3er2vxviwf3boucrnynxjxskmx3vjricslukol.py", line 2051, in call
    extern_kernels.mm(reinterpret_tensor(buf193, (32768, 768), (768, 1), 0), buf192, out=buf194)
KeyboardInterrupt
Traceback (most recent call last):
  File "/teamspace/studios/this_studio/nanoGPT/train.py", line 632, in <module>
    logits, loss = model(X, Y)
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py", line 655, in _fn
    return fn(*args, **kwargs)
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
  File "/teamspace/studios/this_studio/nanoGPT/model.py", line 420, in forward
    def forward(self, idx, targets=None):
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py", line 838, in _fn
    return fn(*args, **kwargs)
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py", line 1201, in forward
    return compiled_fn(full_args)
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 315, in runtime_wrapper
    all_outs = call_func_at_runtime_with_args(
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/_functorch/_aot_autograd/utils.py", line 126, in call_func_at_runtime_with_args
    out = normalize_as_list(f(args))
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/_functorch/_aot_autograd/utils.py", line 100, in g
    return f(*args)
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/autograd/function.py", line 575, in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 1937, in forward
    fw_outs = call_func_at_runtime_with_args(
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/_functorch/_aot_autograd/utils.py", line 126, in call_func_at_runtime_with_args
    out = normalize_as_list(f(args))
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 495, in wrapper
    return compiled_fn(runtime_args)
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/_inductor/output_code.py", line 460, in __call__
    return self.current_callable(inputs)
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/_inductor/utils.py", line 2404, in run
    return model(new_inputs)
  File "/tmp/torchinductor_adamskrodzki/oy/coyddne3ev4fsq3er2vxviwf3boucrnynxjxskmx3vjricslukol.py", line 2051, in call
    extern_kernels.mm(reinterpret_tensor(buf193, (32768, 768), (768, 1), 0), buf192, out=buf194)
KeyboardInterrupt
Exception ignored in atexit callback: <function _start_and_connect_service.<locals>.teardown_atexit at 0x772598efc820>
Traceback (most recent call last):
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/wandb/sdk/lib/service/service_connection.py", line 54, in teardown_atexit
    conn.teardown(hooks.exit_code)
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/wandb/sdk/lib/service/service_connection.py", line 182, in teardown
    self._router.join()
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/wandb/sdk/interface/router.py", line 75, in join
    self._thread.join()
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/threading.py", line 1096, in join
    self._wait_for_tstate_lock()
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/threading.py", line 1116, in _wait_for_tstate_lock
    if lock.acquire(block, timeout):
KeyboardInterrupt:
Exception ignored in atexit callback: <function shutdown_compile_workers at 0x7725b5bcc550>
Traceback (most recent call last):
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/_inductor/async_compile.py", line 113, in shutdown_compile_workers
    pool.shutdown()
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/_inductor/compile_worker/subproc_pool.py", line 239, in shutdown
    self.process.wait(300)
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/subprocess.py", line 1222, in wait
    self._wait(timeout=sigint_timeout)
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/subprocess.py", line 1937, in _wait
    time.sleep(delay)
KeyboardInterrupt:
