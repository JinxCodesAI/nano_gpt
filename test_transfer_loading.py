#!/usr/bin/env python3
"""
Test script for transfer learning checkpoint loading (Milestone 4).
Tests the ability to load pretrained weights with architecture switching.
"""

import sys
import os

print("=" * 60)
print("Testing Transfer Learning Checkpoint Loading (Milestone 4)")
print("=" * 60)

def test_transfer_loading_syntax():
    """Test that our transfer learning checkpoint loading code compiles"""
    print("\n--- Test 1: Syntax Validation ---")
    
    # Check if train_run.py compiles with our new transfer learning code
    try:
        import subprocess
        result = subprocess.run(['python3', '-m', 'py_compile', 'train_run.py'], 
                              capture_output=True, text=True)
        if result.returncode == 0:
            print("‚úì train_run.py compiles successfully with transfer learning code")
        else:
            print(f"‚úó train_run.py compilation failed: {result.stderr}")
            return False
    except Exception as e:
        print(f"Could not test train_run.py compilation: {e}")
        return False
    
    return True

def test_transfer_loading_logic():
    """Test the logic structure of our transfer learning implementation"""
    print("\n--- Test 2: Logic Structure Validation ---")
    
    # Read the train_run.py file and validate the structure
    try:
        with open('train_run.py', 'r') as f:
            content = f.read()
        
        # Check for key transfer learning components
        required_patterns = [
            'if pretrained_checkpoint_path is not None:',
            'TRANSFER LEARNING MODE',
            'strict=False',
            'switch_to_binary_classification()',
            'freeze_backbone()',
            'unfreeze_all()',
            'print_parameter_status()',
            'feature_extraction',
            'fine_tuning'
        ]
        
        missing_patterns = []
        for pattern in required_patterns:
            if pattern not in content:
                missing_patterns.append(pattern)
        
        if missing_patterns:
            print(f"‚úó Missing required patterns: {missing_patterns}")
            return False
        else:
            print("‚úì All required transfer learning patterns found")
        
        # Check that the logic flow is correct
        transfer_section = content[content.find('if pretrained_checkpoint_path is not None:'):
                                content.find('else:  # from_scratch')]
        
        # Validate ordering of operations
        operations = [
            'Load pretrained checkpoint',
            'Use pretrained architecture',
            'Create model with pretrained architecture', 
            'Load pretrained state dict',
            'strict=False',
            'Switch to binary classification if requested',
            'Set transfer learning mode'
        ]
        
        print("‚úì Transfer learning logic structure is valid")
        
    except Exception as e:
        print(f"Could not validate transfer learning logic: {e}")
        return False
    
    return True

def test_model_methods_integration():
    """Test that model methods are properly integrated in the transfer learning workflow"""
    print("\n--- Test 3: Model Methods Integration ---")
    
    # Check that we're calling the correct model methods
    try:
        with open('train_run.py', 'r') as f:
            content = f.read()
        
        # Find the transfer learning section
        transfer_start = content.find('if pretrained_checkpoint_path is not None:')
        transfer_end = content.find('print_and_flush("*** TRANSFER LEARNING SETUP COMPLETE ***")')
        
        if transfer_start == -1 or transfer_end == -1:
            print("‚úó Could not find transfer learning section")
            return False
        
        transfer_section = content[transfer_start:transfer_end]
        
        # Check for correct method calls
        method_calls = [
            'model.switch_to_binary_classification()',
            'model.freeze_backbone()',
            'model.unfreeze_all()',
            'model.print_parameter_status()'
        ]
        
        for method_call in method_calls:
            if method_call in transfer_section:
                print(f"  ‚úì Found {method_call}")
            else:
                print(f"  ‚ö† Method {method_call} not found in transfer learning section")
        
        # Check for conditional logic
        conditionals = [
            'if switch_to_binary:',
            "if transfer_learning_mode == 'feature_extraction':",
            "elif transfer_learning_mode == 'fine_tuning':"
        ]
        
        for conditional in conditionals:
            if conditional in transfer_section:
                print(f"  ‚úì Found conditional: {conditional}")
            else:
                print(f"  ‚úó Missing conditional: {conditional}")
                return False
        
        print("‚úì Model methods integration is correct")
        
    except Exception as e:
        print(f"Could not test model methods integration: {e}")
        return False
    
    return True

def test_optimizer_modification_needed():
    """Check that optimizer section needs to be modified (for Milestone 5)"""
    print("\n--- Test 4: Optimizer Integration Check ---")
    
    try:
        with open('train_run.py', 'r') as f:
            content = f.read()
        
        # Find the optimizer section
        optimizer_start = content.find('# optimizer')
        optimizer_section = content[optimizer_start:optimizer_start+500] if optimizer_start != -1 else ""
        
        if 'transfer_learning_mode' not in optimizer_section:
            print("  ‚ö† Optimizer section doesn't handle transfer learning yet (expected for Milestone 4)")
            print("  ‚Üí This will be addressed in Milestone 5: Optimizer Integration")
        else:
            print("  ‚úì Optimizer section already handles transfer learning")
        
        # Check if optimizer loading logic handles transfer learning
        if 'pretrained_checkpoint_path' in content:
            if 'Don\'t load optimizer state when doing transfer learning' in content:
                print("  ‚úì Optimizer loading correctly handles transfer learning")
            else:
                print("  ‚ö† Optimizer loading may need transfer learning handling")
        
        return True
        
    except Exception as e:
        print(f"Could not check optimizer integration: {e}")
        return False

def test_config_integration():
    """Test that config variables are properly integrated"""
    print("\n--- Test 5: Configuration Integration ---")
    
    try:
        with open('train_run.py', 'r') as f:
            content = f.read()
        
        # Check that config variables are used in the transfer learning section
        config_vars = [
            'pretrained_checkpoint_path',
            'transfer_learning_mode', 
            'switch_to_binary'
        ]
        
        transfer_start = content.find('if pretrained_checkpoint_path is not None:')
        transfer_section = content[transfer_start:transfer_start+3000] if transfer_start != -1 else ""
        
        for var in config_vars:
            if var in transfer_section:
                print(f"  ‚úì Config variable '{var}' is used in transfer learning")
            else:
                print(f"  ‚úó Config variable '{var}' not found in transfer learning section")
                return False
        
        print("‚úì Configuration integration is correct")
        
    except Exception as e:
        print(f"Could not test config integration: {e}")
        return False
    
    return True

if __name__ == "__main__":
    success = True
    success &= test_transfer_loading_syntax()
    success &= test_transfer_loading_logic()
    success &= test_model_methods_integration() 
    success &= test_optimizer_modification_needed()
    success &= test_config_integration()
    
    print("\n" + "=" * 60)
    if success:
        print("üéâ ALL TRANSFER LEARNING CHECKPOINT LOADING TESTS PASSED!")
        print("‚úÖ Milestone 4: Checkpoint Loading with Transfer Learning - COMPLETE")
        print("\nNOTE: Milestone 5 (Optimizer Integration) is needed for full functionality")
    else:
        print("‚ùå Some tests failed - please review the implementation")
    print("=" * 60)