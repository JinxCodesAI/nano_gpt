
Detailed parameter count:
  total                  | Total:   52,149,504 | Trainable:    8,207,616
  token_embeddings       | Total:    2,451,456 | Trainable:    2,451,456
  position_embeddings    | Total:            0 | Trainable:            0
  attention_layers       | Total:    7,520,256 | Trainable:    2,211,840
  feed_forward_layers    | Total:    3,538,944 | Trainable:    3,538,944
  layer_norms            | Total:        4,608 | Trainable:        4,608
  final_layer_norm       | Total:          768 | Trainable:          768
  language_model_head    | Total:   38,633,472 | Trainable:            0
------------------------------------------------------------
W0720 07:03:28.739000 4054 /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/_inductor/utils.py:1250] [0/0] Not enough SMs to use max_autotune_gemm mode
step 0: train loss 10.9683, val loss 10.9694
iter 0: loss 10.9703, lr 0.00005, time 30090.82ms, mfu -100.00%
iter 10: loss 9.6125, lr 0.00055, time 519.01ms, mfu 35.61%
iter 20: loss 9.4394, lr 0.00104, time 532.34ms, mfu 35.52%
iter 30: loss 9.2917, lr 0.00154, time 539.10ms, mfu 35.39%
iter 40: loss 9.1731, lr 0.00204, time 542.68ms, mfu 35.26%
iter 50: loss 9.0320, lr 0.00254, time 544.79ms, mfu 35.13%
iter 60: loss 8.7354, lr 0.00303, time 556.49ms, mfu 34.93%
iter 70: loss 8.7188, lr 0.00353, time 559.40ms, mfu 34.74%
iter 80: loss 8.7096, lr 0.00403, time 563.49ms, mfu 34.55%
iter 90: loss 8.5735, lr 0.00453, time 558.53ms, mfu 34.40%
step 100: train loss 8.5169, val loss 8.5795
saving checkpoint to out

=== SCALING OPERATION TRIGGERED (DDP SYNC) ===
Operation: merge_lora_weights
Trigger reason: Timeout
Current val loss: 8.5795, Trigger loss: 1.0000
Iterations since last op: 100, Max wait: 100
Executing operation: merge_lora_weights with value: None
Performing architectural operation: merge_lora_weights
Merging LoRA weights into main weights...
LoRA weights merged and reset.

Detailed parameter count:
  total                  | Total:   52,149,504 | Trainable:    8,207,616
  token_embeddings       | Total:    2,451,456 | Trainable:    2,451,456
  position_embeddings    | Total:            0 | Trainable:            0
  attention_layers       | Total:    7,520,256 | Trainable:    2,211,840
  feed_forward_layers    | Total:    3,538,944 | Trainable:    3,538,944
  layer_norms            | Total:        4,608 | Trainable:        4,608
  final_layer_norm       | Total:          768 | Trainable:          768
  language_model_head    | Total:   38,633,472 | Trainable:            0
------------------------------------------------------------
Re-configuring optimizer after architectural change...
num decayed parameter tensors: 17, with 8,202,240 parameters
num non-decayed parameter tensors: 7, with 5,376 parameters
using fused AdamW: True
Transferring optimizer state for existing parameters...
Transferred optimizer state for 0 / 28 parameters
Re-compiling the model...
Architectural operation completed successfully.
=== SCALING OPERATION COMPLETE ===


=== SCALING OPERATION TRIGGERED (DDP SYNC) ===
Operation: change_lr
Trigger reason: Loss threshold
Current val loss: 8.5795, Trigger loss: 100.0000
Iterations since last op: 0, Max wait: 1
Executing operation: change_lr with value: 0.5
LR multiplier: 10.0000 -> 5.0000
=== SCALING OPERATION COMPLETE ===


=== SCALING OPERATION TRIGGERED (DDP SYNC) ===
Operation: reset_lr_schedule
Trigger reason: Loss threshold
Current val loss: 8.5795, Trigger loss: 100.0000
Iterations since last op: 0, Max wait: 1
Executing operation: reset_lr_schedule with value: None
LR schedule offset: 0 -> 100
=== SCALING OPERATION COMPLETE ===

iter 100: loss 8.5281, lr 0.00502, time 4960.87ms, mfu 31.34%
iter 110: loss 8.6119, lr 0.00027, time 572.63ms, mfu 31.43%
iter 120: loss 8.3926, lr 0.00052, time 575.90ms, mfu 31.50%
iter 130: loss 8.3146, lr 0.00077, time 580.49ms, mfu 31.53%
iter 140: loss 8.4041, lr 0.00102, time 581.74ms, mfu 31.55%
iter 150: loss 8.2669, lr 0.00127, time 575.91ms, mfu 31.61%
iter 160: loss 8.3741, lr 0.00152, time 583.82ms, mfu 31.61%
iter 170: loss 8.2295, lr 0.00177, time 580.20ms, mfu 31.64%
iter 180: loss 8.0643, lr 0.00201, time 578.82ms, mfu 31.67%
iter 190: loss 8.1588, lr 0.00226, time 573.21ms, mfu 31.72%
step 200: train loss 8.0764, val loss 8.1283
saving checkpoint to out

=== SCALING OPERATION TRIGGERED (DDP SYNC) ===
Operation: merge_lora_weights
Trigger reason: Timeout
Current val loss: 8.1283, Trigger loss: 1.0000
Iterations since last op: 100, Max wait: 100
Executing operation: merge_lora_weights with value: None
Performing architectural operation: merge_lora_weights
Merging LoRA weights into main weights...
LoRA weights merged and reset.

Detailed parameter count:
  total                  | Total:   52,149,504 | Trainable:    8,207,616
  token_embeddings       | Total:    2,451,456 | Trainable:    2,451,456
  position_embeddings    | Total:            0 | Trainable:            0
  attention_layers       | Total:    7,520,256 | Trainable:    2,211,840
  feed_forward_layers    | Total:    3,538,944 | Trainable:    3,538,944
  layer_norms            | Total:        4,608 | Trainable:        4,608
  final_layer_norm       | Total:          768 | Trainable:          768
  language_model_head    | Total:   38,633,472 | Trainable:            0
------------------------------------------------------------
Re-configuring optimizer after architectural change...
num decayed parameter tensors: 17, with 8,202,240 parameters
num non-decayed parameter tensors: 7, with 5,376 parameters
using fused AdamW: True
Transferring optimizer state for existing parameters...
Transferred optimizer state for 0 / 28 parameters
Re-compiling the model...
Architectural operation completed successfully.
=== SCALING OPERATION COMPLETE ===


=== SCALING OPERATION TRIGGERED (DDP SYNC) ===
Operation: change_warmup_iters
Trigger reason: Loss threshold
Current val loss: 8.1283, Trigger loss: 100.0000
Iterations since last op: 0, Max wait: 1
Executing operation: change_warmup_iters with value: 2.0
Warmup iters multiplier: 1.0000 -> 2.0000
=== SCALING OPERATION COMPLETE ===


=== SCALING OPERATION TRIGGERED (DDP SYNC) ===
Operation: reset_lr_schedule
Trigger reason: Loss threshold
Current val loss: 8.1283, Trigger loss: 100.0000
Iterations since last op: 0, Max wait: 1
Executing operation: reset_lr_schedule with value: None
LR schedule offset: 100 -> 200
=== SCALING OPERATION COMPLETE ===


=== SCALING OPERATION TRIGGERED (DDP SYNC) ===
Operation: change_batch_size
Trigger reason: Loss threshold
Current val loss: 8.1283, Trigger loss: 100.0000
Iterations since last op: 0, Max wait: 1
Executing operation: change_batch_size with value: 0.5
Batch size: 32 -> 16
=== SCALING OPERATION COMPLETE ===


=== SCALING OPERATION TRIGGERED (DDP SYNC) ===
Operation: change_grad_accum
Trigger reason: Loss threshold
Current val loss: 8.1283, Trigger loss: 100.0000
Iterations since last op: 0, Max wait: 1
Executing operation: change_grad_accum with value: 2.0
Grad accum steps: 2 -> 4
=== SCALING OPERATION COMPLETE ===


=== SCALING OPERATION TRIGGERED (DDP SYNC) ===
Operation: change_lr
Trigger reason: Loss threshold
Current val loss: 8.1283, Trigger loss: 100.0000
Iterations since last op: 0, Max wait: 1
Executing operation: change_lr with value: 0.5
LR multiplier: 5.0000 -> 2.5000
=== SCALING OPERATION COMPLETE ===


=== SCALING OPERATION TRIGGERED (DDP SYNC) ===
Operation: reset_lr_schedule
Trigger reason: Loss threshold
Current val loss: 8.1283, Trigger loss: 100.0000
Iterations since last op: 0, Max wait: 1
Executing operation: reset_lr_schedule with value: None
LR schedule offset: 200 -> 200
=== SCALING OPERATION COMPLETE ===


=== SCALING OPERATION TRIGGERED (DDP SYNC) ===
Operation: stack_layers
Trigger reason: Loss threshold
Current val loss: 8.1283, Trigger loss: 100.0000
Iterations since last op: 0, Max wait: 1
Executing operation: stack_layers with value: 2
Performing architectural operation: stack_layers
Stacking layers: current depth 3, creating 6 total layers.
Model now has 6 layers.

Detailed parameter count:
  total                  | Total:   63,213,312 | Trainable:   13,963,008
  token_embeddings       | Total:    2,451,456 | Trainable:    2,451,456
  position_embeddings    | Total:            0 | Trainable:            0
  attention_layers       | Total:   15,040,512 | Trainable:    4,423,680
  feed_forward_layers    | Total:    7,077,888 | Trainable:    7,077,888
  layer_norms            | Total:        9,216 | Trainable:        9,216
  final_layer_norm       | Total:          768 | Trainable:          768
  language_model_head    | Total:   38,633,472 | Trainable:            0
------------------------------------------------------------
Re-configuring optimizer after architectural change...
num decayed parameter tensors: 32, with 13,953,024 parameters
num non-decayed parameter tensors: 13, with 9,984 parameters
using fused AdamW: True
Transferring optimizer state for existing parameters...
Transferred optimizer state for 0 / 52 parameters
Re-compiling the model...
Architectural operation completed successfully.
Re-evaluating validation loss after operation...
New val loss after operation: 8.9082
iter 200: loss 8.9708, lr 0.00251, time 43892.12ms, mfu 28.60%
iter 210: loss 8.2290, lr 0.00007, time 749.10ms, mfu 28.90%
iter 220: loss 8.0840, lr 0.00013, time 762.29ms, mfu 29.10%
iter 230: loss 8.1307, lr 0.00019, time 773.20ms, mfu 29.25%
iter 240: loss 8.0985, lr 0.00026, time 776.51ms, mfu 29.36%
iter 250: loss 8.0614, lr 0.00032, time 767.25ms, mfu 29.50%
iter 260: loss 7.9971, lr 0.00038, time 767.42ms, mfu 29.63%
iter 270: loss 7.9176, lr 0.00044, time 757.32ms, mfu 29.78%
iter 280: loss 7.9108, lr 0.00050, time 759.59ms, mfu 29.91%
iter 290: loss 8.0338, lr 0.00057, time 757.89ms, mfu 30.04%
step 300: train loss 7.9453, val loss 8.0249
saving checkpoint to out

=== SCALING OPERATION TRIGGERED (DDP SYNC) ===
Operation: merge_lora_weights
Trigger reason: Timeout
Current val loss: 8.0249, Trigger loss: 1.0000
Iterations since last op: 100, Max wait: 100
Executing operation: merge_lora_weights with value: None
Performing architectural operation: merge_lora_weights
Merging LoRA weights into main weights...
LoRA weights merged and reset.

Detailed parameter count:
  total                  | Total:   63,213,312 | Trainable:   13,963,008
  token_embeddings       | Total:    2,451,456 | Trainable:    2,451,456
  position_embeddings    | Total:            0 | Trainable:            0
  attention_layers       | Total:   15,040,512 | Trainable:    4,423,680
  feed_forward_layers    | Total:    7,077,888 | Trainable:    7,077,888
  layer_norms            | Total:        9,216 | Trainable:        9,216
  final_layer_norm       | Total:          768 | Trainable:          768
  language_model_head    | Total:   38,633,472 | Trainable:            0
------------------------------------------------------------
Re-configuring optimizer after architectural change...
num decayed parameter tensors: 32, with 13,953,024 parameters
num non-decayed parameter tensors: 13, with 9,984 parameters
using fused AdamW: True
Transferring optimizer state for existing parameters...
Transferred optimizer state for 0 / 52 parameters
Re-compiling the model...
Architectural operation completed successfully.
=== SCALING OPERATION COMPLETE ===

iter 300: loss 7.8914, lr 0.00063, time 3297.15ms, mfu 27.75%
iter 310: loss 8.1820, lr 0.00069, time 754.90ms, mfu 28.10%
iter 320: loss 7.8718, lr 0.00075, time 756.38ms, mfu 28.41%
iter 330: loss 8.0283, lr 0.00082, time 757.17ms, mfu 28.69%
iter 340: loss 8.0041, lr 0.00088, time 761.34ms, mfu 28.92%
iter 350: loss 7.8717, lr 0.00094, time 760.23ms, mfu 29.14%
iter 360: loss 7.8979, lr 0.00100, time 764.98ms, mfu 29.31%
iter 370: loss 7.9524, lr 0.00107, time 764.31ms, mfu 29.47%
iter 380: loss 7.7991, lr 0.00113, time 766.46ms, mfu 29.60%
iter 390: loss 7.8211, lr 0.00119, time 769.02ms, mfu 29.71%
step 400: train loss 7.7928, val loss 7.7827
saving checkpoint to out

=== SCALING OPERATION TRIGGERED (DDP SYNC) ===
Operation: merge_lora_weights
Trigger reason: Timeout
Current val loss: 7.7827, Trigger loss: 1.0000
Iterations since last op: 100, Max wait: 100
Executing operation: merge_lora_weights with value: None
Performing architectural operation: merge_lora_weights
Merging LoRA weights into main weights...
LoRA weights merged and reset.

Detailed parameter count:
  total                  | Total:   63,213,312 | Trainable:   13,963,008
  token_embeddings       | Total:    2,451,456 | Trainable:    2,451,456
  position_embeddings    | Total:            0 | Trainable:            0
  attention_layers       | Total:   15,040,512 | Trainable:    4,423,680
  feed_forward_layers    | Total:    7,077,888 | Trainable:    7,077,888
  layer_norms            | Total:        9,216 | Trainable:        9,216
  final_layer_norm       | Total:          768 | Trainable:          768
  language_model_head    | Total:   38,633,472 | Trainable:            0
------------------------------------------------------------
Re-configuring optimizer after architectural change...
num decayed parameter tensors: 32, with 13,953,024 parameters
num non-decayed parameter tensors: 13, with 9,984 parameters
using fused AdamW: True
Transferring optimizer state for existing parameters...
Transferred optimizer state for 0 / 52 parameters
Re-compiling the model...
Architectural operation completed successfully.
=== SCALING OPERATION COMPLETE ===


=== SCALING OPERATION TRIGGERED (DDP SYNC) ===
Operation: change_batch_size
Trigger reason: Loss threshold
Current val loss: 7.7827, Trigger loss: 100.0000
Iterations since last op: 0, Max wait: 1
Executing operation: change_batch_size with value: 0.5
Batch size: 16 -> 8
=== SCALING OPERATION COMPLETE ===


=== SCALING OPERATION TRIGGERED (DDP SYNC) ===
Operation: change_grad_accum
Trigger reason: Loss threshold
Current val loss: 7.7827, Trigger loss: 100.0000
Iterations since last op: 0, Max wait: 1
Executing operation: change_grad_accum with value: 2.0
Grad accum steps: 4 -> 8
=== SCALING OPERATION COMPLETE ===


=== SCALING OPERATION TRIGGERED (DDP SYNC) ===
Operation: stack_layers
Trigger reason: Loss threshold
Current val loss: 7.7827, Trigger loss: 100.0000
Iterations since last op: 0, Max wait: 1
Executing operation: stack_layers with value: 2
Performing architectural operation: stack_layers
Stacking layers: current depth 6, creating 12 total layers.
Model now has 12 layers.

Detailed parameter count:
  total                  | Total:   85,340,928 | Trainable:   25,473,792
  token_embeddings       | Total:    2,451,456 | Trainable:    2,451,456
  position_embeddings    | Total:            0 | Trainable:            0
  attention_layers       | Total:   30,081,024 | Trainable:    8,847,360
  feed_forward_layers    | Total:   14,155,776 | Trainable:   14,155,776
  layer_norms            | Total:       18,432 | Trainable:       18,432
  final_layer_norm       | Total:          768 | Trainable:          768
  language_model_head    | Total:   38,633,472 | Trainable:            0
------------------------------------------------------------
Re-configuring optimizer after architectural change...
num decayed parameter tensors: 62, with 25,454,592 parameters
num non-decayed parameter tensors: 25, with 19,200 parameters
using fused AdamW: True
Transferring optimizer state for existing parameters...
Transferred optimizer state for 0 / 100 parameters
Re-compiling the model...
Architectural operation completed successfully.
Re-evaluating validation loss after operation...
New val loss after operation: 8.1269
iter 400: loss 8.2412, lr 0.00125, time 67873.61ms, mfu 26.79%
iter 410: loss 8.0209, lr 0.00132, time 1139.91ms, mfu 27.08%
iter 420: loss 7.7314, lr 0.00138, time 1172.31ms, mfu 27.26%
iter 430: loss 7.9255, lr 0.00144, time 1165.09ms, mfu 27.44%
iter 440: loss 7.7139, lr 0.00150, time 1157.70ms, mfu 27.62%
iter 450: loss 7.6477, lr 0.00156, time 1139.01ms, mfu 27.83%
iter 460: loss 7.6020, lr 0.00163, time 1144.96ms, mfu 28.01%
iter 470: loss 7.4313, lr 0.00169, time 1138.90ms, mfu 28.18%
iter 480: loss 7.4825, lr 0.00175, time 1137.63ms, mfu 28.34%
iter 490: loss 7.3410, lr 0.00181, time 1140.75ms, mfu 28.48%
step 500: train loss 7.7008, val loss 7.6515
saving checkpoint to out

=== SCALING OPERATION TRIGGERED (DDP SYNC) ===
Operation: change_lr
Trigger reason: Loss threshold
Current val loss: 7.6515, Trigger loss: 100.0000
Iterations since last op: 100, Max wait: 1
Executing operation: change_lr with value: 0.5
LR multiplier: 2.5000 -> 1.2500
=== SCALING OPERATION COMPLETE ===


=== SCALING OPERATION TRIGGERED (DDP SYNC) ===
Operation: reset_lr_schedule
Trigger reason: Loss threshold
Current val loss: 7.6515, Trigger loss: 100.0000
Iterations since last op: 0, Max wait: 1
Executing operation: reset_lr_schedule with value: None
LR schedule offset: 200 -> 500
=== SCALING OPERATION COMPLETE ===

iter 500: loss 7.6433, lr 0.00188, time 3713.83ms, mfu 26.54%
iter 510: loss 7.6875, lr 0.00003, time 1156.62ms, mfu 26.81%
iter 520: loss 7.6198, lr 0.00007, time 1162.39ms, mfu 27.05%
iter 530: loss 7.5514, lr 0.00010, time 1150.17ms, mfu 27.29%
iter 540: loss 7.5627, lr 0.00013, time 1159.63ms, mfu 27.48%
iter 550: loss 7.7666, lr 0.00016, time 1154.65ms, mfu 27.66%
iter 560: loss 7.0109, lr 0.00019, time 1143.02ms, mfu 27.86%
iter 570: loss 7.4983, lr 0.00022, time 1142.31ms, mfu 28.04%
iter 580: loss 7.7031, lr 0.00025, time 1137.89ms, mfu 28.21%
iter 590: loss 7.7931, lr 0.00028, time 1140.51ms, mfu 28.36%
step 600: train loss 7.5068, val loss 7.3937
saving checkpoint to out

=== SCALING OPERATION TRIGGERED (DDP SYNC) ===
Operation: merge_lora_weights
Trigger reason: Timeout
Current val loss: 7.3937, Trigger loss: 1.0000
Iterations since last op: 100, Max wait: 100
Executing operation: merge_lora_weights with value: None
Performing architectural operation: merge_lora_weights
Merging LoRA weights into main weights...
LoRA weights merged and reset.

Detailed parameter count:
  total                  | Total:   85,340,928 | Trainable:   25,473,792
  token_embeddings       | Total:    2,451,456 | Trainable:    2,451,456
  position_embeddings    | Total:            0 | Trainable:            0
  attention_layers       | Total:   30,081,024 | Trainable:    8,847,360
  feed_forward_layers    | Total:   14,155,776 | Trainable:   14,155,776
  layer_norms            | Total:       18,432 | Trainable:       18,432
  final_layer_norm       | Total:          768 | Trainable:          768
  language_model_head    | Total:   38,633,472 | Trainable:            0
------------------------------------------------------------
Re-configuring optimizer after architectural change...
num decayed parameter tensors: 62, with 25,454,592 parameters
num non-decayed parameter tensors: 25, with 19,200 parameters
using fused AdamW: True
Transferring optimizer state for existing parameters...
Transferred optimizer state for 0 / 100 parameters
Re-compiling the model...
Architectural operation completed successfully.
Re-evaluating validation loss after operation...
New val loss after operation: 7.5311
iter 600: loss 7.1954, lr 0.00031, time 4740.78ms, mfu 26.24%
iter 610: loss 7.7576, lr 0.00035, time 1141.77ms, mfu 26.58%
iter 620: loss 7.3895, lr 0.00038, time 1159.63ms, mfu 26.84%
iter 630: loss 7.5763, lr 0.00041, time 1158.76ms, mfu 27.08%
iter 640: loss 7.5156, lr 0.00044, time 1152.23ms, mfu 27.31%
iter 650: loss 7.4014, lr 0.00047, time 1147.17ms, mfu 27.53%
iter 660: loss 7.3555, lr 0.00050, time 1154.97ms, mfu 27.71%
iter 670: loss 7.5588, lr 0.00053, time 1156.59ms, mfu 27.87%
iter 680: loss 7.3218, lr 0.00056, time 1157.11ms, mfu 28.01%
iter 690: loss 7.4985, lr 0.00060, time 1152.67ms, mfu 28.15%
step 700: train loss 7.4752, val loss 7.4423
saving checkpoint to out
iter 700: loss 7.6872, lr 0.00063, time 3640.96ms, mfu 26.26%
iter 710: loss 7.3642, lr 0.00066, time 1152.03ms, mfu 26.58%
iter 720: loss 7.5189, lr 0.00069, time 1150.75ms, mfu 26.86%
iter 730: loss 7.1990, lr 0.00072, time 1157.87ms, mfu 27.10%
iter 740: loss 7.2744, lr 0.00075, time 1163.52ms, mfu 27.30%
iter 750: loss 7.1761, lr 0.00078, time 1156.69ms, mfu 27.50%
iter 760: loss 7.1811, lr 0.00081, time 1149.59ms, mfu 27.69%
iter 770: loss 7.2498, lr 0.00084, time 1158.15ms, mfu 27.85%
iter 780: loss 7.5354, lr 0.00088, time 1151.16ms, mfu 28.01%
iter 790: loss 7.1721, lr 0.00091, time 1154.30ms, mfu 28.14%
step 800: train loss 7.4126, val loss 7.2960
saving checkpoint to out
iter 800: loss 7.5379, lr 0.00094, time 3636.60ms, mfu 26.26%
iter 810: loss 7.1904, lr 0.00097, time 1144.11ms, mfu 26.59%
iter 820: loss 7.0671, lr 0.00100, time 1156.89ms, mfu 26.86%
iter 830: loss 7.2422, lr 0.00103, time 1152.09ms, mfu 27.11%
iter 840: loss 7.0983, lr 0.00106, time 1160.83ms, mfu 27.32%
iter 850: loss 7.1345, lr 0.00109, time 1147.10ms, mfu 27.54%
iter 860: loss 7.4240, lr 0.00113, time 1152.83ms, mfu 27.72%
iter 870: loss 7.2657, lr 0.00116, time 1151.28ms, mfu 27.89%
iter 880: loss 7.5221, lr 0.00119, time 1149.98ms, mfu 28.05%
iter 890: loss 7.2640, lr 0.00122, time 1153.25ms, mfu 28.18%
step 900: train loss 7.3267, val loss 7.2834
saving checkpoint to out
iter 900: loss 6.9039, lr 0.00125, time 3567.08ms, mfu 26.31%
iter 910: loss 7.1335, lr 0.00125, time 1152.89ms, mfu 26.62%
iter 920: loss 7.4038, lr 0.00125, time 1140.72ms, mfu 26.93%
iter 930: loss 7.4306, lr 0.00125, time 1142.54ms, mfu 27.20%
iter 940: loss 7.3620, lr 0.00125, time 1149.42ms, mfu 27.42%
iter 950: loss 6.6313, lr 0.00125, time 1153.23ms, mfu 27.62%
iter 960: loss 7.3363, lr 0.00125, time 1158.60ms, mfu 27.78%
iter 970: loss 7.1185, lr 0.00125, time 1154.76ms, mfu 27.93%
iter 980: loss 7.1665, lr 0.00125, time 1153.54ms, mfu 28.08%
iter 990: loss 6.9974, lr 0.00125, time 1152.11ms, mfu 28.21%
step 1000: train loss 7.1206, val loss 7.1018
saving checkpoint to out
iter 1000: loss 7.2409, lr 0.00125, time 3622.64ms, mfu 26.32%
iter 1010: loss 7.3030, lr 0.00125, time 1153.98ms, mfu 26.63%
iter 1020: loss 7.1387, lr 0.00125, time 1146.64ms, mfu 26.92%
iter 1030: loss 7.1465, lr 0.00125, time 1147.69ms, mfu 27.18%
iter 1040: loss 7.0017, lr 0.00125, time 1158.46ms, mfu 27.38%
iter 1050: loss 7.0333, lr 0.00125, time 1155.16ms, mfu 27.58%
iter 1060: loss 7.1023, lr 0.00125, time 1161.94ms, mfu 27.73%
iter 1070: loss 6.7395, lr 0.00125, time 1153.95ms, mfu 27.89%
iter 1080: loss 6.7479, lr 0.00125, time 1157.84ms, mfu 28.03%
iter 1090: loss 7.0331, lr 0.00125, time 1161.31ms, mfu 28.14%
step 1100: train loss 6.9752, val loss 7.0048
saving checkpoint to out
iter 1100: loss 7.0062, lr 0.00125, time 3714.53ms, mfu 26.24%
iter 1110: loss 6.8227, lr 0.00125, time 1152.20ms, mfu 26.56%
iter 1120: loss 7.1823, lr 0.00125, time 1158.48ms, mfu 26.82%
iter 1130: loss 7.1600, lr 0.00125, time 1158.32ms, mfu 27.06%
iter 1140: loss 7.2278, lr 0.00125, time 1161.05ms, mfu 27.28%
iter 1150: loss 6.7974, lr 0.00125, time 1156.33ms, mfu 27.48%
iter 1160: loss 7.1157, lr 0.00125, time 1156.81ms, mfu 27.66%
iter 1170: loss 6.7243, lr 0.00125, time 1146.51ms, mfu 27.84%
iter 1180: loss 7.2943, lr 0.00125, time 1151.42ms, mfu 28.00%
iter 1190: loss 6.6796, lr 0.00125, time 1148.65ms, mfu 28.15%
step 1200: train loss 6.8896, val loss 6.8119
saving checkpoint to out
iter 1200: loss 6.8542, lr 0.00125, time 4388.08ms, mfu 26.11%
iter 1210: loss 7.0144, lr 0.00125, time 1151.23ms, mfu 26.44%
iter 1220: loss 6.8191, lr 0.00125, time 1159.41ms, mfu 26.72%
iter 1230: loss 7.2141, lr 0.00125, time 1159.30ms, mfu 26.96%
iter 1240: loss 6.8278, lr 0.00125, time 1157.48ms, mfu 27.19%
iter 1250: loss 6.9095, lr 0.00125, time 1151.93ms, mfu 27.42%
iter 1260: loss 7.1929, lr 0.00125, time 1152.37ms, mfu 27.61%
iter 1270: loss 6.9858, lr 0.00125, time 1153.64ms, mfu 27.79%
iter 1280: loss 6.7235, lr 0.00125, time 1147.83ms, mfu 27.96%
iter 1290: loss 6.7899, lr 0.00125, time 1139.70ms, mfu 28.13%
step 1300: train loss 6.9469, val loss 6.8441
saving checkpoint to out
iter 1300: loss 6.4094, lr 0.00125, time 4118.51ms, mfu 26.14%
iter 1310: loss 6.7931, lr 0.00125, time 1143.85ms, mfu 26.49%
iter 1320: loss 6.7849, lr 0.00125, time 1158.90ms, mfu 26.76%
iter 1330: loss 7.0544, lr 0.00125, time 1149.49ms, mfu 27.03%
iter 1340: loss 6.8400, lr 0.00125, time 1160.12ms, mfu 27.25%
iter 1350: loss 6.8693, lr 0.00125, time 1159.67ms, mfu 27.44%
iter 1360: loss 6.5215, lr 0.00125, time 1154.68ms, mfu 27.63%
iter 1370: loss 7.0597, lr 0.00125, time 1158.74ms, mfu 27.79%
iter 1380: loss 6.8206, lr 0.00125, time 1161.22ms, mfu 27.93%
iter 1390: loss 6.6260, lr 0.00125, time 1159.46ms, mfu 28.06%
step 1400: train loss 6.8696, val loss 6.7120
saving checkpoint to out
iter 1400: loss 6.6944, lr 0.00125, time 4188.80ms, mfu 26.06%
iter 1410: loss 6.6954, lr 0.00125, time 1145.74ms, mfu 26.41%
iter 1420: loss 6.6262, lr 0.00125, time 1156.43ms, mfu 26.70%
iter 1430: loss 6.7433, lr 0.00125, time 1166.97ms, mfu 26.93%
iter 1440: loss 7.0008, lr 0.00125, time 1161.55ms, mfu 27.15%
iter 1450: loss 6.6345, lr 0.00125, time 1159.60ms, mfu 27.36%
iter 1460: loss 6.7967, lr 0.00125, time 1156.58ms, mfu 27.55%
iter 1470: loss 6.9556, lr 0.00125, time 1150.19ms, mfu 27.74%
iter 1480: loss 6.6932, lr 0.00125, time 1145.17ms, mfu 27.92%
iter 1490: loss 6.7683, lr 0.00125, time 1158.94ms, mfu 28.05%
step 1500: train loss 6.7388, val loss 6.7286
saving checkpoint to out
iter 1500: loss 6.8446, lr 0.00125, time 3847.16ms, mfu 26.13%
iter 1510: loss 6.9907, lr 0.00125, time 1145.80ms, mfu 26.47%
iter 1520: loss 6.9630, lr 0.00125, time 1158.00ms, mfu 26.75%
iter 1530: loss 6.6597, lr 0.00125, time 1160.05ms, mfu 26.99%
iter 1540: loss 7.2052, lr 0.00125, time 1155.97ms, mfu 27.22%
iter 1550: loss 6.5507, lr 0.00125, time 1160.00ms, mfu 27.42%
iter 1560: loss 6.4013, lr 0.00125, time 1157.61ms, mfu 27.60%
iter 1570: loss 6.6990, lr 0.00125, time 1148.55ms, mfu 27.79%
iter 1580: loss 6.8565, lr 0.00125, time 1143.72ms, mfu 27.97%
iter 1590: loss 6.3045, lr 0.00125, time 1141.09ms, mfu 28.15%
step 1600: train loss 6.7675, val loss 6.7923
saving checkpoint to out

=== SCALING OPERATION TRIGGERED (DDP SYNC) ===
Operation: widen_mlp
Trigger reason: Timeout
Current val loss: 6.7923, Trigger loss: 1.0000
Iterations since last op: 1000, Max wait: 1000
Executing operation: widen_mlp with value: 2
Performing architectural operation: widen_mlp
Widening MLP layers by a factor of 2.
MLP hidden dimension widened to 1536.

Detailed parameter count:
  total                  | Total:   99,496,704 | Trainable:   39,629,568
  token_embeddings       | Total:    2,451,456 | Trainable:    2,451,456
  position_embeddings    | Total:            0 | Trainable:            0
  attention_layers       | Total:   30,081,024 | Trainable:    8,847,360
  feed_forward_layers    | Total:   28,311,552 | Trainable:   28,311,552
  layer_norms            | Total:       18,432 | Trainable:       18,432
  final_layer_norm       | Total:          768 | Trainable:          768
  language_model_head    | Total:   38,633,472 | Trainable:            0
------------------------------------------------------------
Re-configuring optimizer after architectural change...
num decayed parameter tensors: 62, with 39,610,368 parameters
num non-decayed parameter tensors: 25, with 19,200 parameters
using fused AdamW: True
Transferring optimizer state for existing parameters...
Transferred optimizer state for 0 / 100 parameters
Re-compiling the model...
Architectural operation completed successfully.
Re-evaluating validation loss after operation...
New val loss after operation: 6.6923
iter 1600: loss 6.4864, lr 0.00125, time 70063.12ms, mfu 25.39%
iter 1610: loss 7.1820, lr 0.00125, time 1301.35ms, mfu 25.80%
iter 1620: loss 6.7728, lr 0.00125, time 1329.86ms, mfu 26.12%
iter 1630: loss 6.7824, lr 0.00125, time 1322.01ms, mfu 26.41%
iter 1640: loss 7.0224, lr 0.00125, time 1298.76ms, mfu 26.73%
iter 1650: loss 6.7562, lr 0.00125, time 1303.34ms, mfu 27.01%
iter 1660: loss 6.5410, lr 0.00125, time 1299.97ms, mfu 27.27%
iter 1670: loss 6.7793, lr 0.00125, time 1300.08ms, mfu 27.50%
iter 1680: loss 6.7639, lr 0.00125, time 1302.38ms, mfu 27.71%
iter 1690: loss 6.9532, lr 0.00125, time 1301.73ms, mfu 27.89%
step 1700: train loss 6.7608, val loss 6.6223
saving checkpoint to out

=== SCALING OPERATION TRIGGERED (DDP SYNC) ===
Operation: change_lr
Trigger reason: Loss threshold
Current val loss: 6.6223, Trigger loss: 100.0000
Iterations since last op: 100, Max wait: 1
Executing operation: change_lr with value: 0.5
LR multiplier: 1.2500 -> 0.6250
=== SCALING OPERATION COMPLETE ===


=== SCALING OPERATION TRIGGERED (DDP SYNC) ===
Operation: reset_lr_schedule
Trigger reason: Loss threshold
Current val loss: 6.6223, Trigger loss: 100.0000
Iterations since last op: 0, Max wait: 1
Executing operation: reset_lr_schedule with value: None
LR schedule offset: 500 -> 1700
=== SCALING OPERATION COMPLETE ===


=== SCALING OPERATION TRIGGERED (DDP SYNC) ===
Operation: change_batch_size
Trigger reason: Loss threshold
Current val loss: 6.6223, Trigger loss: 100.0000
Iterations since last op: 0, Max wait: 1
Executing operation: change_batch_size with value: 0.5
Batch size: 8 -> 4
=== SCALING OPERATION COMPLETE ===


=== SCALING OPERATION TRIGGERED (DDP SYNC) ===
Operation: change_grad_accum
Trigger reason: Loss threshold
Current val loss: 6.6223, Trigger loss: 100.0000
Iterations since last op: 0, Max wait: 1
Executing operation: change_grad_accum with value: 2.0
Grad accum steps: 8 -> 16
=== SCALING OPERATION COMPLETE ===

iter 1700: loss 6.6711, lr 0.00125, time 4276.97ms, mfu 26.00%
iter 1710: loss 6.0940, lr 0.00002, time 1300.33ms, mfu 26.36%
iter 1720: loss 6.7284, lr 0.00003, time 1299.00ms, mfu 26.68%
iter 1730: loss 6.7411, lr 0.00005, time 1299.42ms, mfu 26.98%
iter 1740: loss 6.1618, lr 0.00006, time 1299.20ms, mfu 27.24%
iter 1750: loss 6.6325, lr 0.00008, time 1300.19ms, mfu 27.47%
iter 1760: loss 6.2982, lr 0.00010, time 1299.80ms, mfu 27.69%
iter 1770: loss 6.4966, lr 0.00011, time 1298.71ms, mfu 27.88%
iter 1780: loss 6.5657, lr 0.00013, time 1297.49ms, mfu 28.06%
iter 1790: loss 6.6152, lr 0.00014, time 1299.21ms, mfu 28.21%
step 1800: train loss 6.6805, val loss 6.5875
saving checkpoint to out

=== SCALING OPERATION TRIGGERED (DDP SYNC) ===
Operation: merge_lora_weights
Trigger reason: Timeout
Current val loss: 6.5875, Trigger loss: 1.0000
Iterations since last op: 100, Max wait: 100
Executing operation: merge_lora_weights with value: None
Performing architectural operation: merge_lora_weights
Merging LoRA weights into main weights...
LoRA weights merged and reset.

Detailed parameter count:
  total                  | Total:   99,496,704 | Trainable:   39,629,568
  token_embeddings       | Total:    2,451,456 | Trainable:    2,451,456
  position_embeddings    | Total:            0 | Trainable:            0
  attention_layers       | Total:   30,081,024 | Trainable:    8,847,360
  feed_forward_layers    | Total:   28,311,552 | Trainable:   28,311,552
  layer_norms            | Total:       18,432 | Trainable:       18,432
  final_layer_norm       | Total:          768 | Trainable:          768
  language_model_head    | Total:   38,633,472 | Trainable:            0
------------------------------------------------------------
Re-configuring optimizer after architectural change...
num decayed parameter tensors: 62, with 39,610,368 parameters
num non-decayed parameter tensors: 25, with 19,200 parameters
using fused AdamW: True
Transferring optimizer state for existing parameters...
Transferred optimizer state for 0 / 100 parameters
Re-compiling the model...
Architectural operation completed successfully.
Re-evaluating validation loss after operation...
New val loss after operation: 6.5805
iter 1800: loss 6.7327, lr 0.00016, time 5227.77ms, mfu 26.13%
iter 1810: loss 7.1469, lr 0.00017, time 1300.43ms, mfu 26.47%
iter 1820: loss 6.6006, lr 0.00019, time 1297.66ms, mfu 26.79%
iter 1830: loss 6.1041, lr 0.00020, time 1298.41ms, mfu 27.07%
iter 1840: loss 6.9385, lr 0.00022, time 1299.57ms, mfu 27.33%
iter 1850: loss 6.4151, lr 0.00024, time 1300.39ms, mfu 27.55%
iter 1860: loss 6.7966, lr 0.00025, time 1301.53ms, mfu 27.75%
iter 1870: loss 6.3887, lr 0.00027, time 1299.24ms, mfu 27.94%
iter 1880: loss 6.5402, lr 0.00028, time 1301.64ms, mfu 28.10%
iter 1890: loss 6.4954, lr 0.00030, time 1299.61ms, mfu 28.25%
step 1900: train loss 6.2679, val loss 6.3724
saving checkpoint to out

=== SCALING OPERATION TRIGGERED (DDP SYNC) ===
Operation: merge_lora_weights
Trigger reason: Timeout
Current val loss: 6.3724, Trigger loss: 1.0000
Iterations since last op: 100, Max wait: 100
Executing operation: merge_lora_weights with value: None
Performing architectural operation: merge_lora_weights
Merging LoRA weights into main weights...
LoRA weights merged and reset.

Detailed parameter count:
  total                  | Total:   99,496,704 | Trainable:   39,629,568
  token_embeddings       | Total:    2,451,456 | Trainable:    2,451,456
  position_embeddings    | Total:            0 | Trainable:            0
  attention_layers       | Total:   30,081,024 | Trainable:    8,847,360
  feed_forward_layers    | Total:   28,311,552 | Trainable:   28,311,552
  layer_norms            | Total:       18,432 | Trainable:       18,432
  final_layer_norm       | Total:          768 | Trainable:          768
  language_model_head    | Total:   38,633,472 | Trainable:            0
------------------------------------------------------------
Re-configuring optimizer after architectural change...
num decayed parameter tensors: 62, with 39,610,368 parameters
num non-decayed parameter tensors: 25, with 19,200 parameters
using fused AdamW: True
Transferring optimizer state for existing parameters...
Transferred optimizer state for 0 / 100 parameters
Re-compiling the model...
Architectural operation completed successfully.
Re-evaluating validation loss after operation...
New val loss after operation: 6.3839
iter 1900: loss 6.1807, lr 0.00031, time 5147.92ms, mfu 26.17%
iter 1910: loss 6.4032, lr 0.00033, time 1298.53ms, mfu 26.52%
iter 1920: loss 6.7334, lr 0.00034, time 1300.29ms, mfu 26.82%
iter 1930: loss 6.5230, lr 0.00036, time 1299.52ms, mfu 27.10%
iter 1940: loss 6.9677, lr 0.00038, time 1298.96ms, mfu 27.35%
iter 1950: loss 6.7509, lr 0.00039, time 1299.35ms, mfu 27.58%
iter 1960: loss 6.3463, lr 0.00041, time 1299.77ms, mfu 27.78%
iter 1970: loss 6.3192, lr 0.00042, time 1301.57ms, mfu 27.96%
iter 1980: loss 6.1022, lr 0.00044, time 1298.84ms, mfu 28.12%
iter 1990: loss 6.4867, lr 0.00045, time 1299.41ms, mfu 28.27%
step 2000: train loss 6.2995, val loss 6.5263
saving checkpoint to out

=== SCALING OPERATION TRIGGERED (DDP SYNC) ===
Operation: merge_lora_weights
Trigger reason: Timeout
Current val loss: 6.5263, Trigger loss: 1.0000
Iterations since last op: 100, Max wait: 100
Executing operation: merge_lora_weights with value: None
Performing architectural operation: merge_lora_weights
Merging LoRA weights into main weights...
LoRA weights merged and reset.

Detailed parameter count:
  total                  | Total:   99,496,704 | Trainable:   39,629,568
  token_embeddings       | Total:    2,451,456 | Trainable:    2,451,456
  position_embeddings    | Total:            0 | Trainable:            0
  attention_layers       | Total:   30,081,024 | Trainable:    8,847,360
  feed_forward_layers    | Total:   28,311,552 | Trainable:   28,311,552
  layer_norms            | Total:       18,432 | Trainable:       18,432
  final_layer_norm       | Total:          768 | Trainable:          768
  language_model_head    | Total:   38,633,472 | Trainable:            0
------------------------------------------------------------
Re-configuring optimizer after architectural change...
num decayed parameter tensors: 62, with 39,610,368 parameters
num non-decayed parameter tensors: 25, with 19,200 parameters
using fused AdamW: True
Transferring optimizer state for existing parameters...
Transferred optimizer state for 0 / 100 parameters
Re-compiling the model...
Architectural operation completed successfully.
Re-evaluating validation loss after operation...
New val loss after operation: 6.4437
iter 2000: loss 6.5871, lr 0.00047, time 4129.10ms, mfu 26.38%
iter 2010: loss 6.5054, lr 0.00048, time 1301.82ms, mfu 26.69%
iter 2020: loss 6.5064, lr 0.00050, time 1298.84ms, mfu 26.99%
iter 2030: loss 6.2106, lr 0.00052, time 1299.55ms, mfu 27.25%
iter 2040: loss 6.4440, lr 0.00053, time 1300.12ms, mfu 27.48%
iter 2050: loss 6.6162, lr 0.00055, time 1301.82ms, mfu 27.69%
iter 2060: loss 6.0079, lr 0.00056, time 1298.30ms, mfu 27.88%
iter 2070: loss 6.5036, lr 0.00058, time 1299.76ms, mfu 28.05%
iter 2080: loss 6.6228, lr 0.00059, time 1299.31ms, mfu 28.21%
iter 2090: loss 6.2660, lr 0.00061, time 1299.99ms, mfu 28.35%
step 2100: train loss 6.4768, val loss 6.4737
saving checkpoint to out

=== SCALING OPERATION TRIGGERED (DDP SYNC) ===
Operation: change_warmup_iters
Trigger reason: Loss threshold
Current val loss: 6.4737, Trigger loss: 100.0000
Iterations since last op: 100, Max wait: 1
Executing operation: change_warmup_iters with value: 1.5
Warmup iters multiplier: 2.0000 -> 3.0000
=== SCALING OPERATION COMPLETE ===


=== SCALING OPERATION TRIGGERED (DDP SYNC) ===
Operation: change_lr
Trigger reason: Loss threshold
Current val loss: 6.4737, Trigger loss: 100.0000
Iterations since last op: 0, Max wait: 1
Executing operation: change_lr with value: 0.5
LR multiplier: 0.6250 -> 0.3125
Re-evaluating validation loss after operation...
New val loss after operation: 6.4733
iter 2100: loss 6.1998, lr 0.00063, time 4152.74ms, mfu 26.44%
iter 2110: loss 6.1488, lr 0.00021, time 1302.23ms, mfu 26.75%
iter 2120: loss 6.4090, lr 0.00022, time 1298.86ms, mfu 27.04%
iter 2130: loss 6.3267, lr 0.00022, time 1300.11ms, mfu 27.29%
iter 2140: loss 6.2044, lr 0.00023, time 1299.41ms, mfu 27.52%
iter 2150: loss 6.7106, lr 0.00023, time 1298.40ms, mfu 27.73%
iter 2160: loss 6.7101, lr 0.00024, time 1299.62ms, mfu 27.92%
iter 2170: loss 6.6156, lr 0.00024, time 1299.79ms, mfu 28.09%
iter 2180: loss 5.9468, lr 0.00025, time 1298.45ms, mfu 28.24%
iter 2190: loss 6.5556, lr 0.00026, time 1300.10ms, mfu 28.38%
step 2200: train loss 6.3180, val loss 6.4846
saving checkpoint to out

=== SCALING OPERATION TRIGGERED (DDP SYNC) ===
Operation: change_eval_iters
Trigger reason: Loss threshold
Current val loss: 6.4846, Trigger loss: 100.0000
Iterations since last op: 100, Max wait: 1
Executing operation: change_eval_iters with value: 2
Eval iters multiplier: 1.0000 -> 2.0000  current evals: 20.0
=== SCALING OPERATION COMPLETE ===


=== SCALING OPERATION TRIGGERED (DDP SYNC) ===
Operation: change_eval_interval
Trigger reason: Loss threshold
Current val loss: 6.4846, Trigger loss: 100.0000
Iterations since last op: 0, Max wait: 1
Executing operation: change_eval_interval with value: 2
Eval interval multiplier: 1.0000 -> 2.0000 current interval: 200.0
=== SCALING OPERATION COMPLETE ===


=== SCALING OPERATION TRIGGERED (DDP SYNC) ===
Operation: decrease_attn_lora_scaling
Trigger reason: Loss threshold
Current val loss: 6.4846, Trigger loss: 7.0000
Iterations since last op: 0, Max wait: 1000
Executing operation: decrease_attn_lora_scaling with value: 0.5
Performing architectural operation: decrease_attn_lora_scaling
Resizing attention LoRA rank to 24.

Detailed parameter count:
  total                  | Total:   98,611,968 | Trainable:   38,744,832
  token_embeddings       | Total:    2,451,456 | Trainable:    2,451,456
  position_embeddings    | Total:            0 | Trainable:            0
  attention_layers       | Total:   29,196,288 | Trainable:    7,962,624
  feed_forward_layers    | Total:   28,311,552 | Trainable:   28,311,552
  layer_norms            | Total:       18,432 | Trainable:       18,432
  final_layer_norm       | Total:          768 | Trainable:          768
  language_model_head    | Total:   38,633,472 | Trainable:            0
------------------------------------------------------------
Re-configuring optimizer after architectural change...
num decayed parameter tensors: 62, with 38,725,632 parameters
num non-decayed parameter tensors: 25, with 19,200 parameters
using fused AdamW: True
Transferring optimizer state for existing parameters...
Transferred optimizer state for 0 / 100 parameters
Re-compiling the model...
Architectural operation completed successfully.
Re-evaluating validation loss after operation...
W0720 07:48:15.481000 4054 /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py:964] [0/8] torch._dynamo hit config.recompile_limit (8)
W0720 07:48:15.481000 4054 /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py:964] [0/8]    function: 'forward' (/teamspace/studios/this_studio/nanoGPT/model.py:420)
W0720 07:48:15.481000 4054 /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py:964] [0/8]    last reason: 0/6: self._modules['transformer']._modules['h']._modules['0']._modules['attn']._modules['c_attn'].rank == 48
W0720 07:48:15.481000 4054 /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py:964] [0/8] To log all recompilation reasons, use TORCH_LOGS="recompiles".
W0720 07:48:15.481000 4054 /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py:964] [0/8] To diagnose recompilation issues, see https://pytorch.org/docs/main/torch.compiler_troubleshooting.html.
New val loss after operation: 6.3740
iter 2200: loss 6.0255, lr 0.00026, time 7477.78ms, mfu 26.05%
iter 2210: loss 6.2439, lr 0.00027, time 1729.70ms, mfu 25.65%
iter 2220: loss 6.4708, lr 0.00027, time 1751.31ms, mfu 25.27%
iter 2230: loss 6.8586, lr 0.00028, time 1737.17ms, mfu 24.94%
iter 2240: loss 6.9589, lr 0.00028, time 1723.32ms, mfu 24.66%
iter 2250: loss 6.1163, lr 0.00029, time 1732.48ms, mfu 24.40%
iter 2260: loss 6.4034, lr 0.00029, time 1728.84ms, mfu 24.17%
iter 2270: loss 6.2758, lr 0.00030, time 1735.33ms, mfu 23.95%
iter 2280: loss 6.0514, lr 0.00030, time 1741.08ms, mfu 23.75%
iter 2290: loss 6.5039, lr 0.00031, time 1727.70ms, mfu 23.58%
iter 2300: loss 6.0863, lr 0.00031, time 1728.92ms, mfu 23.43%
iter 2310: loss 6.3455, lr 0.00031, time 1737.36ms, mfu 23.29%
iter 2320: loss 6.5577, lr 0.00031, time 1731.16ms, mfu 23.16%
iter 2330: loss 6.4077, lr 0.00031, time 1728.92ms, mfu 23.06%
iter 2340: loss 5.6415, lr 0.00031, time 1737.89ms, mfu 22.95%
iter 2350: loss 6.2900, lr 0.00031, time 1725.10ms, mfu 22.87%
iter 2360: loss 5.8578, lr 0.00031, time 1723.07ms, mfu 22.79%
iter 2370: loss 6.1788, lr 0.00031, time 1734.90ms, mfu 22.72%
iter 2380: loss 5.9895, lr 0.00031, time 1724.74ms, mfu 22.66%
iter 2390: loss 6.3013, lr 0.00031, time 1720.83ms, mfu 22.61%
step 2400: train loss 6.2955, val loss 6.3717
saving checkpoint to out

=== SCALING OPERATION TRIGGERED (DDP SYNC) ===
Operation: decrease_vocab_lora_scaling
Trigger reason: Loss threshold
Current val loss: 6.3717, Trigger loss: 100.0000
Iterations since last op: 200, Max wait: 1
Executing operation: decrease_vocab_lora_scaling with value: 0.5
Performing architectural operation: decrease_vocab_lora_scaling
Resizing embedding LoRA rank to 24.

Detailed parameter count:
  total                  | Total:   97,386,240 | Trainable:   37,519,104
  token_embeddings       | Total:    1,225,728 | Trainable:    1,225,728
  position_embeddings    | Total:            0 | Trainable:            0
  attention_layers       | Total:   29,196,288 | Trainable:    7,962,624
  feed_forward_layers    | Total:   28,311,552 | Trainable:   28,311,552
  layer_norms            | Total:       18,432 | Trainable:       18,432
  final_layer_norm       | Total:          768 | Trainable:          768
  language_model_head    | Total:   38,633,472 | Trainable:            0
------------------------------------------------------------
Re-configuring optimizer after architectural change...
num decayed parameter tensors: 62, with 37,499,904 parameters
num non-decayed parameter tensors: 25, with 19,200 parameters
using fused AdamW: True
Transferring optimizer state for existing parameters...
Transferred optimizer state for 0 / 100 parameters
Re-compiling the model...
Architectural operation completed successfully.
Re-evaluating validation loss after operation...
New val loss after operation: 6.3248
iter 2400: loss 6.5805, lr 0.00031, time 9035.86ms, mfu 20.77%
iter 2410: loss 6.8382, lr 0.00031, time 1727.11ms, mfu 20.88%
iter 2420: loss 6.4088, lr 0.00031, time 1731.55ms, mfu 20.97%
iter 2430: loss 6.3317, lr 0.00031, time 1728.17ms, mfu 21.06%
iter 2440: loss 6.2865, lr 0.00031, time 1718.26ms, mfu 21.15%
iter 2450: loss 6.0268, lr 0.00031, time 1725.43ms, mfu 21.23%
iter 2460: loss 6.1286, lr 0.00031, time 1717.75ms, mfu 21.31%
iter 2470: loss 6.5661, lr 0.00031, time 1720.61ms, mfu 21.37%
iter 2480: loss 6.0163, lr 0.00031, time 1725.46ms, mfu 21.42%
iter 2490: loss 6.6879, lr 0.00031, time 1726.93ms, mfu 21.47%
iter 2500: loss 6.3014, lr 0.00031, time 1725.68ms, mfu 21.51%
iter 2510: loss 6.1124, lr 0.00031, time 1719.90ms, mfu 21.56%
iter 2520: loss 6.4658, lr 0.00031, time 1711.02ms, mfu 21.61%
iter 2530: loss 6.2644, lr 0.00031, time 1719.60ms, mfu 21.65%
iter 2540: loss 5.9448, lr 0.00031, time 1721.71ms, mfu 21.68%
iter 2550: loss 6.2792, lr 0.00031, time 1725.75ms, mfu 21.70%
iter 2560: loss 6.1034, lr 0.00031, time 1716.86ms, mfu 21.73%
iter 2570: loss 6.2549, lr 0.00031, time 1731.65ms, mfu 21.74%
iter 2580: loss 6.2592, lr 0.00031, time 1734.09ms, mfu 21.74%
iter 2590: loss 6.3569, lr 0.00031, time 1726.29ms, mfu 21.76%
step 2600: train loss 6.1751, val loss 6.2376
saving checkpoint to out

=== SCALING OPERATION TRIGGERED (DDP SYNC) ===
Operation: change_warmup_iters
Trigger reason: Loss threshold
Current val loss: 6.2376, Trigger loss: 100.0000
Iterations since last op: 200, Max wait: 1
Executing operation: change_warmup_iters with value: 1.5
Warmup iters multiplier: 3.0000 -> 4.5000
=== SCALING OPERATION COMPLETE ===


=== SCALING OPERATION TRIGGERED (DDP SYNC) ===
Operation: reset_lr_schedule
Trigger reason: Loss threshold
Current val loss: 6.2376, Trigger loss: 100.0000
Iterations since last op: 0, Max wait: 1
Executing operation: reset_lr_schedule with value: None
LR schedule offset: 1700 -> 2600
=== SCALING OPERATION COMPLETE ===


=== SCALING OPERATION TRIGGERED (DDP SYNC) ===
Operation: change_warmup_iters
Trigger reason: Loss threshold
Current val loss: 6.2376, Trigger loss: 100.0000
Iterations since last op: 0, Max wait: 1
Executing operation: change_warmup_iters with value: 2.0
Warmup iters multiplier: 4.5000 -> 9.0000
=== SCALING OPERATION COMPLETE ===


=== SCALING OPERATION TRIGGERED (DDP SYNC) ===
Operation: reset_lr_schedule
Trigger reason: Loss threshold
Current val loss: 6.2376, Trigger loss: 100.0000
Iterations since last op: 0, Max wait: 1
Executing operation: reset_lr_schedule with value: None
LR schedule offset: 2600 -> 2600
=== SCALING OPERATION COMPLETE ===


=== SCALING OPERATION TRIGGERED (DDP SYNC) ===
Operation: change_batch_size
Trigger reason: Loss threshold
Current val loss: 6.2376, Trigger loss: 100.0000
Iterations since last op: 0, Max wait: 1
Executing operation: change_batch_size with value: 0.5
Batch size: 4 -> 2
=== SCALING OPERATION COMPLETE ===


=== SCALING OPERATION TRIGGERED (DDP SYNC) ===
Operation: change_grad_accum
Trigger reason: Loss threshold
Current val loss: 6.2376, Trigger loss: 100.0000
Iterations since last op: 0, Max wait: 1
Executing operation: change_grad_accum with value: 2.0
Grad accum steps: 16 -> 32
=== SCALING OPERATION COMPLETE ===


=== SCALING OPERATION TRIGGERED (DDP SYNC) ===
Operation: decrease_attn_lora_scaling
Trigger reason: Loss threshold
Current val loss: 6.2376, Trigger loss: 6.5000
Iterations since last op: 0, Max wait: 1000
Executing operation: decrease_attn_lora_scaling with value: 0.5
Performing architectural operation: decrease_attn_lora_scaling
Resizing attention LoRA rank to 12.

Detailed parameter count:
  total                  | Total:   96,943,872 | Trainable:   37,076,736
  token_embeddings       | Total:    1,225,728 | Trainable:    1,225,728
  position_embeddings    | Total:            0 | Trainable:            0
  attention_layers       | Total:   28,753,920 | Trainable:    7,520,256
  feed_forward_layers    | Total:   28,311,552 | Trainable:   28,311,552
  layer_norms            | Total:       18,432 | Trainable:       18,432
  final_layer_norm       | Total:          768 | Trainable:          768
  language_model_head    | Total:   38,633,472 | Trainable:            0
------------------------------------------------------------
Re-configuring optimizer after architectural change...
num decayed parameter tensors: 62, with 37,057,536 parameters
num non-decayed parameter tensors: 25, with 19,200 parameters
using fused AdamW: True
Transferring optimizer state for existing parameters...
Transferred optimizer state for 0 / 100 parameters
Re-compiling the model...
Architectural operation completed successfully.
Re-evaluating validation loss after operation...
New val loss after operation: 6.3098
iter 2600: loss 6.4399, lr 0.00031, time 6971.35ms, mfu 20.12%
iter 2610: loss 6.9734, lr 0.00000, time 1691.61ms, mfu 20.33%
iter 2620: loss 6.3077, lr 0.00000, time 1694.28ms, mfu 20.52%
iter 2630: loss 5.9009, lr 0.00001, time 1700.34ms, mfu 20.68%
iter 2640: loss 6.5249, lr 0.00001, time 1692.22ms, mfu 20.84%
iter 2650: loss 6.5877, lr 0.00001, time 1689.14ms, mfu 20.98%
iter 2660: loss 6.7503, lr 0.00001, time 1693.92ms, mfu 21.11%
iter 2670: loss 6.5095, lr 0.00001, time 1691.66ms, mfu 21.22%
iter 2680: loss 6.8053, lr 0.00001, time 1667.88ms, mfu 21.36%
iter 2690: loss 6.2669, lr 0.00002, time 1695.33ms, mfu 21.44%
iter 2700: loss 5.9080, lr 0.00002, time 1722.58ms, mfu 21.48%
iter 2710: loss 6.2071, lr 0.00002, time 1697.03ms, mfu 21.55%
iter 2720: loss 5.8806, lr 0.00002, time 1689.24ms, mfu 21.62%
iter 2730: loss 6.2353, lr 0.00002, time 1696.64ms, mfu 21.68%
iter 2740: loss 6.3064, lr 0.00002, time 1693.45ms, mfu 21.73%
iter 2750: loss 6.3224, lr 0.00003, time 1695.96ms, mfu 21.78%
iter 2760: loss 7.6351, lr 0.00003, time 1702.69ms, mfu 21.81%
iter 2770: loss 5.5155, lr 0.00003, time 1695.93ms, mfu 21.85%
iter 2780: loss 5.9820, lr 0.00003, time 1687.59ms, mfu 21.90%
iter 2790: loss 6.2242, lr 0.00003, time 1693.57ms, mfu 21.93%
step 2800: train loss 6.2601, val loss 6.3537
saving checkpoint to out

=== SCALING OPERATION TRIGGERED (DDP SYNC) ===
Operation: decrease_vocab_lora_scaling
Trigger reason: Loss threshold
Current val loss: 6.3537, Trigger loss: 100.0000
Iterations since last op: 200, Max wait: 1
Executing operation: decrease_vocab_lora_scaling with value: 0.5
Performing architectural operation: decrease_vocab_lora_scaling
Resizing embedding LoRA rank to 12.

Detailed parameter count:
  total                  | Total:   96,331,008 | Trainable:   36,463,872
  token_embeddings       | Total:      612,864 | Trainable:      612,864
  position_embeddings    | Total:            0 | Trainable:            0
  attention_layers       | Total:   28,753,920 | Trainable:    7,520,256
  feed_forward_layers    | Total:   28,311,552 | Trainable:   28,311,552
  layer_norms            | Total:       18,432 | Trainable:       18,432
  final_layer_norm       | Total:          768 | Trainable:          768
  language_model_head    | Total:   38,633,472 | Trainable:            0
------------------------------------------------------------
Re-configuring optimizer after architectural change...
num decayed parameter tensors: 62, with 36,444,672 parameters
num non-decayed parameter tensors: 25, with 19,200 parameters
using fused AdamW: True
Transferring optimizer state for existing parameters...
Transferred optimizer state for 0 / 100 parameters
Re-compiling the model...
Architectural operation completed successfully.
Re-evaluating validation loss after operation...
New val loss after operation: 6.3102
iter 2800: loss 6.3158, lr 0.00003, time 6603.58ms, mfu 20.30%
iter 2810: loss 6.5201, lr 0.00004, time 1698.75ms, mfu 20.48%
iter 2820: loss 5.8853, lr 0.00004, time 1701.71ms, mfu 20.63%
iter 2830: loss 6.6066, lr 0.00004, time 1767.84ms, mfu 20.68%
iter 2840: loss 6.0709, lr 0.00004, time 1687.37ms, mfu 20.83%
iter 2850: loss 5.8480, lr 0.00004, time 1696.98ms, mfu 20.96%
iter 2860: loss 6.5190, lr 0.00005, time 1675.74ms, mfu 21.10%
iter 2870: loss 5.7550, lr 0.00005, time 1701.16ms, mfu 21.19%
iter 2880: loss 6.9038, lr 0.00005, time 1676.36ms, mfu 21.30%
iter 2890: loss 6.0542, lr 0.00005, time 1688.82ms, mfu 21.39%
iter 2900: loss 5.9004, lr 0.00005, time 1705.07ms, mfu 21.44%
iter 2910: loss 6.2385, lr 0.00005, time 1694.87ms, mfu 21.51%
iter 2920: loss 6.3279, lr 0.00006, time 1691.86ms, mfu 21.57%
iter 2930: loss 6.2249, lr 0.00006, time 1692.09ms, mfu 21.63%
iter 2940: loss 5.8058, lr 0.00006, time 1678.55ms, mfu 21.69%
iter 2950: loss 6.2578, lr 0.00006, time 1687.32ms, mfu 21.74%
iter 2960: loss 6.2221, lr 0.00006, time 1687.54ms, mfu 21.79%
iter 2970: loss 6.4232, lr 0.00006, time 1690.31ms, mfu 21.82%
iter 2980: loss 5.5977, lr 0.00007, time 1689.33ms, mfu 21.86%
iter 2990: loss 6.4328, lr 0.00007, time 1700.78ms, mfu 21.87%
step 3000: train loss 6.3546, val loss 6.2341
saving checkpoint to out

=== SCALING OPERATION TRIGGERED (DDP SYNC) ===
Operation: change_warmup_iters
Trigger reason: Loss threshold
Current val loss: 6.2341, Trigger loss: 100.0000
Iterations since last op: 200, Max wait: 1
Executing operation: change_warmup_iters with value: 1.5
Warmup iters multiplier: 9.0000 -> 13.5000
=== SCALING OPERATION COMPLETE ===


=== SCALING OPERATION TRIGGERED (DDP SYNC) ===
Operation: reset_lr_schedule
Trigger reason: Loss threshold
Current val loss: 6.2341, Trigger loss: 100.0000
Iterations since last op: 0, Max wait: 1
Executing operation: reset_lr_schedule with value: None
LR schedule offset: 2600 -> 3000
=== SCALING OPERATION COMPLETE ===


=== SCALING OPERATION TRIGGERED (DDP SYNC) ===
Operation: change_warmup_iters
Trigger reason: Loss threshold
Current val loss: 6.2341, Trigger loss: 100.0000
Iterations since last op: 0, Max wait: 1
Executing operation: change_warmup_iters with value: 2.0
Warmup iters multiplier: 13.5000 -> 27.0000
=== SCALING OPERATION COMPLETE ===


=== SCALING OPERATION TRIGGERED (DDP SYNC) ===
Operation: reset_lr_schedule
Trigger reason: Loss threshold
Current val loss: 6.2341, Trigger loss: 100.0000
Iterations since last op: 0, Max wait: 1
Executing operation: reset_lr_schedule with value: None
LR schedule offset: 3000 -> 3000
=== SCALING OPERATION COMPLETE ===


=== SCALING OPERATION TRIGGERED (DDP SYNC) ===
Operation: change_batch_size
Trigger reason: Loss threshold
Current val loss: 6.2341, Trigger loss: 100.0000
Iterations since last op: 0, Max wait: 1
Executing operation: change_batch_size with value: 0.5
Batch size: 2 -> 1
=== SCALING OPERATION COMPLETE ===


=== SCALING OPERATION TRIGGERED (DDP SYNC) ===
Operation: change_grad_accum
Trigger reason: Loss threshold
Current val loss: 6.2341, Trigger loss: 100.0000
Iterations since last op: 0, Max wait: 1
Executing operation: change_grad_accum with value: 2.0
Grad accum steps: 32 -> 64
=== SCALING OPERATION COMPLETE ===

iter 3000: loss 6.4274, lr 0.00007, time 5213.18ms, mfu 20.40%
iter 3010: loss 6.7658, lr 0.00000, time 2507.40ms, mfu 19.86%
iter 3020: loss 7.2395, lr 0.00000, time 2411.17ms, mfu 19.42%
iter 3030: loss 6.1285, lr 0.00000, time 2634.46ms, mfu 18.90%
iter 3040: loss 5.6952, lr 0.00000, time 2616.20ms, mfu 18.44%
iter 3050: loss 7.2766, lr 0.00000, time 2656.89ms, mfu 18.01%
iter 3060: loss 5.8006, lr 0.00000, time 2690.25ms, mfu 17.60%
iter 3070: loss 8.6105, lr 0.00000, time 2609.00ms, mfu 17.27%
iter 3080: loss 6.2330, lr 0.00000, time 2533.81ms, mfu 17.02%
iter 3090: loss 6.2479, lr 0.00001, time 2471.01ms, mfu 16.84%
iter 3100: loss 5.2821, lr 0.00001, time 2435.35ms, mfu 16.69%
iter 3110: loss 6.2069, lr 0.00001, time 2502.06ms, mfu 16.52%
iter 3120: loss 5.8220, lr 0.00001, time 2517.79ms, mfu 16.35%
iter 3130: loss 7.1014, lr 0.00001, time 2508.93ms, mfu 16.21%
iter 3140: loss 6.7408, lr 0.00001, time 2476.46ms, mfu 16.10%
iter 3150: loss 6.0677, lr 0.00001, time 2470.45ms, mfu 16.01%
iter 3160: loss 6.4734, lr 0.00001, time 2519.75ms, mfu 15.89%
iter 3170: loss 5.1029, lr 0.00001, time 2458.26ms, mfu 15.83%
iter 3180: loss 6.4341, lr 0.00001, time 2446.23ms, mfu 15.77%
iter 3190: loss 5.3409, lr 0.00001, time 2507.11ms, mfu 15.69%
step 3200: train loss 6.2741, val loss 6.2007
saving checkpoint to out
iter 3200: loss 6.8040, lr 0.00001, time 5826.99ms, mfu 14.76%
iter 3210: loss 6.3099, lr 0.00001, time 2520.24ms, mfu 14.77%
iter 3220: loss 7.1969, lr 0.00001, time 2487.32ms, mfu 14.80%
iter 3230: loss 6.2533, lr 0.00001, time 2452.52ms, mfu 14.85%
iter 3240: loss 6.7210, lr 0.00001, time 2456.91ms, mfu 14.89%
iter 3250: loss 5.9016, lr 0.00001, time 2421.40ms, mfu 14.94%
iter 3260: loss 6.4435, lr 0.00002, time 2570.58ms, mfu 14.91%
iter 3270: loss 5.7349, lr 0.00002, time 2490.91ms, mfu 14.92%
iter 3280: loss 6.1387, lr 0.00002, time 2405.05ms, mfu 14.98%
iter 3290: loss 6.7123, lr 0.00002, time 2457.94ms, mfu 15.01%
iter 3300: loss 8.8788, lr 0.00002, time 2509.80ms, mfu 15.00%
iter 3310: loss 5.6059, lr 0.00002, time 2520.47ms, mfu 14.98%
iter 3320: loss 8.7221, lr 0.00002, time 2331.05ms, mfu 15.09%
iter 3330: loss 6.2417, lr 0.00002, time 2452.67ms, mfu 15.11%
iter 3340: loss 7.8873, lr 0.00002, time 2323.77ms, mfu 15.21%
iter 3350: loss 6.3769, lr 0.00002, time 2306.98ms, mfu 15.31%
iter 3360: loss 5.9979, lr 0.00002, time 2337.41ms, mfu 15.38%
iter 3370: loss 5.4774, lr 0.00002, time 2376.42ms, mfu 15.42%
iter 3380: loss 6.5774, lr 0.00002, time 2319.32ms, mfu 15.49%
iter 3390: loss 5.9433, lr 0.00002, time 2365.14ms, mfu 15.53%
step 3400: train loss 6.2996, val loss 6.2446
saving checkpoint to out
iter 3400: loss 6.7201, lr 0.00002, time 5601.28ms, mfu 14.64%
iter 3410: loss 5.7138, lr 0.00002, time 2367.04ms, mfu 14.76%
iter 3420: loss 6.1748, lr 0.00002, time 2411.45ms, mfu 14.84%
iter 3430: loss 6.3051, lr 0.00002, time 2735.20ms, mfu 14.72%
iter 3440: loss 6.3770, lr 0.00003, time 2681.04ms, mfu 14.64%
iter 3450: loss 5.8669, lr 0.00003, time 2607.77ms, mfu 14.62%
iter 3460: loss 5.9390, lr 0.00003, time 2567.31ms, mfu 14.61%
iter 3470: loss 5.6774, lr 0.00003, time 2684.49ms, mfu 14.55%
iter 3480: loss 6.4833, lr 0.00003, time 2679.69ms, mfu 14.49%
iter 3490: loss 5.1932, lr 0.00003, time 2541.40ms, mfu 14.51%
iter 3500: loss 7.1775, lr 0.00003, time 2750.20ms, mfu 14.42%
iter 3510: loss 7.5351, lr 0.00003, time 2600.02ms, mfu 14.42%
iter 3520: loss 6.1372, lr 0.00003, time 2605.94ms, mfu 14.42%
iter 3530: loss 7.2457, lr 0.00003, time 2461.79ms, mfu 14.49%
iter 3540: loss 6.5093, lr 0.00003, time 2444.41ms, mfu 14.58%
iter 3550: loss 6.1602, lr 0.00003, time 2536.63ms, mfu 14.59%
iter 3560: loss 7.0181, lr 0.00003, time 2782.54ms, mfu 14.48%
iter 3570: loss 5.7424, lr 0.00003, time 2667.62ms, mfu 14.44%
iter 3580: loss 6.7099, lr 0.00003, time 2708.75ms, mfu 14.37%
iter 3590: loss 5.9974, lr 0.00003, time 2617.40ms, mfu 14.37%
step 3600: train loss 6.1624, val loss 6.2657
saving checkpoint to out
iter 3600: loss 6.9386, lr 0.00003, time 5832.41ms, mfu 13.57%
iter 3610: loss 6.6482, lr 0.00004, time 2668.33ms, mfu 13.62%
iter 3620: loss 6.3514, lr 0.00004, time 2558.00ms, mfu 13.72%
iter 3630: loss 6.2481, lr 0.00004, time 2461.71ms, mfu 13.87%
iter 3640: loss 6.8258, lr 0.00004, time 2571.56ms, mfu 13.94%
iter 3650: loss 6.3830, lr 0.00004, time 2641.24ms, mfu 13.96%
iter 3660: loss 5.7839, lr 0.00004, time 2645.86ms, mfu 13.98%
iter 3670: loss 6.0302, lr 0.00004, time 2568.14ms, mfu 14.04%
iter 3680: loss 6.5187, lr 0.00004, time 2851.68ms, mfu 13.95%
iter 3690: loss 5.6955, lr 0.00004, time 2318.15ms, mfu 14.17%
iter 3700: loss 6.0583, lr 0.00004, time 2302.10ms, mfu 14.38%
iter 3710: loss 5.6726, lr 0.00004, time 2316.34ms, mfu 14.56%
iter 3720: loss 6.0579, lr 0.00004, time 2352.70ms, mfu 14.69%
Traceback (most recent call last):
  File "/teamspace/studios/this_studio/nanoGPT/train.py", line 635, in <module>
    if master_process: print(f"=== SCALING OPERATION COMPLETE ===\n")
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/_tensor.py", line 648, in backward
    torch.autograd.backward(
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/autograd/__init__.py", line 353, in backward
    _engine_run_backward(
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/autograd/graph.py", line 824, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
KeyboardInterrupt
Traceback (most recent call last):
  File "/teamspace/studios/this_studio/nanoGPT/train.py", line 635, in <module>
    if master_process: print(f"=== SCALING OPERATION COMPLETE ===\n")
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/_tensor.py", line 648, in backward
    torch.autograd.backward(
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/autograd/__init__.py", line 353, in backward
    _engine_run_backward(
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/autograd/graph.py", line 824, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
KeyboardInterrupt
