Calculating target architecture based on schedule...

============================================================
       TARGET MODEL ARCHITECTURE (at end of schedule)
============================================================
  n_layer                | 2
  n_head                 | 12
  n_embd                 | 768
  n_hidden               | 1536
  block_size             | 1024
  vocab_size             | 50304
  dropout                | 0.0
  bias                   | False
  embedding_mode         | standard
  attn_lora_rank         | 0
  embedding_rank         | 0
  lora_alpha             | 0.0
============================================================


============================================================
           INITIAL MODEL ARCHITECTURE (at Iter 0)
============================================================
  n_layer                | 1
  n_head                 | 12
  n_embd                 | 768
  n_hidden               | 768
  block_size             | 1024
  vocab_size             | 50304
  dropout                | 0.0
  bias                   | False
  embedding_mode         | lora
  attn_lora_rank         | 48
  embedding_rank         | 48
  lora_alpha             | 1.0
============================================================


Detailed parameter count:
  total                  | Total:   44,773,632 | Trainable:    4,370,688
  token_embeddings       | Total:    2,451,456 | Trainable:    2,451,456
  position_embeddings    | Total:            0 | Trainable:            0
  attention_layers       | Total:    2,506,752 | Trainable:      737,280
  feed_forward_layers    | Total:    1,179,648 | Trainable:    1,179,648
  layer_norms            | Total:        1,536 | Trainable:        1,536
  final_layer_norm       | Total:          768 | Trainable:          768
  language_model_head    | Total:   38,633,472 | Trainable:            0
------------------------------------------------------------
eval every:200
step 0: train loss 10.9454, val loss 10.9543
  MLP Rank Utilization (L0): 71.09% (546/768)
Attn LoRA    | 47/48           | 97.92%
--- Model Analysis ---
  Embedding Utilization (L0): 97.53% (749/768)
Embed LoRA   | 47/48           | 97.92%
  Average Attention Entropy:  5.8912
----------------------
merge_lora_weights 10.954339981079102 4.0 200
iter 0: loss 10.9463, lr 0.00000, time 6027.65ms, mfu -100.00%
iter 10: loss 10.9379, lr 0.00000, time 419.12ms, mfu 35.94%
iter 20: loss 10.9008, lr 0.00000, time 435.76ms, mfu 35.80%
iter 30: loss 10.8509, lr 0.00000, time 432.80ms, mfu 35.70%
iter 40: loss 10.7711, lr 0.00000, time 433.31ms, mfu 35.60%
iter 50: loss 10.6735, lr 0.00001, time 440.71ms, mfu 35.46%
iter 60: loss 10.5486, lr 0.00001, time 440.53ms, mfu 35.33%
iter 70: loss 10.4289, lr 0.00001, time 439.79ms, mfu 35.23%
iter 80: loss 10.3271, lr 0.00001, time 436.15ms, mfu 35.16%
iter 90: loss 10.1815, lr 0.00001, time 431.81ms, mfu 35.13%
iter 100: loss 10.0978, lr 0.00001, time 432.41ms, mfu 35.10%
iter 110: loss 9.9786, lr 0.00001, time 428.91ms, mfu 35.10%
iter 120: loss 9.9158, lr 0.00001, time 431.09ms, mfu 35.08%
iter 130: loss 9.8674, lr 0.00001, time 424.61ms, mfu 35.12%
iter 140: loss 9.7482, lr 0.00001, time 425.38ms, mfu 35.15%
iter 150: loss 9.7161, lr 0.00002, time 425.38ms, mfu 35.18%
iter 160: loss 9.7273, lr 0.00002, time 423.15ms, mfu 35.22%
iter 170: loss 9.6771, lr 0.00002, time 424.55ms, mfu 35.24%
iter 180: loss 9.6473, lr 0.00002, time 423.77ms, mfu 35.27%
iter 190: loss 9.6127, lr 0.00002, time 422.85ms, mfu 35.31%
step 200: train loss 9.5932, val loss 9.6172
  MLP Rank Utilization (L0): 71.09% (546/768)
Attn LoRA    | 47/48           | 97.92%
--- Model Analysis ---
  Embedding Utilization (L0): 97.53% (749/768)
Embed LoRA   | 47/48           | 97.92%
  Average Attention Entropy:  5.8912
----------------------
saving checkpoint to out

=== SCALING OPERATION TRIGGERED (DDP SYNC) ===
Operation: merge_lora_weights
Trigger reason: Timeout
Current val loss: 9.6172, Trigger loss: 4.0000
Iterations since last op: 200, Max wait: 200
Executing operation: merge_lora_weights first burn with value: None
Performing architectural operation: merge_lora_weights
Merging LoRA weights into main weights...
LoRA weights merged and reset.

Detailed parameter count:
  total                  | Total:   44,773,632 | Trainable:    4,370,688
  token_embeddings       | Total:    2,451,456 | Trainable:    2,451,456
  position_embeddings    | Total:            0 | Trainable:            0
  attention_layers       | Total:    2,506,752 | Trainable:      737,280
  feed_forward_layers    | Total:    1,179,648 | Trainable:    1,179,648
  layer_norms            | Total:        1,536 | Trainable:        1,536
  final_layer_norm       | Total:          768 | Trainable:          768
  language_model_head    | Total:   38,633,472 | Trainable:            0
------------------------------------------------------------
Re-configuring optimizer after architectural change...
num decayed parameter tensors: 7, with 4,368,384 parameters
num non-decayed parameter tensors: 3, with 2,304 parameters
using fused AdamW: True
Transferring optimizer state for existing parameters...
Transferred optimizer state for 0 / 12 parameters
Re-compiling the model...

============================================================
             ARCHITECTURE CHANGE (at Iter 200)
============================================================
  n_layer                | 1
  n_head                 | 12
  n_embd                 | 768
  n_hidden               | 768
  block_size             | 1024
  vocab_size             | 50304
  dropout                | 0.0
  bias                   | False
  embedding_mode         | lora
  attn_lora_rank         | 48
  embedding_rank         | 48
  lora_alpha             | 1.0
============================================================

Architectural operation completed successfully.
=== SCALING OPERATION COMPLETE ===

merge_lora_weights 9.617229461669922 4.0 200
iter 200: loss 9.6160, lr 0.00002, time 3877.73ms, mfu 32.17%
iter 210: loss 9.6343, lr 0.00002, time 434.24ms, mfu 32.42%
iter 220: loss 9.5553, lr 0.00002, time 425.96ms, mfu 32.71%
iter 230: loss 9.5482, lr 0.00002, time 425.57ms, mfu 32.98%
iter 240: loss 9.5234, lr 0.00002, time 430.63ms, mfu 33.18%
iter 250: loss 9.5430, lr 0.00002, time 426.75ms, mfu 33.39%
iter 260: loss 9.4684, lr 0.00002, time 424.86ms, mfu 33.60%
iter 270: loss 9.4998, lr 0.00002, time 432.32ms, mfu 33.72%
iter 280: loss 9.4528, lr 0.00002, time 429.71ms, mfu 33.85%
iter 290: loss 9.4902, lr 0.00002, time 428.57ms, mfu 33.98%
iter 300: loss 9.4864, lr 0.00002, time 431.03ms, mfu 34.08%
iter 310: loss 9.4587, lr 0.00002, time 429.68ms, mfu 34.18%
iter 320: loss 9.4402, lr 0.00002, time 431.81ms, mfu 34.25%
iter 330: loss 9.4342, lr 0.00002, time 427.53ms, mfu 34.34%
iter 340: loss 9.4693, lr 0.00002, time 430.32ms, mfu 34.41%
iter 350: loss 9.4150, lr 0.00002, time 431.60ms, mfu 34.46%
iter 360: loss 9.3820, lr 0.00002, time 430.01ms, mfu 34.52%
iter 370: loss 9.4245, lr 0.00002, time 429.81ms, mfu 34.57%
iter 380: loss 9.4426, lr 0.00002, time 429.55ms, mfu 34.62%
iter 390: loss 9.3109, lr 0.00002, time 431.28ms, mfu 34.65%
step 400: train loss 9.3297, val loss 9.3312
  MLP Rank Utilization (L0): 70.96% (545/768)
Attn LoRA    | 24/48           | 50.00%
--- Model Analysis ---
  Embedding Utilization (L0): 97.53% (749/768)
Embed LoRA   | 30/48           | 62.50%
  Average Attention Entropy:  5.8915
----------------------
saving checkpoint to out

=== SCALING OPERATION TRIGGERED (DDP SYNC) ===
Operation: merge_lora_weights
Trigger reason: Timeout
Current val loss: 9.3312, Trigger loss: 4.0000
Iterations since last op: 200, Max wait: 200
Executing operation: merge_lora_weights first burn with value: None
Performing architectural operation: merge_lora_weights
Merging LoRA weights into main weights...
LoRA weights merged and reset.

Detailed parameter count:
  total                  | Total:   44,773,632 | Trainable:    4,370,688
  token_embeddings       | Total:    2,451,456 | Trainable:    2,451,456
  position_embeddings    | Total:            0 | Trainable:            0
  attention_layers       | Total:    2,506,752 | Trainable:      737,280
  feed_forward_layers    | Total:    1,179,648 | Trainable:    1,179,648
  layer_norms            | Total:        1,536 | Trainable:        1,536
  final_layer_norm       | Total:          768 | Trainable:          768
  language_model_head    | Total:   38,633,472 | Trainable:            0
------------------------------------------------------------
Re-configuring optimizer after architectural change...
num decayed parameter tensors: 7, with 4,368,384 parameters
num non-decayed parameter tensors: 3, with 2,304 parameters
using fused AdamW: True
Transferring optimizer state for existing parameters...
Transferred optimizer state for 0 / 12 parameters
Re-compiling the model...

============================================================
             ARCHITECTURE CHANGE (at Iter 400)
============================================================
  n_layer                | 1
  n_head                 | 12
  n_embd                 | 768
  n_hidden               | 768
  block_size             | 1024
  vocab_size             | 50304
  dropout                | 0.0
  bias                   | False
  embedding_mode         | lora
  attn_lora_rank         | 48
  embedding_rank         | 48
  lora_alpha             | 1.0
============================================================

Architectural operation completed successfully.
=== SCALING OPERATION COMPLETE ===

merge_lora_weights 9.331218719482422 4.0 200
iter 400: loss 9.3600, lr 0.00002, time 3964.23ms, mfu 31.56%
iter 410: loss 9.3664, lr 0.00002, time 444.90ms, mfu 31.79%
iter 420: loss 9.3648, lr 0.00002, time 425.18ms, mfu 32.16%
iter 430: loss 9.3362, lr 0.00002, time 425.35ms, mfu 32.48%
iter 440: loss 9.3708, lr 0.00002, time 432.57ms, mfu 32.71%
iter 450: loss 9.3783, lr 0.00002, time 428.25ms, mfu 32.96%
iter 460: loss 9.3518, lr 0.00002, time 428.37ms, mfu 33.18%
iter 470: loss 9.2885, lr 0.00002, time 425.73ms, mfu 33.40%
iter 480: loss 9.2254, lr 0.00002, time 427.87ms, mfu 33.58%
iter 490: loss 9.2260, lr 0.00002, time 430.91ms, mfu 33.72%
iter 500: loss 9.2538, lr 0.00002, time 431.34ms, mfu 33.84%
iter 510: loss 9.1771, lr 0.00002, time 429.36ms, mfu 33.96%
iter 520: loss 9.2599, lr 0.00002, time 430.23ms, mfu 34.07%
iter 530: loss 9.2499, lr 0.00002, time 429.24ms, mfu 34.17%
iter 540: loss 9.2650, lr 0.00002, time 430.18ms, mfu 34.25%
iter 550: loss 9.1593, lr 0.00002, time 427.31ms, mfu 34.35%
iter 560: loss 9.2526, lr 0.00002, time 431.34ms, mfu 34.41%
iter 570: loss 9.2539, lr 0.00002, time 427.89ms, mfu 34.49%
iter 580: loss 9.2116, lr 0.00002, time 428.51ms, mfu 34.55%
iter 590: loss 9.2824, lr 0.00002, time 431.73ms, mfu 34.59%
step 600: train loss 9.2556, val loss 9.3222
  MLP Rank Utilization (L0): 70.83% (544/768)
Attn LoRA    | 31/48           | 64.58%
--- Model Analysis ---
  Embedding Utilization (L0): 97.53% (749/768)
Embed LoRA   | 23/48           | 47.92%
  Average Attention Entropy:  5.8919
----------------------
saving checkpoint to out

=== SCALING OPERATION TRIGGERED (DDP SYNC) ===
Operation: merge_lora_weights
Trigger reason: Timeout
Current val loss: 9.3222, Trigger loss: 4.0000
Iterations since last op: 200, Max wait: 200
Executing operation: merge_lora_weights first burn with value: None
Performing architectural operation: merge_lora_weights
Merging LoRA weights into main weights...
LoRA weights merged and reset.

Detailed parameter count:
  total                  | Total:   44,773,632 | Trainable:    4,370,688
  token_embeddings       | Total:    2,451,456 | Trainable:    2,451,456
  position_embeddings    | Total:            0 | Trainable:            0
  attention_layers       | Total:    2,506,752 | Trainable:      737,280
  feed_forward_layers    | Total:    1,179,648 | Trainable:    1,179,648
  layer_norms            | Total:        1,536 | Trainable:        1,536
  final_layer_norm       | Total:          768 | Trainable:          768
  language_model_head    | Total:   38,633,472 | Trainable:            0
------------------------------------------------------------
Re-configuring optimizer after architectural change...
num decayed parameter tensors: 7, with 4,368,384 parameters
num non-decayed parameter tensors: 3, with 2,304 parameters
using fused AdamW: True
Transferring optimizer state for existing parameters...
Transferred optimizer state for 0 / 12 parameters
Re-compiling the model...

============================================================
             ARCHITECTURE CHANGE (at Iter 600)
============================================================
  n_layer                | 1
  n_head                 | 12
  n_embd                 | 768
  n_hidden               | 768
  block_size             | 1024
  vocab_size             | 50304
  dropout                | 0.0
  bias                   | False
  embedding_mode         | lora
  attn_lora_rank         | 48
  embedding_rank         | 48
  lora_alpha             | 1.0
============================================================

Architectural operation completed successfully.
=== SCALING OPERATION COMPLETE ===

merge_lora_weights 9.322248458862305 4.0 200
iter 600: loss 9.2229, lr 0.00002, time 3689.69ms, mfu 31.54%
iter 610: loss 9.2384, lr 0.00002, time 442.47ms, mfu 31.79%
iter 620: loss 9.2780, lr 0.00002, time 426.11ms, mfu 32.14%
iter 630: loss 9.2464, lr 0.00002, time 427.77ms, mfu 32.45%
iter 640: loss 9.1476, lr 0.00002, time 429.38ms, mfu 32.71%
iter 650: loss 9.1777, lr 0.00002, time 428.94ms, mfu 32.95%
iter 660: loss 9.1563, lr 0.00002, time 429.81ms, mfu 33.16%
iter 670: loss 9.1982, lr 0.00002, time 431.62ms, mfu 33.33%
iter 680: loss 9.1495, lr 0.00002, time 429.61ms, mfu 33.51%
iter 690: loss 9.1153, lr 0.00002, time 432.10ms, mfu 33.64%
iter 700: loss 9.1429, lr 0.00002, time 429.30ms, mfu 33.79%
iter 710: loss 9.1851, lr 0.00002, time 431.32ms, mfu 33.90%
iter 720: loss 9.2017, lr 0.00002, time 430.60ms, mfu 34.01%
iter 730: loss 9.1429, lr 0.00002, time 430.08ms, mfu 34.11%
iter 740: loss 9.1797, lr 0.00002, time 431.20ms, mfu 34.19%
iter 750: loss 9.1110, lr 0.00002, time 431.15ms, mfu 34.26%
iter 760: loss 9.1175, lr 0.00002, time 429.63ms, mfu 34.34%
iter 770: loss 9.2034, lr 0.00002, time 430.14ms, mfu 34.41%
iter 780: loss 9.1608, lr 0.00002, time 428.04ms, mfu 34.49%
iter 790: loss 9.1012, lr 0.00002, time 429.07ms, mfu 34.55%
