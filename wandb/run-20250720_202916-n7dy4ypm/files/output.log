Calculating target architecture based on schedule...

============================================================
       TARGET MODEL ARCHITECTURE (at end of schedule)
============================================================
  n_layer                | 2
  n_head                 | 12
  n_embd                 | 768
  n_hidden               | 1536
  block_size             | 1024
  vocab_size             | 50304
  dropout                | 0.0
  bias                   | False
  embedding_mode         | standard
  attn_lora_rank         | 0
  embedding_rank         | 0
  lora_alpha             | 0.0
============================================================


============================================================
           INITIAL MODEL ARCHITECTURE (at Iter 0)
============================================================
  n_layer                | 1
  n_head                 | 12
  n_embd                 | 768
  n_hidden               | 768
  block_size             | 1024
  vocab_size             | 50304
  dropout                | 0.0
  bias                   | False
  embedding_mode         | lora
  attn_lora_rank         | 48
  embedding_rank         | 48
  lora_alpha             | 1.0
============================================================


Detailed parameter count:
  total                  | Total:   44,773,632 | Trainable:    4,370,688
  token_embeddings       | Total:    2,451,456 | Trainable:    2,451,456
  position_embeddings    | Total:            0 | Trainable:            0
  attention_layers       | Total:    2,506,752 | Trainable:      737,280
  feed_forward_layers    | Total:    1,179,648 | Trainable:    1,179,648
  layer_norms            | Total:        1,536 | Trainable:        1,536
  final_layer_norm       | Total:          768 | Trainable:          768
  language_model_head    | Total:   38,633,472 | Trainable:            0
------------------------------------------------------------
eval every:200
iter 280: loss 9.0129, lr 0.00020, time 3064.80ms, mfu -100.00%
iter 290: loss 9.0197, lr 0.00020, time 5.81ms, mfu 1295.62%
iter 300: loss 9.0288, lr 0.00020, time 9.06ms, mfu 1249.17%
iter 310: loss 8.9318, lr 0.00020, time 5.67ms, mfu 1257.04%
iter 320: loss 9.0309, lr 0.00020, time 6.45ms, mfu 1248.07%
iter 330: loss 8.9371, lr 0.00020, time 6.11ms, mfu 1246.54%
iter 340: loss 8.9338, lr 0.00020, time 5.77ms, mfu 1252.43%
iter 350: loss 8.8872, lr 0.00020, time 9.97ms, mfu 1202.73%
iter 360: loss 8.8964, lr 0.00020, time 5.60ms, mfu 1216.95%
iter 370: loss 8.7985, lr 0.00020, time 9.26ms, mfu 1176.60%
iter 380: loss 8.8776, lr 0.00020, time 5.72ms, mfu 1190.52%
iter 390: loss 8.7769, lr 0.00020, time 8.85ms, mfu 1156.52%
step 400: train loss 8.8061, val loss 8.8450
  MLP Rank Utilization (L0): 69.53% (534/768)
Attn LoRA    | 35/48           | 72.92%
--- Model Analysis ---
  Embedding Utilization (L0): 97.53% (749/768)
Embed LoRA   | 22/48           | 45.83%
  Average Attention Entropy:  5.8944
----------------------
saving checkpoint to out
merge_lora_weights 8.844999313354492 4.0 10500
iter 400: loss 8.8441, lr 0.00020, time 4437.85ms, mfu 1041.04%
iter 410: loss 8.9124, lr 0.00020, time 5.81ms, mfu 1066.57%
iter 420: loss 8.7970, lr 0.00020, time 9.31ms, mfu 1040.80%
iter 430: loss 8.8108, lr 0.00020, time 6.58ms, mfu 1051.25%
iter 440: loss 8.8427, lr 0.00020, time 5.89ms, mfu 1074.04%
iter 450: loss 8.8490, lr 0.00020, time 6.00ms, mfu 1092.23%
iter 460: loss 8.8750, lr 0.00020, time 7.65ms, mfu 1081.44%
iter 470: loss 8.8748, lr 0.00020, time 6.87ms, mfu 1082.95%
iter 480: loss 8.7738, lr 0.00020, time 6.59ms, mfu 1088.96%
iter 490: loss 8.7430, lr 0.00020, time 6.13ms, mfu 1102.90%
iter 500: loss 8.8094, lr 0.00020, time 14.68ms, mfu 1043.91%
iter 510: loss 8.8895, lr 0.00020, time 6.26ms, mfu 1059.91%
iter 520: loss 8.8066, lr 0.00020, time 5.86ms, mfu 1082.41%
iter 530: loss 8.7400, lr 0.00020, time 6.05ms, mfu 1098.73%
iter 540: loss 8.8051, lr 0.00020, time 10.73ms, mfu 1059.06%
iter 550: loss 8.6926, lr 0.00020, time 6.11ms, mfu 1076.37%
iter 560: loss 8.6745, lr 0.00020, time 6.74ms, mfu 1080.49%
iter 570: loss 8.7723, lr 0.00020, time 5.99ms, mfu 1098.22%
iter 580: loss 8.7189, lr 0.00020, time 6.07ms, mfu 1112.55%
iter 590: loss 8.7916, lr 0.00020, time 5.81ms, mfu 1130.96%
step 600: train loss 8.7772, val loss 8.6732
  MLP Rank Utilization (L0): 69.14% (531/768)
Attn LoRA    | 7/48            | 14.58%
--- Model Analysis ---
  Embedding Utilization (L0): 97.53% (749/768)
Embed LoRA   | 2/48            | 4.17%
  Average Attention Entropy:  5.8956
----------------------
saving checkpoint to out
merge_lora_weights 8.673195838928223 4.0 10500
iter 600: loss 8.6330, lr 0.00020, time 3183.73ms, mfu 1018.10%
iter 610: loss 8.6962, lr 0.00020, time 17.94ms, mfu 958.27%
iter 620: loss 8.6687, lr 0.00020, time 16.06ms, mfu 909.33%
iter 630: loss 8.7156, lr 0.00020, time 7.99ms, mfu 912.68%
iter 640: loss 8.7035, lr 0.00020, time 18.02ms, mfu 863.20%
iter 650: loss 8.6223, lr 0.00020, time 6.37ms, mfu 895.09%
iter 660: loss 8.7542, lr 0.00020, time 6.49ms, mfu 921.56%
iter 670: loss 8.5871, lr 0.00020, time 6.13ms, mfu 952.33%
iter 680: loss 8.6183, lr 0.00020, time 16.49ms, mfu 902.77%
iter 690: loss 8.6155, lr 0.00020, time 5.63ms, mfu 946.35%
iter 700: loss 8.6369, lr 0.00020, time 6.37ms, mfu 970.01%
iter 710: loss 8.5904, lr 0.00020, time 6.51ms, mfu 988.61%
iter 720: loss 8.6189, lr 0.00020, time 24.37ms, mfu 920.65%
iter 730: loss 8.6376, lr 0.00020, time 8.52ms, mfu 917.00%
iter 740: loss 8.5813, lr 0.00020, time 5.74ms, mfu 956.53%
iter 750: loss 8.6526, lr 0.00020, time 5.67ms, mfu 993.69%
iter 760: loss 8.5417, lr 0.00020, time 6.85ms, mfu 1004.31%
iter 770: loss 8.5532, lr 0.00020, time 6.24ms, mfu 1024.50%
iter 780: loss 8.6774, lr 0.00020, time 5.81ms, mfu 1051.67%
iter 790: loss 8.5604, lr 0.00020, time 7.88ms, mfu 1042.12%
step 800: train loss 8.6390, val loss 8.5755
  MLP Rank Utilization (L0): 68.88% (529/768)
Attn LoRA    | 3/48            | 6.25%
--- Model Analysis ---
  Embedding Utilization (L0): 97.53% (749/768)
Embed LoRA   | 2/48            | 4.17%
  Average Attention Entropy:  5.8958
----------------------
saving checkpoint to out
merge_lora_weights 8.57554817199707 4.0 10500
iter 800: loss 8.5501, lr 0.00020, time 3529.99ms, mfu 938.12%
iter 810: loss 8.6487, lr 0.00020, time 5.74ms, mfu 975.60%
iter 820: loss 8.5945, lr 0.00020, time 5.85ms, mfu 1006.77%
iter 830: loss 8.6954, lr 0.00020, time 5.73ms, mfu 1037.63%
iter 840: loss 8.6004, lr 0.00020, time 7.25ms, mfu 1037.72%
iter 850: loss 8.6643, lr 0.00020, time 5.86ms, mfu 1062.41%
iter 860: loss 8.5817, lr 0.00020, time 6.87ms, mfu 1065.76%
iter 870: loss 8.6777, lr 0.00020, time 14.27ms, mfu 1011.97%
iter 880: loss 8.5859, lr 0.00020, time 6.57ms, mfu 1025.37%
iter 890: loss 8.5404, lr 0.00020, time 5.78ms, mfu 1053.04%
iter 900: loss 8.5782, lr 0.00020, time 6.00ms, mfu 1073.23%
iter 910: loss 8.5588, lr 0.00020, time 5.81ms, mfu 1095.54%
iter 920: loss 8.5313, lr 0.00020, time 5.91ms, mfu 1113.52%
iter 930: loss 8.4435, lr 0.00020, time 6.01ms, mfu 1127.52%
iter 940: loss 8.4991, lr 0.00020, time 6.20ms, mfu 1136.18%
iter 950: loss 8.4958, lr 0.00020, time 16.45ms, mfu 1068.34%
iter 960: loss 8.4933, lr 0.00020, time 6.08ms, mfu 1085.30%
iter 970: loss 8.5760, lr 0.00020, time 6.26ms, mfu 1097.02%
iter 980: loss 8.4818, lr 0.00020, time 6.94ms, mfu 1095.86%
iter 990: loss 8.4418, lr 0.00020, time 6.42ms, mfu 1103.60%
step 1000: train loss 8.5089, val loss 8.6573
  MLP Rank Utilization (L0): 68.75% (528/768)
Attn LoRA    | 3/48            | 6.25%
--- Model Analysis ---
  Embedding Utilization (L0): 97.53% (749/768)
Embed LoRA   | 2/48            | 4.17%
  Average Attention Entropy:  5.8943
----------------------
merge_lora_weights 8.657302856445312 4.0 10500
iter 1000: loss 8.5142, lr 0.00020, time 2638.62ms, mfu 993.52%
iter 1010: loss 8.4678, lr 0.00020, time 6.00ms, mfu 1019.68%
iter 1020: loss 8.5501, lr 0.00020, time 13.04ms, mfu 975.47%
iter 1030: loss 8.5164, lr 0.00020, time 5.85ms, mfu 1006.70%
iter 1040: loss 8.5647, lr 0.00020, time 7.53ms, mfu 1006.07%
iter 1050: loss 8.6016, lr 0.00020, time 6.24ms, mfu 1026.09%
iter 1060: loss 8.5112, lr 0.00020, time 5.76ms, mfu 1054.11%
iter 1070: loss 8.4279, lr 0.00020, time 6.61ms, mfu 1062.60%
iter 1080: loss 8.5061, lr 0.00020, time 7.03ms, mfu 1063.46%
iter 1090: loss 8.4366, lr 0.00020, time 6.33ms, mfu 1076.11%
iter 1100: loss 8.4074, lr 0.00020, time 9.78ms, mfu 1045.49%
iter 1110: loss 8.4871, lr 0.00020, time 6.37ms, mfu 1059.17%
iter 1120: loss 8.4919, lr 0.00020, time 15.49ms, mfu 1001.86%
iter 1130: loss 8.3301, lr 0.00020, time 6.03ms, mfu 1026.47%
iter 1140: loss 8.5416, lr 0.00020, time 5.76ms, mfu 1054.56%
iter 1150: loss 8.5294, lr 0.00020, time 6.16ms, mfu 1071.33%
iter 1160: loss 8.4392, lr 0.00020, time 5.89ms, mfu 1091.94%
iter 1170: loss 8.4983, lr 0.00020, time 5.96ms, mfu 1109.12%
iter 1180: loss 8.3794, lr 0.00020, time 6.00ms, mfu 1123.73%
iter 1190: loss 8.4713, lr 0.00020, time 5.86ms, mfu 1139.88%
step 1200: train loss 8.3411, val loss 8.3311
  MLP Rank Utilization (L0): 68.75% (528/768)
Attn LoRA    | 2/48            | 4.17%
--- Model Analysis ---
  Embedding Utilization (L0): 97.53% (749/768)
Embed LoRA   | 2/48            | 4.17%
  Average Attention Entropy:  5.8865
----------------------
saving checkpoint to out
merge_lora_weights 8.331085205078125 4.0 10500
iter 1200: loss 8.4487, lr 0.00020, time 3345.12ms, mfu 1026.12%
iter 1210: loss 8.4576, lr 0.00020, time 9.13ms, mfu 1005.96%
iter 1220: loss 8.4221, lr 0.00020, time 5.63ms, mfu 1039.23%
iter 1230: loss 8.4530, lr 0.00020, time 5.70ms, mfu 1067.36%
iter 1240: loss 8.3830, lr 0.00020, time 5.69ms, mfu 1093.09%
iter 1250: loss 8.3875, lr 0.00020, time 6.50ms, mfu 1099.67%
iter 1260: loss 8.4012, lr 0.00020, time 6.51ms, mfu 1105.31%
iter 1270: loss 8.3504, lr 0.00020, time 5.92ms, mfu 1122.04%
iter 1280: loss 8.4106, lr 0.00020, time 15.23ms, mfu 1059.30%
iter 1290: loss 8.5997, lr 0.00020, time 5.84ms, mfu 1082.43%
iter 1300: loss 8.3957, lr 0.00020, time 5.84ms, mfu 1103.17%
iter 1310: loss 8.3509, lr 0.00020, time 7.69ms, mfu 1090.75%
iter 1320: loss 8.3251, lr 0.00020, time 5.83ms, mfu 1110.79%
iter 1330: loss 8.3124, lr 0.00020, time 6.16ms, mfu 1121.90%
iter 1340: loss 8.3355, lr 0.00020, time 6.40ms, mfu 1127.33%
iter 1350: loss 8.3955, lr 0.00020, time 21.55ms, mfu 1049.54%
iter 1360: loss 8.3634, lr 0.00020, time 18.02ms, mfu 986.38%
iter 1370: loss 8.3837, lr 0.00020, time 23.10ms, mfu 920.33%
iter 1380: loss 8.3360, lr 0.00020, time 6.26ms, mfu 948.61%
iter 1390: loss 8.2337, lr 0.00020, time 5.71ms, mfu 985.59%
step 1400: train loss 8.2838, val loss 8.3074
  MLP Rank Utilization (L0): 68.62% (527/768)
Attn LoRA    | 2/48            | 4.17%
--- Model Analysis ---
  Embedding Utilization (L0): 97.53% (749/768)
Embed LoRA   | 2/48            | 4.17%
  Average Attention Entropy:  5.8596
----------------------
saving checkpoint to out
merge_lora_weights 8.307394981384277 4.0 10500
iter 1400: loss 8.4645, lr 0.00020, time 3351.80ms, mfu 887.26%
iter 1410: loss 8.3605, lr 0.00020, time 9.67ms, mfu 876.37%
iter 1420: loss 8.3374, lr 0.00020, time 7.08ms, mfu 895.11%
iter 1430: loss 8.2333, lr 0.00020, time 5.82ms, mfu 934.92%
iter 1440: loss 8.2941, lr 0.00020, time 5.70ms, mfu 973.57%
iter 1450: loss 8.3166, lr 0.00020, time 5.79ms, mfu 1006.27%
iter 1460: loss 8.3476, lr 0.00020, time 5.99ms, mfu 1031.45%
iter 1470: loss 8.3577, lr 0.00020, time 5.86ms, mfu 1056.80%
iter 1480: loss 8.4290, lr 0.00020, time 5.72ms, mfu 1082.84%
iter 1490: loss 8.2807, lr 0.00020, time 5.64ms, mfu 1107.98%
iter 1500: loss 8.4483, lr 0.00020, time 8.46ms, mfu 1086.19%
iter 1510: loss 8.3605, lr 0.00020, time 6.08ms, mfu 1101.41%
iter 1520: loss 8.2770, lr 0.00020, time 16.71ms, mfu 1036.35%
iter 1530: loss 8.3957, lr 0.00020, time 5.74ms, mfu 1063.82%
iter 1540: loss 8.3513, lr 0.00020, time 5.64ms, mfu 1090.87%
iter 1550: loss 8.3100, lr 0.00020, time 9.36ms, mfu 1062.21%
iter 1560: loss 8.3311, lr 0.00020, time 6.23ms, mfu 1076.86%
iter 1570: loss 8.3069, lr 0.00020, time 6.26ms, mfu 1089.55%
iter 1580: loss 8.2368, lr 0.00020, time 6.34ms, mfu 1099.36%
iter 1590: loss 8.3357, lr 0.00020, time 6.89ms, mfu 1098.66%
step 1600: train loss 8.2043, val loss 8.2193
  MLP Rank Utilization (L0): 68.62% (527/768)
Attn LoRA    | 2/48            | 4.17%
--- Model Analysis ---
  Embedding Utilization (L0): 97.53% (749/768)
Embed LoRA   | 2/48            | 4.17%
  Average Attention Entropy:  5.7862
----------------------
saving checkpoint to out
merge_lora_weights 8.219267845153809 4.0 10500
iter 1600: loss 8.2503, lr 0.00020, time 3104.59ms, mfu 989.04%
iter 1610: loss 8.3525, lr 0.00020, time 6.56ms, mfu 1004.86%
iter 1620: loss 8.2099, lr 0.00020, time 6.24ms, mfu 1025.03%
iter 1630: loss 8.2865, lr 0.00020, time 10.53ms, mfu 994.05%
iter 1640: loss 8.3801, lr 0.00020, time 6.00ms, mfu 1020.26%
iter 1650: loss 8.2940, lr 0.00020, time 5.62ms, mfu 1052.28%
iter 1660: loss 8.3363, lr 0.00020, time 10.34ms, mfu 1019.91%
iter 1670: loss 8.2787, lr 0.00020, time 5.80ms, mfu 1047.77%
iter 1680: loss 8.1374, lr 0.00020, time 6.70ms, mfu 1055.46%
iter 1690: loss 8.2388, lr 0.00020, time 7.95ms, mfu 1044.68%
iter 1700: loss 8.1570, lr 0.00020, time 11.62ms, mfu 1005.03%
iter 1710: loss 8.3395, lr 0.00020, time 6.46ms, mfu 1021.11%
iter 1720: loss 8.2466, lr 0.00020, time 5.72ms, mfu 1050.60%
iter 1730: loss 8.4367, lr 0.00020, time 6.14ms, mfu 1068.13%
iter 1740: loss 8.1799, lr 0.00020, time 9.57ms, mfu 1039.98%
iter 1750: loss 8.2270, lr 0.00020, time 6.78ms, mfu 1047.01%
iter 1760: loss 8.2013, lr 0.00020, time 7.99ms, mfu 1036.56%
iter 1770: loss 8.2034, lr 0.00020, time 6.16ms, mfu 1055.17%
iter 1780: loss 8.1941, lr 0.00020, time 5.73ms, mfu 1080.97%
iter 1790: loss 8.2691, lr 0.00020, time 15.08ms, mfu 1022.81%
step 1800: train loss 8.1175, val loss 8.1623
  MLP Rank Utilization (L0): 68.62% (527/768)
Attn LoRA    | 2/48            | 4.17%
--- Model Analysis ---
  Embedding Utilization (L0): 97.53% (749/768)
Embed LoRA   | 2/48            | 4.17%
  Average Attention Entropy:  5.7175
----------------------
saving checkpoint to out
merge_lora_weights 8.162266731262207 4.0 10500
iter 1800: loss 8.1618, lr 0.00020, time 3099.57ms, mfu 920.77%
Traceback (most recent call last):
  File "/teamspace/studios/this_studio/nanoGPT/train.py", line 865, in <module>
    lossf = loss.item() * gradient_accumulation_steps
KeyboardInterrupt
Traceback (most recent call last):
  File "/teamspace/studios/this_studio/nanoGPT/train.py", line 865, in <module>
    lossf = loss.item() * gradient_accumulation_steps
KeyboardInterrupt
