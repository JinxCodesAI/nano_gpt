Calculating target architecture based on schedule...

============================================================
       TARGET MODEL ARCHITECTURE (at end of schedule)
============================================================
  n_layer                | 6
  n_head                 | 12
  n_embd                 | 768
  n_hidden               | 1536
  block_size             | 1024
  vocab_size             | 50304
  dropout                | 0.0
  bias                   | False
  embedding_mode         | standard
  attn_lora_rank         | 0
  embedding_rank         | 0
  lora_alpha             | 0.0
============================================================


============================================================
           INITIAL MODEL ARCHITECTURE (at Iter 0)
============================================================
  n_layer                | 3
  n_head                 | 12
  n_embd                 | 768
  n_hidden               | 768
  block_size             | 1024
  vocab_size             | 50304
  dropout                | 0.0
  bias                   | False
  embedding_mode         | lora
  attn_lora_rank         | 48
  embedding_rank         | 48
  lora_alpha             | 1.0
============================================================


Detailed parameter count:
  total                  | Total:   52,149,504 | Trainable:    8,207,616
  token_embeddings       | Total:    2,451,456 | Trainable:    2,451,456
  position_embeddings    | Total:            0 | Trainable:            0
  attention_layers       | Total:    7,520,256 | Trainable:    2,211,840
  feed_forward_layers    | Total:    3,538,944 | Trainable:    3,538,944
  layer_norms            | Total:        4,608 | Trainable:        4,608
  final_layer_norm       | Total:          768 | Trainable:          768
  language_model_head    | Total:   38,633,472 | Trainable:            0
------------------------------------------------------------
eval every:100
step 0: train loss 10.9690, val loss 10.9694
  MLP Rank Utilization (L0): 71.09% (546/768)
Attn LoRA    | 47/48           | 97.92%
  MLP Rank Utilization (L1): 71.09% (546/768)
Attn LoRA    | 47/48           | 97.92%
  MLP Rank Utilization (L2): 71.22% (547/768)
Attn LoRA    | 47/48           | 97.92%
--- Model Analysis ---
  Embedding Utilization (L2): 97.53% (749/768)
  Average Attention Entropy:  5.8975
----------------------
Embed LoRA   | 47/48           | 97.92%
merge_lora_weights 10.969396591186523 6.0 1500
iter 0: loss 10.9510, lr 0.00000, time 10154.36ms, mfu -100.00%
iter 10: loss 10.4036, lr 0.00002, time 560.18ms, mfu 32.99%
iter 20: loss 9.8116, lr 0.00003, time 559.00ms, mfu 33.00%
iter 30: loss 9.6347, lr 0.00005, time 558.93ms, mfu 33.00%
iter 40: loss 9.5666, lr 0.00006, time 559.40ms, mfu 33.01%
iter 50: loss 9.4947, lr 0.00008, time 567.94ms, mfu 32.96%
iter 60: loss 9.3905, lr 0.00009, time 563.38ms, mfu 32.94%
iter 70: loss 9.2694, lr 0.00011, time 579.25ms, mfu 32.84%
iter 80: loss 9.2477, lr 0.00012, time 577.85ms, mfu 32.75%
iter 90: loss 9.1310, lr 0.00014, time 583.17ms, mfu 32.65%
step 100: train loss 9.0861, val loss 9.1261
  MLP Rank Utilization (L0): 71.09% (546/768)
Attn LoRA    | 47/48           | 97.92%
  MLP Rank Utilization (L1): 71.09% (546/768)
Attn LoRA    | 47/48           | 97.92%
  MLP Rank Utilization (L2): 71.09% (546/768)
Attn LoRA    | 47/48           | 97.92%
--- Model Analysis ---
  Embedding Utilization (L2): 97.53% (749/768)
  Average Attention Entropy:  5.8971
----------------------
Embed LoRA   | 47/48           | 97.92%
saving checkpoint to out
merge_lora_weights 9.126093864440918 6.0 1500
iter 100: loss 9.1430, lr 0.00015, time 5975.28ms, mfu 29.69%
iter 110: loss 9.0396, lr 0.00017, time 575.19ms, mfu 29.94%
iter 120: loss 8.9757, lr 0.00018, time 577.55ms, mfu 30.14%
iter 130: loss 8.9039, lr 0.00020, time 574.97ms, mfu 30.34%
iter 140: loss 8.9719, lr 0.00021, time 576.16ms, mfu 30.52%
iter 150: loss 9.0473, lr 0.00023, time 575.34ms, mfu 30.68%
iter 160: loss 8.7972, lr 0.00024, time 565.76ms, mfu 30.88%
iter 170: loss 8.7422, lr 0.00026, time 572.57ms, mfu 31.02%
iter 180: loss 8.8487, lr 0.00027, time 573.54ms, mfu 31.14%
iter 190: loss 8.7814, lr 0.00029, time 573.56ms, mfu 31.24%
step 200: train loss 8.7565, val loss 8.7752
  MLP Rank Utilization (L0): 71.09% (546/768)
Attn LoRA    | 47/48           | 97.92%
  MLP Rank Utilization (L1): 70.83% (544/768)
Attn LoRA    | 47/48           | 97.92%
  MLP Rank Utilization (L2): 70.96% (545/768)
Attn LoRA    | 47/48           | 97.92%
--- Model Analysis ---
  Embedding Utilization (L2): 97.53% (749/768)
  Average Attention Entropy:  5.8944
----------------------
Embed LoRA   | 43/48           | 89.58%
saving checkpoint to out
merge_lora_weights 8.775187492370605 6.0 1500
iter 200: loss 8.7423, lr 0.00030, time 5950.18ms, mfu 28.43%
iter 210: loss 8.8586, lr 0.00030, time 561.44ms, mfu 28.88%
iter 220: loss 8.7549, lr 0.00030, time 575.40ms, mfu 29.20%
iter 230: loss 8.8056, lr 0.00030, time 573.07ms, mfu 29.51%
iter 240: loss 8.7558, lr 0.00030, time 579.86ms, mfu 29.74%
iter 250: loss 8.7719, lr 0.00030, time 570.38ms, mfu 30.01%
iter 260: loss 8.8105, lr 0.00030, time 576.55ms, mfu 30.21%
iter 270: loss 8.6606, lr 0.00030, time 579.87ms, mfu 30.38%
iter 280: loss 8.6510, lr 0.00030, time 582.40ms, mfu 30.51%
iter 290: loss 8.6527, lr 0.00030, time 579.31ms, mfu 30.65%
step 300: train loss 8.5879, val loss 8.6377
  MLP Rank Utilization (L0): 71.09% (546/768)
Attn LoRA    | 43/48           | 89.58%
  MLP Rank Utilization (L1): 70.70% (543/768)
Attn LoRA    | 44/48           | 91.67%
  MLP Rank Utilization (L2): 70.57% (542/768)
Attn LoRA    | 45/48           | 93.75%
--- Model Analysis ---
  Embedding Utilization (L2): 97.53% (749/768)
  Average Attention Entropy:  5.8932
----------------------
Embed LoRA   | 19/48           | 39.58%
saving checkpoint to out
merge_lora_weights 8.63766860961914 6.0 1500
iter 300: loss 8.5383, lr 0.00030, time 6076.42ms, mfu 27.89%
iter 310: loss 8.6225, lr 0.00030, time 560.00ms, mfu 28.40%
iter 320: loss 8.5736, lr 0.00030, time 574.64ms, mfu 28.78%
iter 330: loss 8.5936, lr 0.00030, time 575.30ms, mfu 29.11%
iter 340: loss 8.6178, lr 0.00030, time 573.34ms, mfu 29.43%
iter 350: loss 8.5269, lr 0.00030, time 569.19ms, mfu 29.73%
Traceback (most recent call last):
  File "/teamspace/studios/this_studio/nanoGPT/train.py", line 873, in <module>
    scaler.scale(loss).backward()
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/_tensor.py", line 648, in backward
    torch.autograd.backward(
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/autograd/__init__.py", line 353, in backward
    _engine_run_backward(
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/autograd/graph.py", line 824, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
KeyboardInterrupt
Traceback (most recent call last):
  File "/teamspace/studios/this_studio/nanoGPT/train.py", line 873, in <module>
    scaler.scale(loss).backward()
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/_tensor.py", line 648, in backward
    torch.autograd.backward(
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/autograd/__init__.py", line 353, in backward
    _engine_run_backward(
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/autograd/graph.py", line 824, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
KeyboardInterrupt
Exception ignored in atexit callback: <function _start_and_connect_service.<locals>.teardown_atexit at 0x7e2e0d23a050>
Traceback (most recent call last):
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/wandb/sdk/lib/service/service_connection.py", line 54, in teardown_atexit
    conn.teardown(hooks.exit_code)
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/wandb/sdk/lib/service/service_connection.py", line 182, in teardown
    self._router.join()
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/wandb/sdk/interface/router.py", line 75, in join
    self._thread.join()
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/threading.py", line 1096, in join
    self._wait_for_tstate_lock()
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/threading.py", line 1116, in _wait_for_tstate_lock
    if lock.acquire(block, timeout):
KeyboardInterrupt:
Exception ignored in atexit callback: <function shutdown_compile_workers at 0x7e2e2a90dbd0>
Traceback (most recent call last):
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/_inductor/async_compile.py", line 113, in shutdown_compile_workers
    pool.shutdown()
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/_inductor/compile_worker/subproc_pool.py", line 239, in shutdown
    self.process.wait(300)
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/subprocess.py", line 1209, in wait
    return self._wait(timeout=timeout)
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/subprocess.py", line 1937, in _wait
    time.sleep(delay)
KeyboardInterrupt:
