
⚡ feature/enhanced-data-augmentation ~/nanoGPT python train.py config/train_shakespeare_char_continue_3.py
Overriding config with config/train_shakespeare_char_continue_3.py:
# train a miniature character-level shakespeare model
# good for debugging and playing on macbooks and such

init_from = 'resume' # 'scratch' or 'resume' or 'gpt2*'
out_dir = 'out-shakespeare-char'
eval_interval = 100 # keep frequent because we'll overfit
eval_iters = 50
log_interval = 10 # don't print too too often

# we expect to overfit on this small dataset, so only save when val improves
always_save_checkpoint = False

wandb_log = False # override via command line if you like
wandb_project = 'shakespeare-char'
wandb_run_name = 'mini-gpt'

dataset = 'shakespeare_char'
gradient_accumulation_steps = 1
batch_size = 256
block_size = 256 # context of up to 256 previous characters

# enhanced data augmentation settings
# uncomment to experiment with enhanced data to reduce overfitting
enhanced_data_probability = 0.2 # 50% enhanced data
min_prefix_length = 20 # shorter prefixes for character-level
max_prefix_length = 230 # shorter prefixes for character-level  
enhanced_generation_temperature = 0.3 # slightly more creative
enhanced_buffer_size = 512 # smaller buffer for this small dataset
enhanced_generation_batch_size =64


# baby GPT model :)
n_layer = 6
n_head = 6
n_embd = 384
dropout = 0.2

learning_rate = 1e-3 # with baby networks can afford to go a bit higher
max_iters = 5000
lr_decay_iters = 5000 # make equal to max_iters usually
min_lr = 1e-4 # learning_rate / 10 usually
beta2 = 0.99 # make a bit bigger because number of tokens per iter is small

warmup_iters = 100 # not super necessary potentially

# on macbook also add
# device = 'cpu'  # run on cpu only
# compile = False # do not torch compile the model

tokens per iteration will be: 65,536
found vocab_size = 65 (inside data/shakespeare_char/meta.pkl)
Resuming training from out-shakespeare-char
number of parameters: 10.65M
Initializing enhanced data augmentation...
number of parameters: 10.65M
Enhanced data augmentation initialized with probability 0.2
⏳ Background generation will start after model compilation...
/teamspace/studios/this_studio/nanoGPT/train.py:591: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(dtype == 'float16'))
num decayed parameter tensors: 26, with 10,740,096 parameters
num non-decayed parameter tensors: 13, with 4,992 parameters
using fused AdamW: True
compiling the model... (takes a ~minute)
🚀 Starting background enhanced sample generation...
📦 Waiting for full enhanced buffer (512 samples)...
🔥 Generated 64 enhanced samples → Buffer: 64/512 (12.5%)
📈 Buffer progress: 64/512 samples (12.5%) - Generated: 64 - Elapsed: 6s
🔥 Generated 64 enhanced samples → Buffer: 128/512 (25.0%)
📈 Buffer progress: 128/512 samples (25.0%) - Generated: 128 - Elapsed: 10s
🔥 Generated 64 enhanced samples → Buffer: 192/512 (37.5%)
📈 Buffer progress: 192/512 samples (37.5%) - Generated: 192 - Elapsed: 14s
🔥 Generated 64 enhanced samples → Buffer: 256/512 (50.0%)
📈 Buffer progress: 256/512 samples (50.0%) - Generated: 256 - Elapsed: 18s
🔥 Generated 64 enhanced samples → Buffer: 320/512 (62.5%)
📈 Buffer progress: 320/512 samples (62.5%) - Generated: 320 - Elapsed: 22s
🔥 Generated 64 enhanced samples → Buffer: 384/512 (75.0%)
📈 Buffer progress: 384/512 samples (75.0%) - Generated: 384 - Elapsed: 26s
🔥 Generated 64 enhanced samples → Buffer: 448/512 (87.5%)
📈 Buffer progress: 448/512 samples (87.5%) - Generated: 448 - Elapsed: 30s
🔥 Generated 64 enhanced samples → Buffer: 512/512 (100.0%)
Enhanced data generator: sleep_time=0.100s, buffer_size=512
📈 Buffer progress: 512/512 samples (100.0%) - Generated: 512 - Elapsed: 34s
✅ Enhanced buffer FULL: 512 samples ready! Generated: 512 - Total time: 34s
step 800: train loss 1.0517, val loss 1.4669
🔥 ENHANCED DATA: Total=13056, Enhanced=2593 (19.9%), Generated=512, Buffer=512
iter 800: loss 1.1481, time 11185.32ms, mfu -100.00%
iter 810: loss 1.1298, time 224.27ms, mfu 6.65%
iter 820: loss 1.1176, time 224.79ms, mfu 6.64%
iter 830: loss 1.1387, time 225.64ms, mfu 6.64%
iter 840: loss 1.1177, time 225.70ms, mfu 6.64%
iter 850: loss 1.1246, time 227.87ms, mfu 6.63%
iter 860: loss 1.0904, time 221.55ms, mfu 6.64%
iter 870: loss 1.1236, time 224.78ms, mfu 6.64%
iter 880: loss 1.0957, time 223.37ms, mfu 6.64%
Enhanced data generator: sleep_time=0.010s, buffer_size=512
iter 890: loss 1.0911, time 221.94ms, mfu 6.65%
step 900: train loss 0.9821, val loss 1.4874
🔥 ENHANCED DATA: Total=51456, Enhanced=10295 (20.0%), Generated=512, Buffer=512
iter 900: loss 1.0721, time 7445.94ms, mfu 6.00%
iter 910: loss 1.0888, time 222.49ms, mfu 6.07%
iter 920: loss 1.0906, time 221.41ms, mfu 6.14%
iter 930: loss 1.0875, time 219.03ms, mfu 6.21%
iter 940: loss 1.0538, time 222.18ms, mfu 6.26%
iter 950: loss 1.0478, time 221.02ms, mfu 6.30%
iter 960: loss 1.0543, time 218.08ms, mfu 6.36%
iter 970: loss 1.0525, time 220.11ms, mfu 6.40%
iter 980: loss 1.0445, time 231.51ms, mfu 6.40%
Enhanced data generator: sleep_time=0.010s, buffer_size=512
iter 990: loss 1.0205, time 219.96ms, mfu 6.44%
step 1000: train loss 0.9389, val loss 1.5342
🔥 ENHANCED DATA: Total=89856, Enhanced=17947 (20.0%), Generated=512, Buffer=512
iter 1000: loss 1.0047, time 7454.44ms, mfu 5.82%
iter 1010: loss 1.0427, time 220.26ms, mfu 5.91%
iter 1020: loss 1.0064, time 222.25ms, mfu 5.99%
iter 1030: loss 1.0471, time 219.02ms, mfu 6.07%
iter 1040: loss 1.0050, time 223.93ms, mfu 6.13%
iter 1050: loss 1.0244, time 218.95ms, mfu 6.20%
iter 1060: loss 0.9919, time 219.28ms, mfu 6.26%
iter 1070: loss 1.0023, time 224.66ms, mfu 6.30%
iter 1080: loss 0.9916, time 220.20ms, mfu 6.34%
Enhanced data generator: sleep_time=0.010s, buffer_size=512
iter 1090: loss 0.9794, time 221.40ms, mfu 6.38%
step 1100: train loss 0.8832, val loss 1.5193
🔥 ENHANCED DATA: Total=128256, Enhanced=25646 (20.0%), Generated=512, Buffer=512
iter 1100: loss 0.9984, time 7495.46ms, mfu 5.76%
iter 1110: loss 0.9819, time 221.26ms, mfu 5.86%
iter 1120: loss 1.0090, time 225.19ms, mfu 5.94%
iter 1130: loss 0.9801, time 226.80ms, mfu 6.00%
iter 1140: loss 0.9795, time 229.81ms, mfu 6.05%
iter 1150: loss 0.9872, time 231.35ms, mfu 6.09%
iter 1160: loss 0.9779, time 227.56ms, mfu 6.13%
iter 1170: loss 0.9584, time 225.16ms, mfu 6.18%
iter 1180: loss 0.9503, time 225.09ms, mfu 6.23%
iter 1190: loss 0.9778, time 221.95ms, mfu 6.28%
Enhanced data generator: sleep_time=0.010s, buffer_size=512
step 1200: train loss 0.8357, val loss 1.5471
🔥 ENHANCED DATA: Total=166656, Enhanced=33217 (19.9%), Generated=512, Buffer=512
iter 1200: loss 0.9550, time 7501.63ms, mfu 5.67%
iter 1210: loss 0.9506, time 220.61ms, mfu 5.78%
iter 1220: loss 0.9529, time 227.38ms, mfu 5.85%
iter 1230: loss 0.9482, time 221.23ms, mfu 5.94%
iter 1240: loss 0.9471, time 222.22ms, mfu 6.02%
iter 1250: loss 0.9443, time 220.78ms, mfu 6.09%
iter 1260: loss 0.9563, time 222.50ms, mfu 6.15%
iter 1270: loss 0.9629, time 221.70ms, mfu 6.21%
iter 1280: loss 0.9274, time 222.73ms, mfu 6.26%
iter 1290: loss 0.9084, time 226.02ms, mfu 6.29%
Enhanced data generator: sleep_time=0.010s, buffer_size=512
step 1300: train loss 0.7853, val loss 1.5791
🔥 ENHANCED DATA: Total=205056, Enhanced=40954 (20.0%), Generated=512, Buffer=512
iter 1300: loss 0.8951, time 7456.02ms, mfu 5.68%
iter 1310: loss 0.9093, time 226.92ms, mfu 5.77%
iter 1320: loss 0.9295, time 222.46ms, mfu 5.86%
iter 1330: loss 0.9029, time 225.33ms, mfu 5.94%
iter 1340: loss 0.9291, time 223.59ms, mfu 6.01%
iter 1350: loss 0.8919, time 226.36ms, mfu 6.07%
iter 1360: loss 0.9037, time 220.41ms, mfu 6.14%
iter 1370: loss 0.8971, time 223.92ms, mfu 6.19%
iter 1380: loss 0.9195, time 220.18ms, mfu 6.25%
iter 1390: loss 0.8915, time 224.24ms, mfu 6.29%
Enhanced data generator: sleep_time=0.010s, buffer_size=512
step 1400: train loss 0.7303, val loss 1.6056
🔥 ENHANCED DATA: Total=243456, Enhanced=48668 (20.0%), Generated=512, Buffer=512
iter 1400: loss 0.8999, time 7601.98ms, mfu 5.68%
iter 1410: loss 0.8624, time 222.88ms, mfu 5.78%
iter 1420: loss 0.8723, time 222.82ms, mfu 5.87%
iter 1430: loss 0.8971, time 224.31ms, mfu 5.95%
iter 1440: loss 0.8580, time 220.30ms, mfu 6.03%
iter 1450: loss 0.8880, time 223.24ms, mfu 6.09%
iter 1460: loss 0.8384, time 226.17ms, mfu 6.14%
iter 1470: loss 0.8516, time 220.40ms, mfu 6.21%
iter 1480: loss 0.8892, time 223.22ms, mfu 6.25%
iter 1490: loss 0.8524, time 227.98ms, mfu 6.28%
Enhanced data generator: sleep_time=0.010s, buffer_size=512
step 1500: train loss 0.6888, val loss 1.6303
🔥 ENHANCED DATA: Total=281856, Enhanced=56381 (20.0%), Generated=512, Buffer=512
iter 1500: loss 0.8625, time 7500.98ms, mfu 5.67%
iter 1510: loss 0.8735, time 221.61ms, mfu 5.78%
iter 1520: loss 0.8651, time 223.76ms, mfu 5.87%
iter 1530: loss 0.8214, time 223.55ms, mfu 5.95%
iter 1540: loss 0.8577, time 222.70ms, mfu 6.02%
iter 1550: loss 0.8332, time 223.77ms, mfu 6.09%
iter 1560: loss 0.8273, time 225.52ms, mfu 6.14%
iter 1570: loss 0.8205, time 229.17ms, mfu 6.17%
iter 1580: loss 0.8275, time 229.76ms, mfu 6.21%
iter 1590: loss 0.8428, time 223.78ms, mfu 6.25%
Enhanced data generator: sleep_time=0.010s, buffer_size=512
step 1600: train loss 0.6404, val loss 1.6685
🔥 ENHANCED DATA: Total=320256, Enhanced=64063 (20.0%), Generated=512, Buffer=512
iter 1600: loss 0.7917, time 7485.30ms, mfu 5.65%
iter 1610: loss 0.8310, time 230.04ms, mfu 5.73%
iter 1620: loss 0.8111, time 224.79ms, mfu 5.82%
iter 1630: loss 0.7988, time 225.51ms, mfu 5.90%
iter 1640: loss 0.8232, time 222.91ms, mfu 5.98%
iter 1650: loss 0.7742, time 220.89ms, mfu 6.05%
iter 1660: loss 0.8179, time 227.14ms, mfu 6.11%
iter 1670: loss 0.7663, time 219.52ms, mfu 6.17%
iter 1680: loss 0.8010, time 227.65ms, mfu 6.21%
iter 1690: loss 0.8048, time 220.74ms, mfu 6.27%
Enhanced data generator: sleep_time=0.010s, buffer_size=512
step 1700: train loss 0.5980, val loss 1.7026
🔥 ENHANCED DATA: Total=358656, Enhanced=71747 (20.0%), Generated=512, Buffer=512
iter 1700: loss 0.7882, time 7464.12ms, mfu 5.66%
iter 1710: loss 0.7922, time 224.54ms, mfu 5.76%
iter 1720: loss 0.7598, time 221.26ms, mfu 5.85%
iter 1730: loss 0.7632, time 227.16ms, mfu 5.93%
iter 1740: loss 0.7722, time 222.40ms, mfu 6.00%
iter 1750: loss 0.7654, time 221.85ms, mfu 6.07%
iter 1760: loss 0.7627, time 223.73ms, mfu 6.13%
iter 1770: loss 0.7597, time 223.31ms, mfu 6.19%
iter 1780: loss 0.7321, time 224.15ms, mfu 6.23%
iter 1790: loss 0.7641, time 226.33ms, mfu 6.27%
Enhanced data generator: sleep_time=0.010s, buffer_size=512
step 1800: train loss 0.5616, val loss 1.7356
🔥 ENHANCED DATA: Total=397056, Enhanced=79525 (20.0%), Generated=512, Buffer=512
iter 1800: loss 0.7642, time 7490.57ms, mfu 5.66%
iter 1810: loss 0.7683, time 231.76ms, mfu 5.74%
iter 1820: loss 0.7577, time 219.97ms, mfu 5.84%
iter 1830: loss 0.7455, time 220.57ms, mfu 5.93%
iter 1840: loss 0.7328, time 222.25ms, mfu 6.01%
iter 1850: loss 0.7435, time 223.35ms, mfu 6.08%
iter 1860: loss 0.7491, time 226.14ms, mfu 6.13%
iter 1870: loss 0.7514, time 225.39ms, mfu 6.18%
iter 1880: loss 0.7337, time 224.29ms, mfu 6.22%
iter 1890: loss 0.7180, time 224.78ms, mfu 6.26%
Enhanced data generator: sleep_time=0.010s, buffer_size=512
step 1900: train loss 0.5277, val loss 1.7504
🔥 ENHANCED DATA: Total=435456, Enhanced=87165 (20.0%), Generated=512, Buffer=512
iter 1900: loss 0.7339, time 7483.34ms, mfu 5.66%
iter 1910: loss 0.7543, time 220.28ms, mfu 5.77%
iter 1920: loss 0.7135, time 221.40ms, mfu 5.87%
iter 1930: loss 0.7104, time 221.83ms, mfu 5.95%
iter 1940: loss 0.7443, time 227.69ms, mfu 6.01%
iter 1950: loss 0.7037, time 224.32ms, mfu 6.07%
iter 1960: loss 0.7152, time 225.67ms, mfu 6.13%
iter 1970: loss 0.7102, time 221.78ms, mfu 6.19%
iter 1980: loss 0.6951, time 221.36ms, mfu 6.24%
iter 1990: loss 0.7020, time 222.97ms, mfu 6.29%
Enhanced data generator: sleep_time=0.010s, buffer_size=512
c^CTraceback (most recent call last):
  File "/teamspace/studios/this_studio/nanoGPT/train.py", line 692, in <module>
    losses = estimate_loss()
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
  File "/teamspace/studios/this_studio/nanoGPT/train.py", line 620, in estimate_loss
    losses[k] = loss.item()
KeyboardInterrupt
^CException ignored in atexit callback: <function shutdown_compile_workers at 0x7aa6d19d7130>
Traceback (most recent call last):
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/_inductor/async_compile.py", line 113, in shutdown_compile_workers
    pool.shutdown()
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/_inductor/compile_worker/subproc_pool.py", line 239, in shutdown
    self.process.wait(300)
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/subprocess.py", line 1209, in wait
    return self._wait(timeout=timeout)
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/subprocess.py", line 1937, in _wait
    time.sleep(delay)
KeyboardInterrupt: 

⚡ feature/enhanced-data-augmentation ~/nanoGPT 