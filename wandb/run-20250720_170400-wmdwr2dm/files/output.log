Calculating target architecture based on schedule...

============================================================
       TARGET MODEL ARCHITECTURE (at end of schedule)
============================================================
  n_layer                | 6
  n_head                 | 12
  n_embd                 | 768
  n_hidden               | 1536
  block_size             | 1024
  vocab_size             | 50304
  dropout                | 0.0
  bias                   | False
  embedding_mode         | standard
  attn_lora_rank         | 0
  embedding_rank         | 0
  lora_alpha             | 0.0
============================================================


============================================================
           INITIAL MODEL ARCHITECTURE (at Iter 0)
============================================================
  n_layer                | 3
  n_head                 | 12
  n_embd                 | 768
  n_hidden               | 768
  block_size             | 1024
  vocab_size             | 50304
  dropout                | 0.0
  bias                   | False
  embedding_mode         | lora
  attn_lora_rank         | 48
  embedding_rank         | 48
  lora_alpha             | 1.0
============================================================


Detailed parameter count:
  total                  | Total:   52,149,504 | Trainable:    8,207,616
  token_embeddings       | Total:    2,451,456 | Trainable:    2,451,456
  position_embeddings    | Total:            0 | Trainable:            0
  attention_layers       | Total:    7,520,256 | Trainable:    2,211,840
  feed_forward_layers    | Total:    3,538,944 | Trainable:    3,538,944
  layer_norms            | Total:        4,608 | Trainable:        4,608
  final_layer_norm       | Total:          768 | Trainable:          768
  language_model_head    | Total:   38,633,472 | Trainable:            0
------------------------------------------------------------
eval every:100
step 0: train loss 10.9717, val loss 10.9694
--- Model Analysis ---
  MLP Rank Utilization (L0): 71.09% (546/768)
--- Model Analysis ---
Attention Q  | 547/768         | 71.22%
Attention K  | 547/768         | 71.22%
Attention V  | 546/768         | 71.09%
--- Model Analysis ---
  MLP Rank Utilization (L1): 71.09% (546/768)
--- Model Analysis ---
Attention Q  | 547/768         | 71.22%
Attention K  | 547/768         | 71.22%
Attention V  | 547/768         | 71.22%
--- Model Analysis ---
  MLP Rank Utilization (L2): 71.22% (547/768)
--- Model Analysis ---
Attention Q  | 547/768         | 71.22%
Attention K  | 546/768         | 71.09%
Attention V  | 547/768         | 71.22%
--- Model Analysis ---
  Embedding Utilization (L2): 97.53% (749/768)
  Average Attention Entropy:  5.8975
----------------------
Embed LoRA   | 47/48           | 97.92%
Attn LoRA    | 47/48           | 97.92%
merge_lora_weights 10.969396591186523 8.0 1500
iter 0: loss 10.9716, lr 0.00005, time 9628.76ms, mfu -100.00%
iter 10: loss 9.5894, lr 0.00055, time 540.78ms, mfu 34.17%
iter 20: loss 9.4718, lr 0.00104, time 545.82ms, mfu 34.14%
iter 30: loss 9.2637, lr 0.00154, time 552.23ms, mfu 34.07%
iter 40: loss 9.1110, lr 0.00204, time 565.22ms, mfu 33.94%
iter 50: loss 8.9507, lr 0.00254, time 569.05ms, mfu 33.79%
iter 60: loss 8.8061, lr 0.00303, time 563.92ms, mfu 33.69%
iter 70: loss 8.8973, lr 0.00353, time 571.67ms, mfu 33.55%
iter 80: loss 8.7502, lr 0.00403, time 570.23ms, mfu 33.44%
iter 90: loss 8.5850, lr 0.00453, time 580.11ms, mfu 33.28%
step 100: train loss 8.6094, val loss 8.5895
--- Model Analysis ---
  MLP Rank Utilization (L0): 65.10% (500/768)
--- Model Analysis ---
Attention Q  | 547/768         | 71.22%
Attention K  | 547/768         | 71.22%
Attention V  | 546/768         | 71.09%
--- Model Analysis ---
  MLP Rank Utilization (L1): 66.67% (512/768)
--- Model Analysis ---
Attention Q  | 547/768         | 71.22%
Attention K  | 547/768         | 71.22%
Attention V  | 547/768         | 71.22%
--- Model Analysis ---
  MLP Rank Utilization (L2): 67.71% (520/768)
--- Model Analysis ---
Attention Q  | 547/768         | 71.22%
Attention K  | 546/768         | 71.09%
Attention V  | 547/768         | 71.22%
--- Model Analysis ---
  Embedding Utilization (L2): 97.53% (749/768)
  Average Attention Entropy:  5.4798
----------------------
Embed LoRA   | 2/48            | 4.17%
Attn LoRA    | 2/48            | 4.17%
saving checkpoint to out
merge_lora_weights 8.589521408081055 8.0 1500
iter 100: loss 8.5737, lr 0.00502, time 6204.19ms, mfu 30.25%
iter 110: loss 8.5795, lr 0.00552, time 574.67ms, mfu 30.44%
iter 120: loss 8.4159, lr 0.00602, time 577.53ms, mfu 30.60%
iter 130: loss 8.3002, lr 0.00652, time 582.62ms, mfu 30.71%
iter 140: loss 8.3533, lr 0.00701, time 580.80ms, mfu 30.82%
iter 150: loss 8.2904, lr 0.00751, time 579.40ms, mfu 30.93%
iter 160: loss 8.1401, lr 0.00801, time 579.37ms, mfu 31.02%
iter 170: loss 8.0170, lr 0.00851, time 581.80ms, mfu 31.10%
iter 180: loss 8.1015, lr 0.00900, time 579.36ms, mfu 31.18%
iter 190: loss 7.9683, lr 0.00950, time 568.68ms, mfu 31.31%
step 200: train loss 7.9578, val loss 7.9812
--- Model Analysis ---
  MLP Rank Utilization (L0): 60.03% (461/768)
--- Model Analysis ---
Attention Q  | 547/768         | 71.22%
Attention K  | 547/768         | 71.22%
Attention V  | 546/768         | 71.09%
--- Model Analysis ---
  MLP Rank Utilization (L1): 50.65% (389/768)
--- Model Analysis ---
Attention Q  | 547/768         | 71.22%
Attention K  | 547/768         | 71.22%
Attention V  | 547/768         | 71.22%
--- Model Analysis ---
  MLP Rank Utilization (L2): 51.95% (399/768)
--- Model Analysis ---
Attention Q  | 547/768         | 71.22%
Attention K  | 546/768         | 71.09%
Attention V  | 547/768         | 71.22%
--- Model Analysis ---
  Embedding Utilization (L2): 97.53% (749/768)
  Average Attention Entropy:  4.5793
----------------------
Embed LoRA   | 2/48            | 4.17%
Attn LoRA    | 5/48            | 10.42%
saving checkpoint to out

=== SCALING OPERATION TRIGGERED (DDP SYNC) ===
Operation: merge_lora_weights
Trigger reason: Loss threshold
Current val loss: 7.9812, Trigger loss: 8.0000
Iterations since last op: 200, Max wait: 1500
Executing operation: merge_lora_weights first burn with value: None
Performing architectural operation: merge_lora_weights
Merging LoRA weights into main weights...
LoRA weights merged and reset.

Detailed parameter count:
  total                  | Total:   52,149,504 | Trainable:    8,207,616
  token_embeddings       | Total:    2,451,456 | Trainable:    2,451,456
  position_embeddings    | Total:            0 | Trainable:            0
  attention_layers       | Total:    7,520,256 | Trainable:    2,211,840
  feed_forward_layers    | Total:    3,538,944 | Trainable:    3,538,944
  layer_norms            | Total:        4,608 | Trainable:        4,608
  final_layer_norm       | Total:          768 | Trainable:          768
  language_model_head    | Total:   38,633,472 | Trainable:            0
------------------------------------------------------------
Re-configuring optimizer after architectural change...
num decayed parameter tensors: 17, with 8,202,240 parameters
num non-decayed parameter tensors: 7, with 5,376 parameters
using fused AdamW: True
Transferring optimizer state for existing parameters...
Transferred optimizer state for 0 / 28 parameters
Re-compiling the model...

============================================================
             ARCHITECTURE CHANGE (at Iter 200)
============================================================
  n_layer                | 3
  n_head                 | 12
  n_embd                 | 768
  n_hidden               | 768
  block_size             | 1024
  vocab_size             | 50304
  dropout                | 0.0
  bias                   | False
  embedding_mode         | lora
  attn_lora_rank         | 48
  embedding_rank         | 48
  lora_alpha             | 1.0
============================================================

Architectural operation completed successfully.
=== SCALING OPERATION COMPLETE ===


=== SCALING OPERATION TRIGGERED (DDP SYNC) ===
Operation: change_lr
Trigger reason: Loss threshold
Current val loss: 7.9812, Trigger loss: 100.0000
Iterations since last op: 0, Max wait: 1
Executing operation: change_lr  with value: 0.3
LR multiplier: 10.0000 -> 3.0000
=== SCALING OPERATION COMPLETE ===


=== SCALING OPERATION TRIGGERED (DDP SYNC) ===
Operation: reset_lr_schedule
Trigger reason: Loss threshold
Current val loss: 7.9812, Trigger loss: 100.0000
Iterations since last op: 0, Max wait: 1
Executing operation: reset_lr_schedule  with value: None
LR schedule offset: 0 -> 200
=== SCALING OPERATION COMPLETE ===

merge_lora_weights 7.981232643127441 6.7 1500
iter 200: loss 10.6596, lr 0.01000, time 6022.26ms, mfu 28.49%
iter 210: loss 8.6654, lr 0.00016, time 570.13ms, mfu 28.88%
iter 220: loss 8.2860, lr 0.00031, time 569.45ms, mfu 29.24%
iter 230: loss 8.1066, lr 0.00046, time 573.94ms, mfu 29.53%
iter 240: loss 7.8953, lr 0.00061, time 575.69ms, mfu 29.79%
iter 250: loss 7.9143, lr 0.00076, time 578.90ms, mfu 30.00%
iter 260: loss 7.7880, lr 0.00091, time 579.53ms, mfu 30.19%
iter 270: loss 8.0194, lr 0.00106, time 573.16ms, mfu 30.40%
iter 280: loss 7.8614, lr 0.00121, time 579.32ms, mfu 30.55%
iter 290: loss 7.8675, lr 0.00136, time 579.14ms, mfu 30.68%
step 300: train loss 7.7964, val loss 7.8080
--- Model Analysis ---
  MLP Rank Utilization (L0): 60.42% (464/768)
--- Model Analysis ---
Attention Q  | 543/768         | 70.70%
Attention K  | 545/768         | 70.96%
Attention V  | 546/768         | 71.09%
--- Model Analysis ---
  MLP Rank Utilization (L1): 50.52% (388/768)
--- Model Analysis ---
Attention Q  | 544/768         | 70.83%
Attention K  | 543/768         | 70.70%
Attention V  | 547/768         | 71.22%
--- Model Analysis ---
  MLP Rank Utilization (L2): 51.69% (397/768)
--- Model Analysis ---
Attention Q  | 544/768         | 70.83%
Attention K  | 543/768         | 70.70%
Attention V  | 547/768         | 71.22%
--- Model Analysis ---
  Embedding Utilization (L2): 97.27% (747/768)
  Average Attention Entropy:  4.6015
----------------------
Embed LoRA   | 3/48            | 6.25%
Attn LoRA    | 24/48           | 50.00%
saving checkpoint to out
merge_lora_weights 7.807960510253906 6.7 1500
iter 300: loss 7.9089, lr 0.00151, time 6096.23ms, mfu 27.92%
iter 310: loss 7.9535, lr 0.00166, time 577.16ms, mfu 28.33%
iter 320: loss 7.7310, lr 0.00181, time 579.50ms, mfu 28.68%
iter 330: loss 7.6001, lr 0.00196, time 580.21ms, mfu 29.00%
iter 340: loss 7.6632, lr 0.00210, time 581.11ms, mfu 29.28%
iter 350: loss 7.6853, lr 0.00225, time 575.56ms, mfu 29.56%
iter 360: loss 7.6385, lr 0.00240, time 580.35ms, mfu 29.79%
iter 370: loss 7.8437, lr 0.00255, time 580.24ms, mfu 30.00%
iter 380: loss 7.5150, lr 0.00270, time 580.30ms, mfu 30.18%
iter 390: loss 7.6946, lr 0.00285, time 578.09ms, mfu 30.36%
step 400: train loss 7.6176, val loss 7.5699
--- Model Analysis ---
  MLP Rank Utilization (L0): 61.46% (472/768)
--- Model Analysis ---
Attention Q  | 543/768         | 70.70%
Attention K  | 545/768         | 70.96%
Attention V  | 546/768         | 71.09%
--- Model Analysis ---
  MLP Rank Utilization (L1): 50.26% (386/768)
--- Model Analysis ---
Attention Q  | 544/768         | 70.83%
Attention K  | 543/768         | 70.70%
Attention V  | 547/768         | 71.22%
--- Model Analysis ---
  MLP Rank Utilization (L2): 50.65% (389/768)
--- Model Analysis ---
Attention Q  | 544/768         | 70.83%
Attention K  | 543/768         | 70.70%
Attention V  | 547/768         | 71.22%
--- Model Analysis ---
  Embedding Utilization (L2): 97.27% (747/768)
  Average Attention Entropy:  4.4871
----------------------
Embed LoRA   | 3/48            | 6.25%
Attn LoRA    | 6/48            | 12.50%
saving checkpoint to out
merge_lora_weights 7.569907188415527 6.7 1500
iter 400: loss 7.5711, lr 0.00300, time 5658.01ms, mfu 27.65%
iter 410: loss 7.5759, lr 0.00300, time 571.47ms, mfu 28.12%
iter 420: loss 7.4740, lr 0.00300, time 581.50ms, mfu 28.49%
iter 430: loss 7.4975, lr 0.00300, time 580.36ms, mfu 28.82%
Traceback (most recent call last):
  File "/teamspace/studios/this_studio/nanoGPT/train.py", line 882, in <module>
    if local_iter_num >= 5:
KeyboardInterrupt
Traceback (most recent call last):
  File "/teamspace/studios/this_studio/nanoGPT/train.py", line 882, in <module>
    if local_iter_num >= 5:
KeyboardInterrupt
Exception ignored in atexit callback: <function _start_and_connect_service.<locals>.teardown_atexit at 0x7389e0f9a0e0>
Traceback (most recent call last):
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/wandb/sdk/lib/service/service_connection.py", line 54, in teardown_atexit
    conn.teardown(hooks.exit_code)
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/wandb/sdk/lib/service/service_connection.py", line 182, in teardown
    self._router.join()
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/wandb/sdk/interface/router.py", line 75, in join
    self._thread.join()
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/threading.py", line 1096, in join
    self._wait_for_tstate_lock()
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/threading.py", line 1116, in _wait_for_tstate_lock
    if lock.acquire(block, timeout):
KeyboardInterrupt:
Exception ignored in atexit callback: <function shutdown_compile_workers at 0x7389ec52dc60>
Traceback (most recent call last):
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/_inductor/async_compile.py", line 113, in shutdown_compile_workers
    pool.shutdown()
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/_inductor/compile_worker/subproc_pool.py", line 239, in shutdown
    self.process.wait(300)
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/subprocess.py", line 1209, in wait
    return self._wait(timeout=timeout)
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/subprocess.py", line 1937, in _wait
    time.sleep(delay)
KeyboardInterrupt:
