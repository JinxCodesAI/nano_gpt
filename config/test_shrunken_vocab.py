# Test configuration for shrunken vocabulary training feature
# This config demonstrates the shrunken vocabulary feature with a small model for testing

import torch

# Basic training settings
wandb_log = False # Disable wandb for testing
eval_interval = 100 # Evaluate more frequently for testing
log_interval = 10
eval_iters = 10 # Fewer eval iterations for faster testing
eval_only = False
always_save_checkpoint = False # Don't save checkpoints during testing
init_from = 'scratch'

# Data settings
dataset = 'fineweb10B' # Use the dataset that has the token mapping
batch_size = 4 # Small batch size for testing
block_size = 256 # Smaller block size for testing
gradient_accumulation_steps = 2

# Model settings - small model for testing
n_layer = 4
n_head = 4
n_embd = 128
dropout = 0.0
bias = False
n_hidden = None # Will default to 4 * n_embd = 512

# Training settings
learning_rate = 1e-3 # Higher learning rate for faster testing
max_iters = 500 # Short training for testing
weight_decay = 1e-2
beta1 = 0.9
beta2 = 0.95
grad_clip = 1.0

# Learning rate decay settings
decay_lr = True
warmup_iters = 50 # Short warmup for testing
lr_decay_iters = 400 # Most of the training
min_lr = 1e-4

# System settings
device = 'cuda' if torch.cuda.is_available() else 'cpu'
dtype = 'bfloat16' if torch.cuda.is_available() and torch.cuda.is_bf16_supported() else 'float16'
compile = False # Disable compilation for testing to avoid overhead

# LoRA settings (disabled for this test)
embedding_mode = 'standard'
attn_lora_rank = 0
embedding_rank = 0
lora_alpha = 1.0

# Shrunken vocabulary settings - THE KEY FEATURE BEING TESTED
shrunken_vocab_size = 11199 # Size from the token mapping file
vocab_remapping_file = 'data/vocab_remapping.pt' # Generated by our script
RARE_TOKEN_ID = 11198 # The rare token ID from the mapping

# Scaling schedule to test vocabulary operations
scaling_schedule = [
    {
        "name": "resize_vocabulary",
        "desc": "Grow vocabulary to full size",
        "trigger_loss": 4.0,  # Trigger when loss drops below 4.0
        "max_wait_iters": 300,  # Or after 300 iterations
        "value": [11198, 0.01],  # [source_token_id, noise_std]
        "reevaluate": True,
        "blocking": True
    },
    {
        "name": "disable_vocab_remapping", 
        "desc": "Stop vocabulary remapping",
        "trigger_loss": None,  # Execute immediately after resize_vocabulary
        "max_wait_iters": 0,
        "value": None,
        "reevaluate": False,
        "blocking": False
    },
    {
        "name": "set_embedding_finetune_mode",
        "desc": "Enable embedding fine-tuning mode",
        "trigger_loss": None,  # Execute immediately
        "max_wait_iters": 0,
        "value": True,
        "reevaluate": False,
        "blocking": True
    }
]

# Output directory for test
out_dir = 'out-test-shrunken-vocab'
