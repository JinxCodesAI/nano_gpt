================================================================================
Training run started at 2025-07-20 20:27:24
================================================================================
Configuration:
----------------------------------------
always_save_checkpoint: False
attn_lora_rank: 48
attn_lora_rank_divisor: 16
backend: nccl
batch_size: 32
batch_size_multiplier: 1.0
beta1: 0.9
beta2: 0.95
bias: False
block_size: 1024
compile: True
dataset: fineweb10B
decay_lr: True
device: cuda
dropout: 0.0
dtype: bfloat16
embedding_mode: lora
embedding_rank: 48
eval_interval: 20
eval_interval_multiplier: 1.0
eval_iters: 1
eval_iters_multiplier: 1.0
eval_only: False
file_logging: True
grad_accum_multiplier: 1.0
grad_clip: 1.0
gradient_accumulation_steps: 1
init_from: scratch
learning_rate: 0.001
log_dir: logs
log_interval: 10
lora_alpha: 1.0
lora_alpha_multiplier: 1.0
lr_decay_iters: 600000
lr_multiplier: 0.2
max_iters: 600000
min_lr: 6e-05
n_embd: 768
n_head: 12
n_hidden_divisor: 4
n_layer: 12
n_layer_divisor: 12
out_dir: out
rotary_base: 10000.0
rotary_max_position_embeddings: 2048
use_rotary_embeddings: True
vocab_lora_rank_divisor: 16
wandb_log: True
wandb_project: owt
wandb_run_name: gpt2
warmup_iters: 200
warmup_iters_multiplier: 1.0
weight_decay: 0.1
----------------------------------------
[2025-07-20 20:27:33] step 0: train loss 10.9453, val loss 10.9543
[2025-07-20 20:27:39] step 20: train loss 10.5200, val loss 10.5191
[2025-07-20 20:27:44] step 40: train loss 9.8811, val loss 9.8522
[2025-07-20 20:27:50] step 60: train loss 9.6863, val loss 9.6878
[2025-07-20 20:27:55] step 80: train loss 9.5858, val loss 9.6277
[2025-07-20 20:28:01] step 100: train loss 9.5384, val loss 9.4798
[2025-07-20 20:28:06] step 120: train loss 9.3906, val loss 9.4590
[2025-07-20 20:28:12] step 140: train loss 9.3640, val loss 9.3088
[2025-07-20 20:28:18] step 160: train loss 9.2748, val loss 9.2711
[2025-07-20 20:28:23] step 180: train loss 9.1589, val loss 9.1125
[2025-07-20 20:28:29] step 200: train loss 9.2234, val loss 9.1287
[2025-07-20 20:28:34] step 220: train loss 9.1026, val loss 9.0835
[2025-07-20 20:28:40] step 240: train loss 9.1048, val loss 9.1422
[2025-07-20 20:28:45] step 260: train loss 8.9967, val loss 9.0505
[2025-07-20 20:28:50] step 280: train loss 8.9491, val loss 8.9000
